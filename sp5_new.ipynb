{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 17:45:15.033435: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-04 17:45:15.045755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743768915.060522  640800 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743768915.064943  640800 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743768915.077012  640800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743768915.077028  640800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743768915.077030  640800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743768915.077031  640800 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-04 17:45:15.080675: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLA JIT compilation disabled.\n",
      "TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, GRU, Dense, Dropout, BatchNormalization, LayerNormalization, Reshape, Permute, Bidirectional, Add, Attention, Flatten, TimeDistributed, Conv2DTranspose, Conv2D, Layer, Concatenate, Multiply, AdditiveAttention # Added Multiply, AdditiveAttention for potential SelfAttention implementation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop # Added RMSprop as an option for WGAN\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve # Moved confusion_matrix here\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "# import noisereduce as nr # Consider if still needed/effective\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, sosfilt\n",
    "import logging\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for better Jupyter integration\n",
    "import random\n",
    "import seaborn as sns # Added for confusion matrix plotting\n",
    "tf.config.optimizer.set_jit(False)\n",
    "print(\"XLA JIT compilation disabled.\")\n",
    "\n",
    "# WGAN-GP specific\n",
    "from tensorflow import GradientTape\n",
    "\n",
    "# Create directory for saving figures\n",
    "FIGURES_DIR = 'training_figures_wgan_sa' # Changed dir name\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='audio_errors_wgan_sa.log', level=logging.ERROR,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and configured.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Force GPU usage\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"GPU is available and configured.\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(f\"Error configuring GPU: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices found. Falling back to CPU.\")\n",
    "\n",
    "# Set mixed precision policy if desired (can speed up training on compatible GPUs)\n",
    "#from tensorflow.keras import mixed_precision\n",
    "#policy = mixed_precision.Policy('mixed_float16')\n",
    "#mixed_precision.set_global_policy(policy)\n",
    "#print('Mixed precision enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Audio Loading and Preprocessing Function\n",
    "def load_and_preprocess_audio(file_path, sr=16000, duration=4):\n",
    "    try:\n",
    "        # Load audio, potentially reducing noise first if beneficial\n",
    "        audio, current_sr = librosa.load(file_path, sr=None, duration=duration) # Load native SR first\n",
    "\n",
    "        # Optional: Noise Reduction (experiment if needed)\n",
    "        # audio = nr.reduce_noise(y=audio, sr=current_sr)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if current_sr != sr:\n",
    "            audio = librosa.resample(audio, orig_sr=current_sr, target_sr=sr)\n",
    "\n",
    "        # Pad or truncate to fixed duration *before* augmentation/normalization\n",
    "        target_len = sr * duration\n",
    "        if len(audio) < target_len:\n",
    "            audio = np.pad(audio, (0, target_len - len(audio)), mode='constant')\n",
    "        else:\n",
    "            audio = audio[:target_len]\n",
    "\n",
    "        # Data Augmentation (applied *before* normalization)\n",
    "        if np.random.random() < 0.5: # 50% chance\n",
    "            augmentation_type = np.random.choice(['noise', 'pitch', 'speed'])\n",
    "            if augmentation_type == 'noise':\n",
    "                noise_amp = 0.005 * np.random.uniform(0.1, 1.0) * np.max(np.abs(audio)) # Scale noise relative to audio\n",
    "                noise = np.random.randn(len(audio)) * noise_amp\n",
    "                audio = audio + noise\n",
    "            elif augmentation_type == 'pitch':\n",
    "                pitch_shift_steps = np.random.uniform(-2.5, 2.5)\n",
    "                audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=pitch_shift_steps)\n",
    "            else: # speed (time stretch)\n",
    "                speed_rate = np.random.uniform(0.85, 1.15)\n",
    "                audio = librosa.effects.time_stretch(audio, rate=speed_rate)\n",
    "                 # Time stretching changes length, re-pad/truncate\n",
    "                if len(audio) < target_len:\n",
    "                    audio = np.pad(audio, (0, target_len - len(audio)), mode='constant')\n",
    "                else:\n",
    "                    audio = audio[:target_len]\n",
    "\n",
    "        # Normalize audio (peak normalization) - crucial for consistency\n",
    "        max_amp = np.max(np.abs(audio))\n",
    "        if max_amp > 1e-6: # Avoid division by zero\n",
    "             audio = audio / max_amp\n",
    "        # Optional: RMS normalization instead\n",
    "        # rms = np.sqrt(np.mean(audio**2))\n",
    "        # if rms > 1e-6:\n",
    "        #    audio = audio / rms * 0.5 # Scale to a target RMS\n",
    "\n",
    "        return audio\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading/preprocessing {file_path}: {e}\")\n",
    "        print(f\"Error loading/preprocessing {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Feature Extraction Function\n",
    "def extract_features(audio, sr=16000, n_mels=80, n_fft=2048, hop_length=512):\n",
    "    if audio is None:\n",
    "        return None\n",
    "    try:\n",
    "        # Extract mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio,\n",
    "            sr=sr,\n",
    "            n_mels=n_mels,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length\n",
    "        )\n",
    "        # Convert to log scale (dB)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        # Normalize features (per spectrogram) - Standard Scaling\n",
    "        mean = np.mean(log_mel_spec)\n",
    "        std = np.std(log_mel_spec)\n",
    "        if std > 1e-6: # Avoid division by zero\n",
    "            log_mel_spec = (log_mel_spec - mean) / std\n",
    "        else:\n",
    "            log_mel_spec = log_mel_spec - mean # Just center if std is near zero\n",
    "\n",
    "        # Ensure the shape is consistent (should be handled by fixed duration loading)\n",
    "        # Expected frames: int(np.ceil(target_len / hop_length)) -> int(ceil(16000*4/512)) = 125? Check calculation\n",
    "        # target_len = 16000 * 4 = 64000\n",
    "        # expected_frames = 64000 // hop_length + 1 if 64000 % hop_length != 0 else 64000 // hop_length\n",
    "        # expected_frames = 64000 / 512 = 125. Check librosa padding. It often adds a frame. Let's stick to TARGET_FRAMES=126 based on previous findings.\n",
    "\n",
    "        return log_mel_spec # Shape (n_mels, n_frames) e.g. (80, 126)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting features: {e}\")\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Class Distribution Analysis Function\n",
    "def analyze_class_distribution(data_path):\n",
    "    try:\n",
    "        real_dir = os.path.join(data_path, 'real')\n",
    "        fake_dir = os.path.join(data_path, 'fake')\n",
    "        real_count = len([f for f in os.listdir(real_dir) if f.endswith('.wav')]) if os.path.exists(real_dir) else 0\n",
    "        fake_count = len([f for f in os.listdir(fake_dir) if f.endswith('.wav')]) if os.path.exists(fake_dir) else 0\n",
    "        total = real_count + fake_count\n",
    "        if total == 0:\n",
    "            print(f\"\\nNo .wav files found in {data_path}\")\n",
    "            return {'real': 0, 'fake': 0}\n",
    "        print(f\"\\nClass Distribution for {data_path}:\")\n",
    "        print(f\"Real: {real_count} ({real_count/total*100:.2f}%)\")\n",
    "        print(f\"Fake: {fake_count} ({fake_count/total*100:.2f}%)\")\n",
    "        return {'real': real_count, 'fake': fake_count}\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nError: Data path not found - {data_path}\")\n",
    "        return {'real': 0, 'fake': 0}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error analyzing distribution for {data_path}: {e}\")\n",
    "        print(f\"Error analyzing distribution for {data_path}: {e}\")\n",
    "        return {'real': 0, 'fake': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Data Generators\n",
    "\n",
    "# Define the fixed number of frames for GAN/Classifier consistency\n",
    "TARGET_FRAMES = 126 # Recalculate based on sr=16000, duration=4, hop_length=512 if needed\n",
    "\n",
    "# Data generator for STANDALONE CLASSIFIER training (Yields X, y, sample_weights)\n",
    "# --- (data_generator_classifier function remains the same as corrected before) ---\n",
    "def data_generator_classifier(data_path, batch_size=32, shuffle=True, target_frames=TARGET_FRAMES, sr=16000, duration=4, n_mels=80, n_fft=2048, hop_length=512):\n",
    "    real_files = []\n",
    "    fake_files = []\n",
    "    try:\n",
    "        real_dir = os.path.join(data_path, 'real')\n",
    "        fake_dir = os.path.join(data_path, 'fake')\n",
    "        if os.path.exists(real_dir):\n",
    "             real_files = [os.path.join(real_dir, f) for f in os.listdir(real_dir) if f.endswith('.wav')]\n",
    "        if os.path.exists(fake_dir):\n",
    "             fake_files = [os.path.join(fake_dir, f) for f in os.listdir(fake_dir) if f.endswith('.wav')]\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error finding directories in {data_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    all_files = real_files + fake_files\n",
    "    labels = [1] * len(real_files) + [0] * len(fake_files) # Real=1, Fake=0\n",
    "\n",
    "    if not all_files:\n",
    "        print(f\"No WAV files found in {data_path}. Classifier generator stopping.\")\n",
    "        return\n",
    "\n",
    "    total_samples = len(all_files)\n",
    "    class_weights = {\n",
    "        1: total_samples / (2 * len(real_files)) if len(real_files) > 0 else 1.0,\n",
    "        0: total_samples / (2 * len(fake_files)) if len(fake_files) > 0 else 1.0,\n",
    "    }\n",
    "    print(f\"Using class weights: {class_weights} for path {data_path}\")\n",
    "\n",
    "\n",
    "    indices = np.arange(total_samples)\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            batch_files = [all_files[k] for k in batch_indices]\n",
    "            batch_labels = [labels[k] for k in batch_indices]\n",
    "\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            batch_sample_weights = []\n",
    "\n",
    "            for file_path, label in zip(batch_files, batch_labels):\n",
    "                audio = load_and_preprocess_audio(file_path, sr=sr, duration=duration)\n",
    "                if audio is None: continue\n",
    "\n",
    "                features = extract_features(audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "                if features is None: continue\n",
    "\n",
    "                current_frames = features.shape[1]\n",
    "                if current_frames != target_frames:\n",
    "                     if current_frames < target_frames:\n",
    "                        pad_width = target_frames - current_frames\n",
    "                        padded_features = np.pad(features, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                     else:\n",
    "                        padded_features = features[:, :target_frames]\n",
    "                else:\n",
    "                    padded_features = features\n",
    "\n",
    "                batch_x.append(padded_features)\n",
    "                batch_y.append(label)\n",
    "                batch_sample_weights.append(class_weights[label])\n",
    "\n",
    "            # --- Check for classifier generator ---\n",
    "            if not batch_x:\n",
    "                print(f\"Warning: Skipping empty batch yield in data_generator_classifier for path {data_path}\")\n",
    "                continue\n",
    "\n",
    "            batch_x_4d = np.expand_dims(np.array(batch_x), axis=-1).astype(np.float32)\n",
    "            batch_y_arr = np.array(batch_y).astype(np.float32)\n",
    "            batch_weights_arr = np.array(batch_sample_weights).astype(np.float32)\n",
    "            yield batch_x_4d, batch_y_arr, batch_weights_arr\n",
    "\n",
    "\n",
    "# Data generator for WGAN training (Yields real fake samples X only)\n",
    "# --- (MODIFIED data_generator_gan function) ---\n",
    "def data_generator_gan(data_path, batch_size=32, shuffle=True, target_frames=TARGET_FRAMES, sr=16000, duration=4, n_mels=80, n_fft=2048, hop_length=512):\n",
    "    fake_files = []\n",
    "    try:\n",
    "        fake_dir = os.path.join(data_path, 'fake')\n",
    "        if os.path.exists(fake_dir):\n",
    "             fake_files = [os.path.join(fake_dir, f) for f in os.listdir(fake_dir) if f.endswith('.wav')]\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error finding fake directory in {data_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    if not fake_files:\n",
    "        print(f\"No fake WAV files found in {data_path}. GAN generator stopping.\")\n",
    "        return\n",
    "\n",
    "    total_samples = len(fake_files)\n",
    "    indices = np.arange(total_samples)\n",
    "\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            batch_files = [fake_files[k] for k in batch_indices]\n",
    "\n",
    "            batch_x = []\n",
    "\n",
    "            for file_path in batch_files:\n",
    "                audio = load_and_preprocess_audio(file_path, sr=sr, duration=duration)\n",
    "                if audio is None: continue\n",
    "\n",
    "                features = extract_features(audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "                if features is None: continue\n",
    "\n",
    "                current_frames = features.shape[1]\n",
    "                if current_frames != target_frames:\n",
    "                    if current_frames < target_frames:\n",
    "                        pad_width = target_frames - current_frames\n",
    "                        padded_features = np.pad(features, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                    else:\n",
    "                        padded_features = features[:, :target_frames]\n",
    "                else:\n",
    "                     padded_features = features\n",
    "\n",
    "                batch_x.append(padded_features)\n",
    "\n",
    "            # ---> ADD THIS CHECK for GAN generator <---\n",
    "            if not batch_x:\n",
    "                print(f\"Warning: Skipping empty batch yield in data_generator_gan for path {data_path}\")\n",
    "                continue # Skip yield if batch ended up empty\n",
    "\n",
    "            # If batch is not empty, yield the data\n",
    "            batch_x_4d = np.expand_dims(np.array(batch_x), axis=-1).astype(np.float32)\n",
    "            yield batch_x_4d # Yield only the features (4D array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Self-Attention Layer (SAGAN style)\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    \"\"\"\n",
    "    Self-attention layer based on SAGAN.\n",
    "    Input shape: (batch, height, width, channels)\n",
    "    Output shape: (batch, height, width, channels_out) where channels_out is typically channels\n",
    "    \"\"\"\n",
    "    def __init__(self, channels_out=None, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_channels = input_shape[-1]\n",
    "        if self.channels_out is None:\n",
    "            self.channels_out = self.input_channels\n",
    "\n",
    "        # Convolution layers for query, key, value\n",
    "        # Use 1x1 convolutions to reduce/transform channels\n",
    "        self.f = Conv2D(self.input_channels // 8, kernel_size=1, strides=1, padding='same', name='conv_f') # Query\n",
    "        self.g = Conv2D(self.input_channels // 8, kernel_size=1, strides=1, padding='same', name='conv_g') # Key\n",
    "        self.h = Conv2D(self.channels_out, kernel_size=1, strides=1, padding='same', name='conv_h')        # Value\n",
    "\n",
    "        # Final 1x1 convolution\n",
    "        self.out_conv = Conv2D(self.channels_out, kernel_size=1, strides=1, padding='same', name='conv_out')\n",
    "\n",
    "        # Learnable scale parameter\n",
    "        self.gamma = self.add_weight(name='gamma', shape=(1,), initializer='zeros', trainable=True)\n",
    "\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size, height, width, num_channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        location_num = height * width\n",
    "        downsampled_num = location_num\n",
    "\n",
    "        # Query (f), Key (g), Value (h) projections\n",
    "        f_proj = self.f(x) # Shape: (batch, h, w, c/8)\n",
    "        g_proj = self.g(x) # Shape: (batch, h, w, c/8)\n",
    "        h_proj = self.h(x) # Shape: (batch, h, w, c_out)\n",
    "\n",
    "        # Reshape for matrix multiplication\n",
    "        f_flat = tf.reshape(f_proj, shape=(batch_size, location_num, self.input_channels // 8)) # (batch, h*w, c/8)\n",
    "        g_flat = tf.reshape(g_proj, shape=(batch_size, location_num, self.input_channels // 8)) # (batch, h*w, c/8)\n",
    "        h_flat = tf.reshape(h_proj, shape=(batch_size, location_num, self.channels_out))       # (batch, h*w, c_out)\n",
    "\n",
    "        # Attention map calculation\n",
    "        # Transpose g for matmul: (batch, c/8, h*w)\n",
    "        g_flat_t = tf.transpose(g_flat, perm=[0, 2, 1])\n",
    "        # Attention score: (batch, h*w, c/8) x (batch, c/8, h*w) -> (batch, h*w, h*w)\n",
    "        attention_score = tf.matmul(f_flat, g_flat_t)\n",
    "        attention_prob = tf.nn.softmax(attention_score, axis=-1) # Apply softmax across locations\n",
    "\n",
    "        # Apply attention map to value projection\n",
    "        # (batch, h*w, h*w) x (batch, h*w, c_out) -> (batch, h*w, c_out)\n",
    "        attention_output = tf.matmul(attention_prob, h_flat)\n",
    "\n",
    "        # Reshape back to image format\n",
    "        attention_output_reshaped = tf.reshape(attention_output, shape=(batch_size, height, width, self.channels_out))\n",
    "\n",
    "        # Apply final 1x1 convolution and scale by gamma\n",
    "        o = self.out_conv(attention_output_reshaped)\n",
    "        y = self.gamma * o + x # Additive skip connection\n",
    "\n",
    "        return y\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (self.channels_out,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Generator Model (with Self-Attention)\n",
    "\n",
    "def create_generator(latent_dim, output_shape): # output_shape (n_mels, n_frames) e.g., (80, 126)\n",
    "    \"\"\"Creates the generator model with Self-Attention.\"\"\"\n",
    "    n_mels, n_frames = output_shape\n",
    "    init_h, init_w = n_mels // 8, n_frames // 8 # Calculate initial dimensions based on 3 upsamples (2*2*2=8)\n",
    "    init_c = 128 # Initial channels\n",
    "\n",
    "    if init_h * 8 != n_mels or init_w * 8 != n_frames:\n",
    "         print(f\"Warning: Output shape {output_shape} might not be perfectly reached with 3 strides of 2. Adjusting initial size or layers.\")\n",
    "         # Adjust init_w slightly if needed, e.g. target 128 -> 16, target 126 -> 16 (trim later)\n",
    "         init_w = (n_frames + 7) // 8 # Ceiling division equivalent for width\n",
    "\n",
    "    nodes = init_h * init_w * init_c\n",
    "\n",
    "    model = Sequential(name='generator')\n",
    "    model.add(Input(shape=(latent_dim,)))\n",
    "\n",
    "    # Dense layer and reshape\n",
    "    model.add(Dense(nodes))\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2)) # Use negative_slope\n",
    "    model.add(Reshape((init_h, init_w, init_c))) # e.g., (10, 16, 128)\n",
    "\n",
    "    # Upsample 1: (10, 16, 128) -> (20, 32, 64)\n",
    "    model.add(Conv2DTranspose(init_c // 2, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False)) # Typically use_bias=False with Norm\n",
    "    model.add(LayerNormalization()) # Using LayerNorm instead of BatchNorm\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "    # Upsample 2: (20, 32, 64) -> (40, 64, 32)\n",
    "    model.add(Conv2DTranspose(init_c // 4, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "    # Add Self-Attention Layer Here (applied to 40x64 feature map)\n",
    "    # Note: Attention can be computationally expensive. Apply strategically.\n",
    "    model.add(SelfAttention(channels_out=init_c // 4)) # Keep channels the same\n",
    "    # model.add(LayerNormalization()) # Optional normalization after attention\n",
    "    # model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2)) # Optional activation after attention\n",
    "\n",
    "    # Upsample 3: (40, 64, 32) -> (80, 128, 1)\n",
    "    model.add(Conv2DTranspose(1, kernel_size=(4, 4), strides=(2, 2), padding='same', activation='tanh'))\n",
    "\n",
    "    # Final adjustment layer if needed (e.g., width 128 -> 126)\n",
    "    current_width = init_w * 8\n",
    "    if current_width != n_frames:\n",
    "        print(f\"Generator adding final Conv2D to adjust width from {current_width} to {n_frames}\")\n",
    "        # Calculate kernel size needed for 'valid' padding: K = W_in - W_out + 1\n",
    "        kernel_w = current_width - n_frames + 1\n",
    "        if kernel_w > 0:\n",
    "             model.add(Conv2D(1, kernel_size=(1, kernel_w), padding='valid', activation='tanh'))\n",
    "        else:\n",
    "            # This case shouldn't happen with the init_w calculation but handle just in case\n",
    "             print(f\"Warning: Could not adjust width with Conv2D. Current {current_width}, Target {n_frames}\")\n",
    "             # May need padding='same' and cropping layer if kernel_w is not positive\n",
    "\n",
    "    # Ensure final output shape is correct\n",
    "    # model.add(Reshape((n_mels, n_frames, 1))) # Add reshape just to be certain, though last Conv should handle it\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Critic (Discriminator) Model (with Self-Attention) - NO DROPOUT\n",
    "\n",
    "def create_critic(input_shape): # input_shape (n_mels, n_frames) e.g., (80, 126)\n",
    "    \"\"\"Creates the Critic model for WGAN-GP with Self-Attention. NO DROPOUT.\"\"\"\n",
    "    n_mels, n_frames = input_shape\n",
    "    model_input_shape = (n_mels, n_frames, 1) # Expects (80, 126, 1)\n",
    "\n",
    "    model = Sequential(name='critic')\n",
    "    model.add(Input(shape=model_input_shape))\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "    # model.add(Dropout(0.25)) # REMOVED\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Conv2D(128, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "    # model.add(Dropout(0.25)) # REMOVED\n",
    "\n",
    "    # Add Self-Attention Layer Here\n",
    "    model.add(SelfAttention(channels_out=128))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(Conv2D(256, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "    # model.add(Dropout(0.25)) # REMOVED (If you had one here)\n",
    "\n",
    "    # Flatten and Output Score\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Define the GAN Model (Not used for WGAN-GP training loop)\n",
    "# def create_gan(generator, discriminator, latent_dim):\n",
    "#     \"\"\"Creates the combined GAN model.\"\"\"\n",
    "#     # Make discriminator non-trainable\n",
    "#     discriminator.trainable = False\n",
    "#\n",
    "#     # Stack generator and discriminator\n",
    "#     gan_input = Input(shape=(latent_dim,))\n",
    "#     gan_output = discriminator(generator(gan_input))\n",
    "#     gan = Model(gan_input, gan_output)\n",
    "#\n",
    "#     return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743768917.534606  640800 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1904 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Output shape (80, 126) might not be perfectly reached with 3 strides of 2. Adjusting initial size or layers.\n",
      "Generator adding final Conv2D to adjust width from 128 to 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743768918.184631  640800 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of generated image (Generator Output): (1, 80, 126, 1)\n",
      "Shape of critic output: (1, 1)\n",
      "Expected shape for Critic Input: (80, 126, 1)\n",
      "\n",
      "--- Generator Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"generator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"generator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20480</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,068,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20480</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttention</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,377</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20480\u001b[0m)          │     \u001b[38;5;34m2,068,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20480\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m131,072\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m32,768\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention (\u001b[38;5;33mSelfAttention\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m2,377\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │           \u001b[38;5;34m513\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │             \u001b[38;5;34m4\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,235,406</span> (8.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,235,406\u001b[0m (8.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,235,406</span> (8.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,235,406\u001b[0m (8.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Critic Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"critic\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"critic\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,153</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttention</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40960</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,961</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m1,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m131,072\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m37,153\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttention\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m524,288\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40960\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m40,961\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">735,330</span> (2.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m735,330\u001b[0m (2.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">735,330</span> (2.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m735,330\u001b[0m (2.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 11: Data Path and Parameters\n",
    "\n",
    "# Data Paths\n",
    "train_data_path = 'datasetNEW/train'\n",
    "dev_data_path = 'datasetNEW/dev'\n",
    "eval_data_path = 'datasetNEW/eval'\n",
    "\n",
    "# Define the fixed number of frames\n",
    "TARGET_FRAMES = 126\n",
    "N_MELS = 80\n",
    "mel_spectrogram_shape = (N_MELS, TARGET_FRAMES)\n",
    "\n",
    "# WGAN-GP specific parameters\n",
    "latent_dim = 100\n",
    "n_critic = 5         # Train critic 5 times per generator update\n",
    "gp_weight = 10.0     # Gradient penalty weight\n",
    "gan_epochs = 40      # WGAN often needs more epochs, adjust as needed\n",
    "gan_batch_size = 8  # Adjust based on GPU memory (WGAN-GP can be memory intensive)\n",
    "\n",
    "# Optimizers (Typical WGAN settings: lower LR, betas=(0.5, 0.9))\n",
    "critic_optimizer = Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.9)\n",
    "generator_optimizer = Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.9)\n",
    "# critic_optimizer = RMSprop(learning_rate=0.00005) # Alternative optimizer\n",
    "# generator_optimizer = RMSprop(learning_rate=0.00005)\n",
    "\n",
    "# Create instances\n",
    "generator = create_generator(latent_dim, mel_spectrogram_shape)\n",
    "critic = create_critic(mel_spectrogram_shape)\n",
    "\n",
    "# Diagnostic Code: Verify Output Shape\n",
    "test_noise = tf.random.normal((1, latent_dim))\n",
    "generated_image = generator(test_noise, training=False) # Use tf.random and call model directly\n",
    "critic_output = critic(generated_image, training=False)\n",
    "print(\"Shape of generated image (Generator Output):\", generated_image.shape)\n",
    "print(\"Shape of critic output:\", critic_output.shape)\n",
    "\n",
    "critic_input_shape_expected = (mel_spectrogram_shape[0], mel_spectrogram_shape[1], 1)\n",
    "print(\"Expected shape for Critic Input:\", critic_input_shape_expected)\n",
    "assert generated_image.shape[1:] == critic_input_shape_expected, \"Generator output shape mismatch!\"\n",
    "assert len(critic_output.shape) == 2 and critic_output.shape[1] == 1, \"Critic output shape mismatch!\"\n",
    "\n",
    "\n",
    "# Report the models\n",
    "print(\"\\n--- Generator Summary ---\")\n",
    "generator.summary()\n",
    "print(\"\\n--- Critic Summary ---\")\n",
    "critic.summary()\n",
    "\n",
    "# Parameters for standalone classifier training (can be different)\n",
    "classifier_batch_size = 8 # Keep smaller for classifier fine-tuning?\n",
    "classifier_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated 2850 GAN steps per epoch.\n",
      "\n",
      "Begin WGAN-GP training!\n",
      "\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349f9b2b83b843ab8b748bcf8d5496dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 12: WGAN-GP Training Loop (Saving Critic Weights Only)\n",
    "\n",
    "# --- Imports ---\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for better Jupyter integration\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Assuming necessary variables (train_data_path, gan_batch_size, latent_dim, n_critic,\n",
    "# gp_weight, gan_epochs, critic_optimizer, generator_optimizer, generator, critic,\n",
    "# FIGURES_DIR) and functions (data_generator_gan) are defined and accessible\n",
    "# from previous cells.\n",
    "\n",
    "# --- Loss Functions ---\n",
    "def critic_loss(real_output, fake_output):\n",
    "    \"\"\"Wasserstein loss for the critic.\"\"\"\n",
    "    # Ensure outputs are float32 for stable loss calculation\n",
    "    real_output = tf.cast(real_output, tf.float32)\n",
    "    fake_output = tf.cast(fake_output, tf.float32)\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    \"\"\"Wasserstein loss for the generator.\"\"\"\n",
    "    # Ensure output is float32\n",
    "    fake_output = tf.cast(fake_output, tf.float32)\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "# --- Gradient Penalty Function (Corrected) ---\n",
    "def gradient_penalty(batch_size, real_images, fake_images):\n",
    "    \"\"\" Calculates the gradient penalty loss for WGAN GP, handling mixed precision. \"\"\"\n",
    "    # Ensure both images have the same dtype before interpolation\n",
    "    if real_images.dtype != fake_images.dtype:\n",
    "        real_images = tf.cast(real_images, fake_images.dtype)\n",
    "\n",
    "    # Generate interpolation alpha with the correct dtype\n",
    "    alpha_shape = [tf.shape(real_images)[0]] + [1] * (len(real_images.shape) - 1)\n",
    "    alpha = tf.random.uniform(shape=alpha_shape, minval=0., maxval=1., dtype=real_images.dtype)\n",
    "\n",
    "    # Interpolate images\n",
    "    interpolated = real_images + alpha * (fake_images - real_images)\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        pred = critic(interpolated, training=True)\n",
    "        pred = tf.cast(pred, tf.float32) # Cast prediction to float32 for stable GP calculation\n",
    "\n",
    "    grads = gp_tape.gradient(pred, [interpolated])\n",
    "    if grads is None or grads[0] is None:\n",
    "        logging.warning(\"Gradients are None in gradient_penalty. Returning 0 penalty.\")\n",
    "        # print(\"Warning: Gradients are None in gradient_penalty. Returning 0 penalty.\") # Optional print\n",
    "        return tf.constant(0.0, dtype=tf.float32)\n",
    "    grads = grads[0]\n",
    "    grads = tf.cast(grads, tf.float32) # Cast gradients to float32 before norm\n",
    "\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=tf.range(1, tf.rank(grads))))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "\n",
    "    return gp * gp_weight # gp_weight should be float\n",
    "\n",
    "\n",
    "# --- Data Generator and Steps Calculation ---\n",
    "train_gen_gan = data_generator_gan(train_data_path, batch_size=gan_batch_size)\n",
    "\n",
    "fake_files_count = 0\n",
    "try:\n",
    "    fake_dir = os.path.join(train_data_path, 'fake')\n",
    "    if os.path.exists(fake_dir):\n",
    "        fake_files_count = len([f for f in os.listdir(fake_dir) if f.endswith('.wav')])\n",
    "except FileNotFoundError:\n",
    "    fake_files_count = 0\n",
    "\n",
    "if gan_batch_size <= 0:\n",
    "    raise ValueError(\"GAN Batch size must be positive.\")\n",
    "if fake_files_count == 0:\n",
    "    print(\"Warning: No fake training files found for GAN. Setting GAN steps to 0.\")\n",
    "    gan_steps_per_epoch = 0\n",
    "else:\n",
    "    gan_steps_per_epoch = int(np.ceil(fake_files_count / float(gan_batch_size)))\n",
    "    print(f\"Calculated {gan_steps_per_epoch} GAN steps per epoch.\")\n",
    "\n",
    "\n",
    "# --- Training Step Function (Decorated with tf.function) ---\n",
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    current_batch_size = tf.shape(real_images)[0]\n",
    "    # Use float32 for noise if mixed precision is enabled, otherwise default might be okay\n",
    "    noise = tf.random.normal([current_batch_size, latent_dim], dtype=tf.float32)\n",
    "\n",
    "    # Train Critic (n_critic times)\n",
    "    total_crit_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "    for _ in tf.range(n_critic):\n",
    "        with tf.GradientTape() as crit_tape:\n",
    "            fake_images = generator(noise, training=True)\n",
    "            real_output = critic(real_images, training=True)\n",
    "            fake_output = critic(fake_images, training=True)\n",
    "            crit_loss = critic_loss(real_output, fake_output)\n",
    "            gp = gradient_penalty(current_batch_size, real_images, fake_images)\n",
    "            total_crit_loss = crit_loss + gp\n",
    "\n",
    "        crit_gradients = crit_tape.gradient(total_crit_loss, critic.trainable_variables)\n",
    "        valid_grads_and_vars = [(g, v) for g, v in zip(crit_gradients, critic.trainable_variables) if g is not None]\n",
    "        if len(valid_grads_and_vars) < len(critic.trainable_variables):\n",
    "             tf.print(\"Warning: Some critic gradients are None.\")\n",
    "        if len(valid_grads_and_vars) > 0:\n",
    "            critic_optimizer.apply_gradients(valid_grads_and_vars)\n",
    "\n",
    "    # Train Generator\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_images_gen = generator(noise, training=True)\n",
    "        fake_output_gen = critic(fake_images_gen, training=True)\n",
    "        gen_loss = generator_loss(fake_output_gen)\n",
    "\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    valid_grads_and_vars_gen = [(g, v) for g, v in zip(gen_gradients, generator.trainable_variables) if g is not None]\n",
    "    if len(valid_grads_and_vars_gen) < len(generator.trainable_variables):\n",
    "         tf.print(\"Warning: Some generator gradients are None.\")\n",
    "    if len(valid_grads_and_vars_gen) > 0:\n",
    "        generator_optimizer.apply_gradients(valid_grads_and_vars_gen)\n",
    "\n",
    "    return total_crit_loss, gen_loss\n",
    "\n",
    "\n",
    "# --- WGAN-GP Training Loop ---\n",
    "print(\"\\nBegin WGAN-GP training!\")\n",
    "\n",
    "if train_gen_gan is None or gan_steps_per_epoch == 0:\n",
    "    print(\"Skipping WGAN-GP training: Generator not initialized or no steps per epoch.\")\n",
    "else:\n",
    "    c_loss_history = []\n",
    "    g_loss_history = []\n",
    "\n",
    "    for epoch in range(gan_epochs): # Use gan_epochs defined in Cell 11\n",
    "        print(f\"\\nEpoch {epoch+1}/{gan_epochs}\")\n",
    "        epoch_pbar = tqdm(range(gan_steps_per_epoch), desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "        epoch_c_loss = 0.0\n",
    "        epoch_g_loss = 0.0\n",
    "        batches_processed = 0\n",
    "\n",
    "        for batch_idx in epoch_pbar:\n",
    "            try:\n",
    "                real_spoof_samples_np = next(train_gen_gan)\n",
    "                # Ensure input to train_step is float32\n",
    "                real_spoof_samples = tf.convert_to_tensor(real_spoof_samples_np, dtype=tf.float32)\n",
    "\n",
    "                if tf.shape(real_spoof_samples)[0] == 0:\n",
    "                    # print(f\"Warning: Skipped empty batch at step {batch_idx}\") # Optional print\n",
    "                    continue\n",
    "\n",
    "                c_loss, g_loss = train_step(real_spoof_samples)\n",
    "\n",
    "                # Check for NaN or Inf losses (important for stability)\n",
    "                if np.isnan(c_loss.numpy()) or np.isinf(c_loss.numpy()) or \\\n",
    "                   np.isnan(g_loss.numpy()) or np.isinf(g_loss.numpy()):\n",
    "                    print(f\"\\nError: NaN or Inf loss detected at epoch {epoch+1}, batch {batch_idx}. Stopping training.\")\n",
    "                    logging.error(f\"NaN/Inf loss detected: C Loss={c_loss.numpy()}, G Loss={g_loss.numpy()}. Epoch {epoch+1}, Batch {batch_idx}\")\n",
    "                    # Optional: Save current state before breaking if needed\n",
    "                    # generator.save_weights('generator_nan_inf.weights.h5')\n",
    "                    # critic.save_weights('critic_nan_inf.weights.h5')\n",
    "                    raise ValueError(\"NaN or Inf loss detected, stopping training.\") # Stop execution\n",
    "\n",
    "                epoch_c_loss += c_loss.numpy()\n",
    "                epoch_g_loss += g_loss.numpy()\n",
    "                batches_processed += 1\n",
    "\n",
    "                epoch_pbar.set_postfix({\"C Loss\": f\"{c_loss.numpy():.4f}\", \"G Loss\": f\"{g_loss.numpy():.4f}\"})\n",
    "\n",
    "            except StopIteration:\n",
    "                print(f\"\\nGAN Generator exhausted prematurely at batch {batch_idx}. Moving to next epoch.\")\n",
    "                train_gen_gan = data_generator_gan(train_data_path, batch_size=gan_batch_size)\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch NaN/Inf error from the check above or other errors\n",
    "                logging.error(f\"Error during WGAN training epoch {epoch+1}, batch {batch_idx}: {e}\", exc_info=True)\n",
    "                print(f\"\\nError during WGAN training epoch {epoch+1}, batch {batch_idx}: {e}\")\n",
    "                # Decide whether to continue or break based on the error type\n",
    "                if isinstance(e, ValueError) and \"NaN or Inf loss\" in str(e):\n",
    "                     break # Stop the outer loop for NaN/Inf\n",
    "                continue # Skip other problematic batches\n",
    "\n",
    "\n",
    "        # Check if loop was broken due to NaN/Inf\n",
    "        if np.isnan(epoch_c_loss) or np.isinf(epoch_c_loss) or np.isnan(epoch_g_loss) or np.isinf(epoch_g_loss):\n",
    "             print(\"Training stopped due to NaN/Inf loss.\")\n",
    "             break # Exit the epoch loop\n",
    "\n",
    "        # --- End-of-epoch actions ---\n",
    "        if batches_processed > 0:\n",
    "             avg_c_loss = epoch_c_loss / batches_processed\n",
    "             avg_g_loss = epoch_g_loss / batches_processed\n",
    "             c_loss_history.append(avg_c_loss)\n",
    "             g_loss_history.append(avg_g_loss)\n",
    "             print(f\"Epoch {epoch+1} finished. Avg C Loss: {avg_c_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}\")\n",
    "        else:\n",
    "             print(f\"Epoch {epoch+1} finished. No batches processed.\")\n",
    "             c_loss_history.append(np.nan)\n",
    "             g_loss_history.append(np.nan)\n",
    "\n",
    "\n",
    "        # --- Save Models/Weights Periodically ---\n",
    "        # Save every 5 epochs and at the very last epoch\n",
    "        if (epoch + 1) % 5 == 0 or (epoch + 1) == gan_epochs:\n",
    "             try:\n",
    "                 # Save generator as full model (assuming it doesn't have loading issues)\n",
    "                 generator.save(f'generator_wgan_sa_epoch_{epoch+1}.keras')\n",
    "\n",
    "                 # Save critic weights ONLY\n",
    "                 critic_weights_filename = f'critic_wgan_sa_epoch_{epoch+1}.weights.h5' # Define filename\n",
    "                 critic.save_weights(critic_weights_filename) # Use save_weights\n",
    "\n",
    "                 print(f\"Saved generator model and critic weights ({critic_weights_filename}) for epoch {epoch+1}\")\n",
    "             except Exception as e:\n",
    "                 logging.error(f\"Error saving models/weights at epoch {epoch+1}: {e}\", exc_info=True)\n",
    "                 print(f\"Error saving models/weights at epoch {epoch+1}: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\nWGAN-GP training finished (or stopped early).\")\n",
    "\n",
    "    # Final loss plot\n",
    "    if c_loss_history and g_loss_history: # Plot only if history exists\n",
    "         plt.figure(figsize=(10, 5))\n",
    "         # Filter out NaN values for plotting if training stopped early\n",
    "         epochs_ran = range(1, len([loss for loss in c_loss_history if not np.isnan(loss)]) + 1)\n",
    "         plt.plot(epochs_ran, [loss for loss in c_loss_history if not np.isnan(loss)], label='Avg Critic Loss per Epoch')\n",
    "         plt.plot(epochs_ran, [loss for loss in g_loss_history if not np.isnan(loss)], label='Avg Generator Loss per Epoch')\n",
    "         plt.title('WGAN-GP Training Losses')\n",
    "         plt.xlabel('Epoch')\n",
    "         plt.ylabel('Average Loss')\n",
    "         plt.legend()\n",
    "         plt.grid(True)\n",
    "         plot_filename = os.path.join(FIGURES_DIR, 'gan_loss_final.png')\n",
    "         try:\n",
    "              plt.savefig(plot_filename)\n",
    "              print(f\"Saved final loss plot to {plot_filename}\")\n",
    "         except Exception as e:\n",
    "              logging.error(f\"Failed to save final loss plot: {e}\", exc_info=True)\n",
    "              print(f\"Failed to save final loss plot: {e}\")\n",
    "         plt.show()\n",
    "    else:\n",
    "         print(\"No loss history recorded, skipping final plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Train Standalone Classifier (using Critic Weights)\n",
    "\n",
    "# --- Imports needed specifically for this cell's logic ---\n",
    "import fnmatch # For finding model files\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model # Need Model for functional API approach\n",
    "from tensorflow.keras.layers import Dense, Input # Need Dense and Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Assuming other necessary variables (train_data_path, dev_data_path, classifier_batch_size,\n",
    "# mel_spectrogram_shape=(N_MELS, TARGET_FRAMES), classifier_epochs, FIGURES_DIR) and functions\n",
    "# (data_generator_classifier, count_total_files, SelfAttention, create_critic) are defined and\n",
    "# accessible from previous cells. create_critic is now essential.\n",
    "\n",
    "\n",
    "# --- Custom Callback for Plotting Training History ---\n",
    "# ... (PlotTrainingHistory class definition - KEEP AS IS from previous version) ...\n",
    "class PlotTrainingHistory(Callback):\n",
    "    def __init__(self, model_name='model'):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.auc = [] # Added to track AUC\n",
    "        self.val_auc = [] # Added to track validation AUC\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('accuracy'))\n",
    "        self.val_acc.append(logs.get('val_accuracy'))\n",
    "        self.auc.append(logs.get('auc')) # Get AUC\n",
    "        self.val_auc.append(logs.get('val_auc')) # Get val_AUC\n",
    "\n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(18, 5))\n",
    "\n",
    "        # Accuracy Plot\n",
    "        plt.subplot(1, 3, 1)\n",
    "        if any(v is not None for v in self.acc):\n",
    "            plt.plot(self.acc, label='Training Accuracy')\n",
    "        if any(v is not None for v in self.val_acc):\n",
    "            plt.plot(self.val_acc, label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        if any(v is not None for v in self.acc) or any(v is not None for v in self.val_acc):\n",
    "            plt.legend()\n",
    "\n",
    "        # Loss Plot\n",
    "        plt.subplot(1, 3, 2)\n",
    "        if any(v is not None for v in self.loss):\n",
    "            plt.plot(self.loss, label='Training Loss')\n",
    "        if any(v is not None for v in self.val_loss):\n",
    "             plt.plot(self.val_loss, label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        if any(v is not None for v in self.loss) or any(v is not None for v in self.val_loss):\n",
    "            plt.legend()\n",
    "\n",
    "        # AUC Plot\n",
    "        plt.subplot(1, 3, 3)\n",
    "        if any(v is not None for v in self.auc):\n",
    "             plt.plot(self.auc, label='Training AUC')\n",
    "        if any(v is not None for v in self.val_auc):\n",
    "             plt.plot(self.val_auc, label='Validation AUC')\n",
    "        plt.title('Model AUC')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('AUC')\n",
    "        if any(v is not None for v in self.auc) or any(v is not None for v in self.val_auc):\n",
    "            plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        filepath = os.path.join(FIGURES_DIR, f'{self.model_name}_epoch_{epoch+1}.png')\n",
    "        try:\n",
    "             plt.savefig(filepath)\n",
    "             plt.close()\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error saving plot to {filepath}: {e}\")\n",
    "             print(f\"\\nError saving plot to {filepath}: {e}\")\n",
    "             plt.close()\n",
    "# --- END OF CLASS DEFINITION ---\n",
    "\n",
    "\n",
    "print(\"\\nSetting up Standalone Classifier training...\")\n",
    "\n",
    "# --- Create Data Generators for Classifier ---\n",
    "train_gen_clf = data_generator_classifier(train_data_path, batch_size=classifier_batch_size)\n",
    "dev_gen_clf = data_generator_classifier(dev_data_path, batch_size=classifier_batch_size, shuffle=False)\n",
    "\n",
    "# --- Calculate steps for Classifier Training ---\n",
    "train_samples_count = count_total_files(train_data_path)\n",
    "dev_samples_count = count_total_files(dev_data_path)\n",
    "\n",
    "if classifier_batch_size <= 0:\n",
    "    raise ValueError(\"Classifier batch size must be positive.\")\n",
    "\n",
    "clf_steps_per_epoch = int(np.ceil(train_samples_count / float(classifier_batch_size))) if train_samples_count > 0 else 0\n",
    "clf_validation_steps = int(np.ceil(dev_samples_count / float(classifier_batch_size))) if dev_samples_count > 0 else 0\n",
    "\n",
    "print(f\"Classifier Train Steps/Epoch: {clf_steps_per_epoch}, Validation Steps: {clf_validation_steps}\")\n",
    "\n",
    "\n",
    "# --- Recreate Critic Structure and Load Weights ---\n",
    "critic_weights_path = None\n",
    "critic_for_classifier = None # Use a new variable name\n",
    "\n",
    "# 1. Recreate the critic structure\n",
    "try:\n",
    "    # Ensure mel_spectrogram_shape is defined, e.g., (80, 126)\n",
    "    print(f\"Recreating critic structure using shape: {mel_spectrogram_shape}\")\n",
    "    # create_critic function must be available from Cell 9\n",
    "    critic_for_classifier = create_critic(mel_spectrogram_shape)\n",
    "    print(\"Critic structure recreated successfully.\")\n",
    "except NameError as ne:\n",
    "     logging.error(f\"Failed to recreate critic structure: {ne}. 'create_critic' or 'mel_spectrogram_shape' not defined?\", exc_info=True)\n",
    "     raise NameError(f\"Failed to recreate critic structure: {ne}. Ensure 'create_critic' and 'mel_spectrogram_shape' are defined.\")\n",
    "except Exception as ce:\n",
    "     logging.error(f\"Failed to recreate critic structure: {ce}\", exc_info=True)\n",
    "     raise RuntimeError(f\"Failed to recreate critic structure: {ce}\")\n",
    "\n",
    "\n",
    "# 2. Find the latest critic weights file\n",
    "try:\n",
    "    critic_weights_pattern = 'critic_wgan_sa_epoch_*.weights.h5'\n",
    "    list_of_critic_weights_files = [f for f in os.listdir('.') if fnmatch.fnmatch(f, critic_weights_pattern)]\n",
    "    if list_of_critic_weights_files:\n",
    "        latest_weights_file = max(list_of_critic_weights_files, key=os.path.getctime)\n",
    "        critic_weights_path = latest_weights_file\n",
    "        print(f\"Found latest critic weights file: {critic_weights_path}\")\n",
    "    else:\n",
    "        print(\"Warning: No critic weights file found matching pattern.\")\n",
    "        print(\"Proceeding with randomly initialized critic weights for the classifier.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error finding critic weights files: {e}\", exc_info=True)\n",
    "    print(f\"Error finding critic weights files: {e}\")\n",
    "    print(\"Proceeding with randomly initialized critic weights for the classifier.\")\n",
    "\n",
    "\n",
    "# 3. Load weights if found\n",
    "if critic_weights_path:\n",
    "    try:\n",
    "        print(f\"Loading weights from {critic_weights_path} into recreated critic structure...\")\n",
    "        critic_for_classifier.load_weights(critic_weights_path)\n",
    "        print(\"Successfully loaded critic weights.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading weights from {critic_weights_path}: {e}\", exc_info=True)\n",
    "        print(f\"Error loading weights from {critic_weights_path}: {e}\")\n",
    "        print(\"Classifier training will proceed with initialized weights.\")\n",
    "# If weights not found or failed to load, critic_for_classifier will just have its initial random weights\n",
    "\n",
    "\n",
    "# --- Build the Spoof Detector Model using the Critic with Loaded Weights ---\n",
    "print(\"Building classifier using critic layers...\")\n",
    "\n",
    "# Make the critic layers trainable for fine-tuning\n",
    "critic_for_classifier.trainable = True\n",
    "\n",
    "# 1. Get the expected input shape\n",
    "try:\n",
    "    if len(mel_spectrogram_shape) == 2:\n",
    "        critic_input_shape = (mel_spectrogram_shape[0], mel_spectrogram_shape[1], 1)\n",
    "    else:\n",
    "        critic_input_shape = mel_spectrogram_shape\n",
    "    print(f\"Using input shape for new model: {critic_input_shape}\")\n",
    "except NameError:\n",
    "    logging.error(\"Variable 'mel_spectrogram_shape' is not defined.\")\n",
    "    raise NameError(\"Variable 'mel_spectrogram_shape' is not defined. Cannot determine input shape.\")\n",
    "\n",
    "# 2. Create a new Input layer\n",
    "new_input = Input(shape=critic_input_shape, name=\"classifier_input\")\n",
    "\n",
    "# 3. Sequentially connect the layers from the recreated critic (excluding the original output layer)\n",
    "x = new_input\n",
    "processed_layers_count = 0\n",
    "for layer in critic_for_classifier.layers[:-1]: # Iterate through all layers EXCEPT the last one\n",
    "    try:\n",
    "        layer.trainable = True # Ensure layers are trainable in the new model\n",
    "        x = layer(x)\n",
    "        processed_layers_count += 1\n",
    "    except Exception as layer_e:\n",
    "        logging.error(f\"Error connecting layer '{layer.name}': {layer_e}\", exc_info=True)\n",
    "        print(f\"Error connecting layer '{layer.name}': {layer_e}\")\n",
    "        raise RuntimeError(f\"Failed to rebuild model at layer: {layer.name}\")\n",
    "\n",
    "print(f\"Connected {processed_layers_count} layers from the critic.\")\n",
    "if processed_layers_count == 0:\n",
    "    raise ValueError(\"No layers were processed from the critic. Cannot build classifier.\")\n",
    "\n",
    "# 4. Add the final binary classification layer\n",
    "classifier_output = Dense(1, activation='sigmoid', name='classifier_output')(x)\n",
    "\n",
    "# 5. Create the final Functional API model\n",
    "spoof_detector = Model(inputs=new_input, outputs=classifier_output, name='spoof_detector')\n",
    "\n",
    "\n",
    "print(\"\\n--- Spoof Detector (Classifier) Summary ---\")\n",
    "spoof_detector.summary()\n",
    "\n",
    "\n",
    "# --- Callbacks for Classifier Training ---\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True, verbose=1\n",
    ")\n",
    "\n",
    "checkpoint_dir_clf = './training_checkpoints_spoof_detector_wgan_sa'\n",
    "os.makedirs(checkpoint_dir_clf, exist_ok=True)\n",
    "checkpoint_prefix_clf = os.path.join(checkpoint_dir_clf, \"ckpt_clf_{epoch:02d}.weights.h5\")\n",
    "\n",
    "# Checkpoint callback saves only weights\n",
    "checkpoint_callback_clf = ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix_clf,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_auc',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_training_callback = PlotTrainingHistory(model_name='spoof_detector_wgan_sa')\n",
    "\n",
    "\n",
    "# --- Compile and Train the Classifier ---\n",
    "classifier_optimizer = Adam(learning_rate=1e-5)\n",
    "spoof_detector.compile(\n",
    "    optimizer=classifier_optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Standalone Classifier training...\")\n",
    "\n",
    "history = None # Initialize history\n",
    "if clf_steps_per_epoch > 0 and clf_validation_steps > 0:\n",
    "    try:\n",
    "        history = spoof_detector.fit(\n",
    "            train_gen_clf,\n",
    "            steps_per_epoch=clf_steps_per_epoch,\n",
    "            epochs=classifier_epochs,\n",
    "            validation_data=dev_gen_clf,\n",
    "            validation_steps=clf_validation_steps,\n",
    "            callbacks=[reduce_lr, early_stopping, checkpoint_callback_clf, plot_training_callback],\n",
    "        )\n",
    "    except Exception as fit_e:\n",
    "        logging.error(f\"Error during classifier training (model.fit): {fit_e}\", exc_info=True)\n",
    "        print(f\"\\n--- ERROR DURING CLASSIFIER TRAINING ---\")\n",
    "        print(f\"Error: {fit_e}\")\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "\n",
    "    # --- Save the Final Trained Classifier Model ---\n",
    "    # Since loading the full model was problematic, saving only weights might be safer here too,\n",
    "    # although saving the full model *might* work now that it's built differently.\n",
    "    # Let's try saving the full model first, with a fallback.\n",
    "    if history is not None:\n",
    "        final_model_path = 'spoof_detector_final_wgan_sa.keras'\n",
    "        try:\n",
    "            # Need custom_objects because SelfAttention is still part of the model layers\n",
    "            custom_objects = {'SelfAttention': SelfAttention}\n",
    "            spoof_detector.save(final_model_path, custom_objects=custom_objects)\n",
    "            print(f\"\\nClassifier training complete or stopped. Final model saved as {final_model_path}\")\n",
    "        except Exception as e:\n",
    "             logging.error(f\"Error saving final classifier model (full): {e}\", exc_info=True)\n",
    "             print(f\"\\nError saving final classifier model (full): {e}. Saving weights only as fallback.\")\n",
    "             spoof_detector.save_weights('spoof_detector_final_wgan_sa.weights.h5')\n",
    "\n",
    "        # --- Optional: Load Best Weights and Save Best Model ---\n",
    "        try:\n",
    "            best_checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir_clf)\n",
    "            if best_checkpoint_path:\n",
    "                print(f\"Loading best classifier weights from checkpoint: {best_checkpoint_path}\")\n",
    "                # Load the best weights into the current model structure\n",
    "                spoof_detector.load_weights(best_checkpoint_path) # load_weights works reliably\n",
    "\n",
    "                # Try saving the full model with best weights\n",
    "                best_model_path = 'spoof_detector_best_val_auc_wgan_sa.keras'\n",
    "                custom_objects = {'SelfAttention': SelfAttention}\n",
    "                spoof_detector.save(best_model_path, custom_objects=custom_objects)\n",
    "                print(f\"Saved classifier with best validation AUC weights as {best_model_path}\")\n",
    "            else:\n",
    "                print(\"Could not find best checkpoint weights. The final model saved corresponds to the last epoch.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading best weights or saving best model: {e}\", exc_info=True)\n",
    "            print(f\"Error loading best weights or saving best model: {e}\")\n",
    "            # Optionally save just the best weights if saving the full model failed\n",
    "            if best_checkpoint_path:\n",
    "                 try:\n",
    "                      spoof_detector.save_weights('spoof_detector_best_val_auc_wgan_sa.weights.h5')\n",
    "                      print(\"Saved best weights only as spoof_detector_best_val_auc_wgan_sa.weights.h5\")\n",
    "                 except Exception as e_ws:\n",
    "                      print(f\"Failed to save best weights only: {e_ws}\")\n",
    "    else:\n",
    "        print(\"\\nClassifier training did not run or failed. No final model saved.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping classifier training because steps_per_epoch or validation_steps is zero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Evaluation (Improved Error Handling)\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np # Needed for count_total_files if redefined\n",
    "from tensorflow.keras.models import load_model # Ensure load_model is imported\n",
    "\n",
    "# --- Ensure SelfAttention Class Definition is Available ---\n",
    "# Copy the class definition from Cell 7 here for robustness,\n",
    "# especially if the kernel might have been restarted.\n",
    "\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Self-attention layer based on SAGAN.\n",
    "    Input shape: (batch, height, width, channels)\n",
    "    Output shape: (batch, height, width, channels_out) where channels_out is typically channels\n",
    "    \"\"\"\n",
    "    def __init__(self, channels_out=None, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_channels = input_shape[-1]\n",
    "        if self.channels_out is None:\n",
    "            self.channels_out = self.input_channels\n",
    "\n",
    "        # Convolution layers for query, key, value\n",
    "        self.f = tf.keras.layers.Conv2D(self.input_channels // 8, kernel_size=1, strides=1, padding='same', name='conv_f') # Query\n",
    "        self.g = tf.keras.layers.Conv2D(self.input_channels // 8, kernel_size=1, strides=1, padding='same', name='conv_g') # Key\n",
    "        self.h = tf.keras.layers.Conv2D(self.channels_out, kernel_size=1, strides=1, padding='same', name='conv_h')        # Value\n",
    "        self.out_conv = tf.keras.layers.Conv2D(self.channels_out, kernel_size=1, strides=1, padding='same', name='conv_out')\n",
    "        self.gamma = self.add_weight(name='gamma', shape=(1,), initializer='zeros', trainable=True)\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size, height, width, num_channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        location_num = height * width\n",
    "        f_proj = self.f(x)\n",
    "        g_proj = self.g(x)\n",
    "        h_proj = self.h(x)\n",
    "        f_flat = tf.reshape(f_proj, shape=(batch_size, location_num, self.input_channels // 8))\n",
    "        g_flat = tf.reshape(g_proj, shape=(batch_size, location_num, self.input_channels // 8))\n",
    "        h_flat = tf.reshape(h_proj, shape=(batch_size, location_num, self.channels_out))\n",
    "        g_flat_t = tf.transpose(g_flat, perm=[0, 2, 1])\n",
    "        attention_score = tf.matmul(f_flat, g_flat_t)\n",
    "        attention_prob = tf.nn.softmax(attention_score, axis=-1)\n",
    "        attention_output = tf.matmul(attention_prob, h_flat)\n",
    "        attention_output_reshaped = tf.reshape(attention_output, shape=(batch_size, height, width, self.channels_out))\n",
    "        o = self.out_conv(attention_output_reshaped)\n",
    "        y = self.gamma * o + x\n",
    "        return y\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (self.channels_out,)\n",
    "\n",
    "    # Add get_config for proper saving/loading if needed (optional for this fix but good practice)\n",
    "    def get_config(self):\n",
    "        config = super(SelfAttention, self).get_config()\n",
    "        config.update({\"channels_out\": self.channels_out})\n",
    "        return config\n",
    "\n",
    "# --- END OF SelfAttention DEFINITION ---\n",
    "\n",
    "\n",
    "print(\"\\nEvaluating the final Spoof Detector...\")\n",
    "\n",
    "# Define expected model paths\n",
    "best_model_filename = 'spoof_detector_best_val_auc_wgan_sa.keras'\n",
    "final_model_filename = 'spoof_detector_final_wgan_sa.keras'\n",
    "detector_model_path = None\n",
    "spoof_detector_eval = None # Initialize to None\n",
    "\n",
    "# --- Check for Model Files and Attempt Loading ---\n",
    "print(\"Checking for model files...\")\n",
    "if os.path.exists(best_model_filename):\n",
    "    print(f\"Found best model file: {best_model_filename}\")\n",
    "    detector_model_path = best_model_filename\n",
    "elif os.path.exists(final_model_filename):\n",
    "    print(f\"Best model not found. Found final model file: {final_model_filename}\")\n",
    "    detector_model_path = final_model_filename\n",
    "else:\n",
    "    print(f\"Error: Could not find '{best_model_filename}' or '{final_model_filename}' in the current directory.\")\n",
    "    print(\"Please ensure Cell 13 completed successfully and saved the model.\")\n",
    "\n",
    "# Proceed only if a model file was found\n",
    "if detector_model_path:\n",
    "    try:\n",
    "        print(f\"\\nAttempting to load model: {detector_model_path}\")\n",
    "        custom_objects = {'SelfAttention': SelfAttention}\n",
    "        spoof_detector_eval = load_model(\n",
    "            detector_model_path,\n",
    "            custom_objects=custom_objects\n",
    "        )\n",
    "        print(\"Model loaded successfully.\")\n",
    "\n",
    "        # Re-compile the model after loading (important for evaluation)\n",
    "        # Use the same optimizer settings as in Cell 13 if continuing training,\n",
    "        # but for evaluation, just compiling with loss/metrics is usually enough.\n",
    "        print(\"Compiling loaded model...\")\n",
    "        spoof_detector_eval.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), # Use a dummy optimizer or same as training\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "        )\n",
    "        print(\"Model compiled successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- ERROR DURING MODEL LOADING OR COMPILATION ---\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print detailed traceback\n",
    "        print(\"----------------------------------------------------\")\n",
    "        print(\"Evaluation cannot proceed due to loading/compilation error.\")\n",
    "        spoof_detector_eval = None # Ensure it's None on error\n",
    "\n",
    "# --- Perform Evaluation if Model Loaded ---\n",
    "if spoof_detector_eval:\n",
    "    print(\"\\nProceeding with evaluation...\")\n",
    "    # --- Create Evaluation Generator ---\n",
    "    # Ensure data_generator_classifier and classifier_batch_size are defined\n",
    "    eval_gen_clf = data_generator_classifier(eval_data_path, batch_size=classifier_batch_size, shuffle=False)\n",
    "\n",
    "    # --- Calculate Evaluation Steps ---\n",
    "    # Ensure count_total_files and eval_data_path are defined\n",
    "    eval_samples_count = count_total_files(eval_data_path)\n",
    "    if classifier_batch_size <= 0:\n",
    "        raise ValueError(\"Classifier batch size must be positive.\")\n",
    "    eval_steps = int(np.ceil(eval_samples_count / float(classifier_batch_size))) if eval_samples_count > 0 else 0\n",
    "    print(f\"Using {eval_steps} steps for evaluation.\")\n",
    "\n",
    "    # --- Evaluate ---\n",
    "    if eval_steps > 0:\n",
    "        print(\"Running model.evaluate...\")\n",
    "        results = spoof_detector_eval.evaluate(\n",
    "            eval_gen_clf,\n",
    "            steps=eval_steps,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Evaluation Results ---\")\n",
    "        # Assuming results format [loss, accuracy, auc] based on compile metrics\n",
    "        if isinstance(results, list) and len(results) >= 3:\n",
    "             print(f\"Loss: {results[0]:.4f}\")\n",
    "             print(f\"Accuracy: {results[1]:.4f}\")\n",
    "             print(f\"AUC: {results[2]:.4f}\")\n",
    "        else:\n",
    "             print(f\"Raw Results: {results}\") # Print raw results if format is unexpected\n",
    "        print(\"--------------------------\")\n",
    "    else:\n",
    "        print(\"Skipping evaluation because eval_steps is zero (no evaluation data found?).\")\n",
    "\n",
    "else:\n",
    "    # This message will now appear if file not found OR if loading failed\n",
    "    print(\"\\nSkipping evaluation because the spoof detector model could not be loaded.\")\n",
    "\n",
    "# --- End of Cell 14 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Reporting (Confusion Matrix, EER, t-DCF)\n",
    "\n",
    "print(\"\\nGenerating Final Reports (F1, CM, EER, t-DCF)...\")\n",
    "\n",
    "# Ensure the evaluation model is loaded from Cell 14\n",
    "if 'spoof_detector_eval' not in locals() or spoof_detector_eval is None:\n",
    "    print(\"Spoof detector model not loaded. Cannot generate reports.\")\n",
    "else:\n",
    "    # --- Parameters for t-DCF ---\n",
    "    p_target = 0.05  # Prior probability of target (real=1) - Adjust based on ASVspoof challenge or use case\n",
    "    c_miss = 1       # Cost of miss (classifying real as fake - FN)\n",
    "    c_false_alarm = 1 # Cost of false alarm (classifying fake as real - FP)\n",
    "\n",
    "    # --- Regenerate Predictions ---\n",
    "    # Reset the eval generator\n",
    "    eval_gen_report = data_generator_classifier(eval_data_path, batch_size=classifier_batch_size, shuffle=False)\n",
    "\n",
    "    # Recalculate eval_steps if needed (should match Cell 14)\n",
    "    eval_samples_count_report = count_total_files(eval_data_path)\n",
    "    eval_steps_report = int(np.ceil(eval_samples_count_report / float(classifier_batch_size))) if eval_samples_count_report > 0 else 0\n",
    "\n",
    "    y_pred_scores = []\n",
    "    y_true_labels = []\n",
    "\n",
    "    if eval_steps_report > 0:\n",
    "        print(f\"Generating predictions using {eval_steps_report} steps...\")\n",
    "        for _ in tqdm(range(eval_steps_report), desc=\"Predicting\"):\n",
    "            try:\n",
    "                # Generator yields (batch_x, batch_y, batch_weights)\n",
    "                batch_x, batch_y, _ = next(eval_gen_report)\n",
    "                if batch_x.size == 0: continue # Skip empty batches if they occur\n",
    "\n",
    "                # Use predict, not predict_on_batch for potentially better performance over many batches\n",
    "                batch_pred = spoof_detector_eval.predict(batch_x, verbose=0)\n",
    "                y_pred_scores.extend(batch_pred.flatten())\n",
    "                y_true_labels.extend(batch_y)\n",
    "            except StopIteration:\n",
    "                print(\"Evaluation generator stopped.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error during prediction generation: {e}\")\n",
    "                continue\n",
    "\n",
    "        y_pred_scores = np.array(y_pred_scores).astype(np.float32)\n",
    "        y_true_labels = np.array(y_true_labels).astype(np.int32)\n",
    "\n",
    "        # Ensure lengths match (might be off if last batch was incomplete and generator didn't handle it perfectly)\n",
    "        min_len = min(len(y_pred_scores), len(y_true_labels))\n",
    "        if min_len == 0:\n",
    "            print(\"No predictions or labels collected. Cannot generate reports.\")\n",
    "        else:\n",
    "            y_pred_scores = y_pred_scores[:min_len]\n",
    "            y_true_labels = y_true_labels[:min_len]\n",
    "\n",
    "            # --- Calculations ---\n",
    "            # Binary predictions for F1 and CM\n",
    "            y_pred_binary = (y_pred_scores > 0.5).astype(int)\n",
    "\n",
    "            # F1 Score\n",
    "            f1 = f1_score(y_true_labels, y_pred_binary)\n",
    "            print(f\"\\nF1 Score (threshold 0.5): {f1:.4f}\")\n",
    "\n",
    "            # Confusion Matrix\n",
    "            cm = confusion_matrix(y_true_labels, y_pred_binary)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake (0)', 'Real (1)'], yticklabels=['Fake (0)', 'Real (1)'])\n",
    "            plt.title('Confusion Matrix (Counts)')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, 'confusion_matrix_counts_wgan_sa.png'))\n",
    "            plt.show()\n",
    "\n",
    "            # Confusion Matrix (Percentages)\n",
    "            cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "            cm_perc = cm / cm_sum.astype(float) * 100\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm_perc, annot=True, fmt='.2f', cmap='Greens', xticklabels=['Fake (0)', 'Real (1)'], yticklabels=['Fake (0)', 'Real (1)'])\n",
    "            plt.title('Confusion Matrix (Row Percentages)')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, 'confusion_matrix_perc_wgan_sa.png'))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            # EER Calculation\n",
    "            fpr, tpr, thresholds_roc = roc_curve(y_true_labels, y_pred_scores, pos_label=1)\n",
    "            fnr = 1 - tpr\n",
    "            eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
    "            eer_threshold = thresholds_roc[eer_index]\n",
    "            eer = fpr[eer_index] # Or use (fpr[eer_index] + fnr[eer_index]) / 2\n",
    "            print(f\"EER: {eer:.4f} at threshold {eer_threshold:.4f}\")\n",
    "\n",
    "            # Plot ROC Curve\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(fpr, tpr, label=f'ROC curve (AUC = {results[2]:.4f})') # Use AUC from evaluate\n",
    "            plt.plot(fpr, fnr, label='FN Rate')\n",
    "            plt.plot([0, 1], [0, 1], 'k--') # Random guess line\n",
    "            plt.scatter(fpr[eer_index], tpr[eer_index], color='red', zorder=5, label=f'EER = {eer:.4f}')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate (FPR)')\n",
    "            plt.ylabel('True Positive Rate (TPR)')\n",
    "            plt.title('Receiver Operating Characteristic (ROC)')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, 'roc_curve_wgan_sa.png'))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            # t-DCF Calculation\n",
    "            # Define function (can be moved to utils if used often)\n",
    "            def calculate_t_dcf(y_true, y_scores, p_target, c_miss, c_fa, thresholds):\n",
    "                \"\"\"Calculates normalized t-DCF for a range of thresholds.\"\"\"\n",
    "                num_thresholds = len(thresholds)\n",
    "                dcf_values = np.zeros(num_thresholds)\n",
    "\n",
    "                num_real = np.sum(y_true == 1)\n",
    "                num_fake = np.sum(y_true == 0)\n",
    "\n",
    "                if num_real == 0 or num_fake == 0:\n",
    "                     print(\"Warning: Cannot calculate t-DCF with zero samples in a class.\")\n",
    "                     return np.inf # Return infinity or handle as error\n",
    "\n",
    "                for i, thr in enumerate(thresholds):\n",
    "                    y_pred_binary = (y_scores >= thr).astype(int)\n",
    "                    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary, labels=[0, 1]).ravel()\n",
    "\n",
    "                    p_miss = fn / num_real if num_real > 0 else 0 # Miss Rate (FN / Total Real)\n",
    "                    p_fa = fp / num_fake if num_fake > 0 else 0   # False Alarm Rate (FP / Total Fake)\n",
    "\n",
    "                    cost = (c_miss * p_miss * p_target) + (c_fa * p_fa * (1 - p_target))\n",
    "\n",
    "                    # Normalize the cost\n",
    "                    dcf_norm = min(c_miss * p_target, c_fa * (1 - p_target))\n",
    "                    dcf_values[i] = cost / dcf_norm if dcf_norm > 0 else cost # Avoid division by zero\n",
    "\n",
    "                # Find the minimum t-DCF\n",
    "                min_dcf_index = np.argmin(dcf_values)\n",
    "                min_dcf = dcf_values[min_dcf_index]\n",
    "                min_dcf_threshold = thresholds[min_dcf_index]\n",
    "\n",
    "                return min_dcf, min_dcf_threshold\n",
    "\n",
    "            # Calculate min t-DCF over a range of thresholds (more robust than just EER threshold)\n",
    "            # Use ROC thresholds, but filter out +/- inf if present\n",
    "            valid_thresholds = thresholds_roc[np.isfinite(thresholds_roc)]\n",
    "            min_tdcf, min_tdcf_thresh = calculate_t_dcf(y_true_labels, y_pred_scores, p_target, c_miss, c_false_alarm, valid_thresholds)\n",
    "\n",
    "            print(f\"Minimum t-DCF: {min_tdcf:.4f} at threshold {min_tdcf_thresh:.4f}\")\n",
    "\n",
    "    else:\n",
    "         print(\"Skipping report generation because eval_steps_report is zero.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
