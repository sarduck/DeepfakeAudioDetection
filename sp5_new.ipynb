{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 15:09:22.851022: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 15:09:22.864246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744191562.878644 1164916 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744191562.882990 1164916 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744191562.894963 1164916 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744191562.894981 1164916 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744191562.894982 1164916 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744191562.894983 1164916 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-09 15:09:22.898496: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLA JIT compilation disabled.\n",
      "TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, GRU, Dense, Dropout, BatchNormalization, LayerNormalization, Reshape, Permute, Bidirectional, Add, Attention, Flatten, TimeDistributed, Conv2DTranspose, Conv2D, Layer, Concatenate, Multiply, AdditiveAttention # Added Multiply, AdditiveAttention for potential SelfAttention implementation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop # Added RMSprop as an option for WGAN\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve # Moved confusion_matrix here\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "# import noisereduce as nr # Consider if still needed/effective\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, sosfilt\n",
    "import logging\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for better Jupyter integration\n",
    "import random\n",
    "import seaborn as sns # Added for confusion matrix plotting\n",
    "tf.config.optimizer.set_jit(False)\n",
    "print(\"XLA JIT compilation disabled.\")\n",
    "\n",
    "# WGAN-GP specific\n",
    "from tensorflow import GradientTape\n",
    "\n",
    "# Create directory for saving figures\n",
    "FIGURES_DIR = 'training_figures_wgan_sa' # Changed dir name\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='audio_errors_wgan_sa.log', level=logging.ERROR,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and configured.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Force GPU usage\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"GPU is available and configured.\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(f\"Error configuring GPU: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices found. Falling back to CPU.\")\n",
    "\n",
    "# Set mixed precision policy if desired (can speed up training on compatible GPUs)\n",
    "#from tensorflow.keras import mixed_precision\n",
    "#policy = mixed_precision.Policy('mixed_float16')\n",
    "#mixed_precision.set_global_policy(policy)\n",
    "#print('Mixed precision enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Audio Loading and Preprocessing Function\n",
    "def load_and_preprocess_audio(file_path, sr=16000, duration=4):\n",
    "    try:\n",
    "        # Load audio, potentially reducing noise first if beneficial\n",
    "        audio, current_sr = librosa.load(file_path, sr=None, duration=duration) # Load native SR first\n",
    "\n",
    "        # Optional: Noise Reduction (experiment if needed)\n",
    "        # audio = nr.reduce_noise(y=audio, sr=current_sr)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if current_sr != sr:\n",
    "            audio = librosa.resample(audio, orig_sr=current_sr, target_sr=sr)\n",
    "\n",
    "        # Pad or truncate to fixed duration *before* augmentation/normalization\n",
    "        target_len = sr * duration\n",
    "        if len(audio) < target_len:\n",
    "            audio = np.pad(audio, (0, target_len - len(audio)), mode='constant')\n",
    "        else:\n",
    "            audio = audio[:target_len]\n",
    "\n",
    "        # Data Augmentation (applied *before* normalization)\n",
    "        if np.random.random() < 0.5: # 50% chance\n",
    "            augmentation_type = np.random.choice(['noise', 'pitch', 'speed'])\n",
    "            if augmentation_type == 'noise':\n",
    "                noise_amp = 0.005 * np.random.uniform(0.1, 1.0) * np.max(np.abs(audio)) # Scale noise relative to audio\n",
    "                noise = np.random.randn(len(audio)) * noise_amp\n",
    "                audio = audio + noise\n",
    "            elif augmentation_type == 'pitch':\n",
    "                pitch_shift_steps = np.random.uniform(-2.5, 2.5)\n",
    "                audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=pitch_shift_steps)\n",
    "            else: # speed (time stretch)\n",
    "                speed_rate = np.random.uniform(0.85, 1.15)\n",
    "                audio = librosa.effects.time_stretch(audio, rate=speed_rate)\n",
    "                 # Time stretching changes length, re-pad/truncate\n",
    "                if len(audio) < target_len:\n",
    "                    audio = np.pad(audio, (0, target_len - len(audio)), mode='constant')\n",
    "                else:\n",
    "                    audio = audio[:target_len]\n",
    "\n",
    "        # Normalize audio (peak normalization) - crucial for consistency\n",
    "        max_amp = np.max(np.abs(audio))\n",
    "        if max_amp > 1e-6: # Avoid division by zero\n",
    "             audio = audio / max_amp\n",
    "        # Optional: RMS normalization instead\n",
    "        # rms = np.sqrt(np.mean(audio**2))\n",
    "        # if rms > 1e-6:\n",
    "        #    audio = audio / rms * 0.5 # Scale to a target RMS\n",
    "\n",
    "        return audio\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading/preprocessing {file_path}: {e}\")\n",
    "        print(f\"Error loading/preprocessing {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Feature Extraction Function\n",
    "def extract_features(audio, sr=16000, n_mels=80, n_fft=2048, hop_length=512):\n",
    "    if audio is None:\n",
    "        return None\n",
    "    try:\n",
    "        # Extract mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio,\n",
    "            sr=sr,\n",
    "            n_mels=n_mels,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length\n",
    "        )\n",
    "        # Convert to log scale (dB)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        # Normalize features (per spectrogram) - Standard Scaling\n",
    "        mean = np.mean(log_mel_spec)\n",
    "        std = np.std(log_mel_spec)\n",
    "        if std > 1e-6: # Avoid division by zero\n",
    "            log_mel_spec = (log_mel_spec - mean) / std\n",
    "        else:\n",
    "            log_mel_spec = log_mel_spec - mean # Just center if std is near zero\n",
    "\n",
    "        # Ensure the shape is consistent (should be handled by fixed duration loading)\n",
    "        # Expected frames: int(np.ceil(target_len / hop_length)) -> int(ceil(16000*4/512)) = 125? Check calculation\n",
    "        # target_len = 16000 * 4 = 64000\n",
    "        # expected_frames = 64000 // hop_length + 1 if 64000 % hop_length != 0 else 64000 // hop_length\n",
    "        # expected_frames = 64000 / 512 = 125. Check librosa padding. It often adds a frame. Let's stick to TARGET_FRAMES=126 based on previous findings.\n",
    "\n",
    "        return log_mel_spec # Shape (n_mels, n_frames) e.g. (80, 126)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting features: {e}\")\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Class Distribution Analysis Function\n",
    "def analyze_class_distribution(data_path):\n",
    "    try:\n",
    "        real_dir = os.path.join(data_path, 'real')\n",
    "        fake_dir = os.path.join(data_path, 'fake')\n",
    "        real_count = len([f for f in os.listdir(real_dir) if f.endswith('.wav')]) if os.path.exists(real_dir) else 0\n",
    "        fake_count = len([f for f in os.listdir(fake_dir) if f.endswith('.wav')]) if os.path.exists(fake_dir) else 0\n",
    "        total = real_count + fake_count\n",
    "        if total == 0:\n",
    "            print(f\"\\nNo .wav files found in {data_path}\")\n",
    "            return {'real': 0, 'fake': 0}\n",
    "        print(f\"\\nClass Distribution for {data_path}:\")\n",
    "        print(f\"Real: {real_count} ({real_count/total*100:.2f}%)\")\n",
    "        print(f\"Fake: {fake_count} ({fake_count/total*100:.2f}%)\")\n",
    "        return {'real': real_count, 'fake': fake_count}\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nError: Data path not found - {data_path}\")\n",
    "        return {'real': 0, 'fake': 0}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error analyzing distribution for {data_path}: {e}\")\n",
    "        print(f\"Error analyzing distribution for {data_path}: {e}\")\n",
    "        return {'real': 0, 'fake': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Data Generators\n",
    "\n",
    "# Define the fixed number of frames for GAN/Classifier consistency\n",
    "TARGET_FRAMES = 126 # Recalculate based on sr=16000, duration=4, hop_length=512 if needed\n",
    "\n",
    "# Data generator for STANDALONE CLASSIFIER training (Yields X, y, sample_weights)\n",
    "# --- (data_generator_classifier function remains the same as corrected before) ---\n",
    "def data_generator_classifier(data_path, batch_size=32, shuffle=True, target_frames=TARGET_FRAMES, sr=16000, duration=4, n_mels=80, n_fft=2048, hop_length=512):\n",
    "    real_files = []\n",
    "    fake_files = []\n",
    "    try:\n",
    "        real_dir = os.path.join(data_path, 'real')\n",
    "        fake_dir = os.path.join(data_path, 'fake')\n",
    "        if os.path.exists(real_dir):\n",
    "             real_files = [os.path.join(real_dir, f) for f in os.listdir(real_dir) if f.endswith('.wav')]\n",
    "        if os.path.exists(fake_dir):\n",
    "             fake_files = [os.path.join(fake_dir, f) for f in os.listdir(fake_dir) if f.endswith('.wav')]\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error finding directories in {data_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    all_files = real_files + fake_files\n",
    "    labels = [1] * len(real_files) + [0] * len(fake_files) # Real=1, Fake=0\n",
    "\n",
    "    if not all_files:\n",
    "        print(f\"No WAV files found in {data_path}. Classifier generator stopping.\")\n",
    "        return\n",
    "\n",
    "    total_samples = len(all_files)\n",
    "    class_weights = {\n",
    "        1: total_samples / (2 * len(real_files)) if len(real_files) > 0 else 1.0,\n",
    "        0: total_samples / (2 * len(fake_files)) if len(fake_files) > 0 else 1.0,\n",
    "    }\n",
    "    print(f\"Using class weights: {class_weights} for path {data_path}\")\n",
    "\n",
    "\n",
    "    indices = np.arange(total_samples)\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            batch_files = [all_files[k] for k in batch_indices]\n",
    "            batch_labels = [labels[k] for k in batch_indices]\n",
    "\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            batch_sample_weights = []\n",
    "\n",
    "            for file_path, label in zip(batch_files, batch_labels):\n",
    "                audio = load_and_preprocess_audio(file_path, sr=sr, duration=duration)\n",
    "                if audio is None: continue\n",
    "\n",
    "                features = extract_features(audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "                if features is None: continue\n",
    "\n",
    "                current_frames = features.shape[1]\n",
    "                if current_frames != target_frames:\n",
    "                     if current_frames < target_frames:\n",
    "                        pad_width = target_frames - current_frames\n",
    "                        padded_features = np.pad(features, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                     else:\n",
    "                        padded_features = features[:, :target_frames]\n",
    "                else:\n",
    "                    padded_features = features\n",
    "\n",
    "                batch_x.append(padded_features)\n",
    "                batch_y.append(label)\n",
    "                batch_sample_weights.append(class_weights[label])\n",
    "\n",
    "            # --- Check for classifier generator ---\n",
    "            if not batch_x:\n",
    "                print(f\"Warning: Skipping empty batch yield in data_generator_classifier for path {data_path}\")\n",
    "                continue\n",
    "\n",
    "            batch_x_4d = np.expand_dims(np.array(batch_x), axis=-1).astype(np.float32)\n",
    "            batch_y_arr = np.array(batch_y).astype(np.float32)\n",
    "            batch_weights_arr = np.array(batch_sample_weights).astype(np.float32)\n",
    "            yield batch_x_4d, batch_y_arr, batch_weights_arr\n",
    "\n",
    "\n",
    "# Data generator for WGAN training (Yields real fake samples X only)\n",
    "# --- (MODIFIED data_generator_gan function) ---\n",
    "def data_generator_gan(data_path, batch_size=32, shuffle=True, target_frames=TARGET_FRAMES, sr=16000, duration=4, n_mels=80, n_fft=2048, hop_length=512):\n",
    "    fake_files = []\n",
    "    try:\n",
    "        fake_dir = os.path.join(data_path, 'fake')\n",
    "        if os.path.exists(fake_dir):\n",
    "             fake_files = [os.path.join(fake_dir, f) for f in os.listdir(fake_dir) if f.endswith('.wav')]\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error finding fake directory in {data_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    if not fake_files:\n",
    "        print(f\"No fake WAV files found in {data_path}. GAN generator stopping.\")\n",
    "        return\n",
    "\n",
    "    total_samples = len(fake_files)\n",
    "    indices = np.arange(total_samples)\n",
    "\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            batch_files = [fake_files[k] for k in batch_indices]\n",
    "\n",
    "            batch_x = []\n",
    "\n",
    "            for file_path in batch_files:\n",
    "                audio = load_and_preprocess_audio(file_path, sr=sr, duration=duration)\n",
    "                if audio is None: continue\n",
    "\n",
    "                features = extract_features(audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "                if features is None: continue\n",
    "\n",
    "                current_frames = features.shape[1]\n",
    "                if current_frames != target_frames:\n",
    "                    if current_frames < target_frames:\n",
    "                        pad_width = target_frames - current_frames\n",
    "                        padded_features = np.pad(features, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                    else:\n",
    "                        padded_features = features[:, :target_frames]\n",
    "                else:\n",
    "                     padded_features = features\n",
    "\n",
    "                batch_x.append(padded_features)\n",
    "\n",
    "            # ---> ADD THIS CHECK for GAN generator <---\n",
    "            if not batch_x:\n",
    "                print(f\"Warning: Skipping empty batch yield in data_generator_gan for path {data_path}\")\n",
    "                continue # Skip yield if batch ended up empty\n",
    "\n",
    "            # If batch is not empty, yield the data\n",
    "            batch_x_4d = np.expand_dims(np.array(batch_x), axis=-1).astype(np.float32)\n",
    "            yield batch_x_4d # Yield only the features (4D array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Self-Attention Layer (SAGAN style)\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    \"\"\"\n",
    "    Self-attention layer based on SAGAN.\n",
    "    Input shape: (batch, height, width, channels)\n",
    "    Output shape: (batch, height, width, channels_out) where channels_out is typically channels\n",
    "    \"\"\"\n",
    "    def __init__(self, channels_out=None, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_channels = input_shape[-1]\n",
    "        if self.channels_out is None:\n",
    "            self.channels_out = self.input_channels\n",
    "\n",
    "        # Convolution layers for query, key, value\n",
    "        # Use 1x1 convolutions to reduce/transform channels\n",
    "        self.f = Conv2D(self.input_channels // 8, kernel_size=1, strides=1, padding='same', name='conv_f') # Query\n",
    "        self.g = Conv2D(self.input_channels // 8, kernel_size=1, strides=1, padding='same', name='conv_g') # Key\n",
    "        self.h = Conv2D(self.channels_out, kernel_size=1, strides=1, padding='same', name='conv_h')        # Value\n",
    "\n",
    "        # Final 1x1 convolution\n",
    "        self.out_conv = Conv2D(self.channels_out, kernel_size=1, strides=1, padding='same', name='conv_out')\n",
    "\n",
    "        # Learnable scale parameter\n",
    "        self.gamma = self.add_weight(name='gamma', shape=(1,), initializer='zeros', trainable=True)\n",
    "\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size, height, width, num_channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        location_num = height * width\n",
    "        downsampled_num = location_num\n",
    "\n",
    "        # Query (f), Key (g), Value (h) projections\n",
    "        f_proj = self.f(x) # Shape: (batch, h, w, c/8)\n",
    "        g_proj = self.g(x) # Shape: (batch, h, w, c/8)\n",
    "        h_proj = self.h(x) # Shape: (batch, h, w, c_out)\n",
    "\n",
    "        # Reshape for matrix multiplication\n",
    "        f_flat = tf.reshape(f_proj, shape=(batch_size, location_num, self.input_channels // 8)) # (batch, h*w, c/8)\n",
    "        g_flat = tf.reshape(g_proj, shape=(batch_size, location_num, self.input_channels // 8)) # (batch, h*w, c/8)\n",
    "        h_flat = tf.reshape(h_proj, shape=(batch_size, location_num, self.channels_out))       # (batch, h*w, c_out)\n",
    "\n",
    "        # Attention map calculation\n",
    "        # Transpose g for matmul: (batch, c/8, h*w)\n",
    "        g_flat_t = tf.transpose(g_flat, perm=[0, 2, 1])\n",
    "        # Attention score: (batch, h*w, c/8) x (batch, c/8, h*w) -> (batch, h*w, h*w)\n",
    "        attention_score = tf.matmul(f_flat, g_flat_t)\n",
    "        attention_prob = tf.nn.softmax(attention_score, axis=-1) # Apply softmax across locations\n",
    "\n",
    "        # Apply attention map to value projection\n",
    "        # (batch, h*w, h*w) x (batch, h*w, c_out) -> (batch, h*w, c_out)\n",
    "        attention_output = tf.matmul(attention_prob, h_flat)\n",
    "\n",
    "        # Reshape back to image format\n",
    "        attention_output_reshaped = tf.reshape(attention_output, shape=(batch_size, height, width, self.channels_out))\n",
    "\n",
    "        # Apply final 1x1 convolution and scale by gamma\n",
    "        o = self.out_conv(attention_output_reshaped)\n",
    "        y = self.gamma * o + x # Additive skip connection\n",
    "\n",
    "        return y\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (self.channels_out,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Generator Model (with Self-Attention)\n",
    "\n",
    "def create_generator(latent_dim, output_shape): # output_shape (n_mels, n_frames) e.g., (80, 126)\n",
    "    \"\"\"Creates the generator model with Self-Attention.\"\"\"\n",
    "    n_mels, n_frames = output_shape\n",
    "    init_h, init_w = n_mels // 8, n_frames // 8 # Calculate initial dimensions based on 3 upsamples (2*2*2=8)\n",
    "    init_c = 128 # Initial channels\n",
    "\n",
    "    if init_h * 8 != n_mels or init_w * 8 != n_frames:\n",
    "         print(f\"Warning: Output shape {output_shape} might not be perfectly reached with 3 strides of 2. Adjusting initial size or layers.\")\n",
    "         # Adjust init_w slightly if needed, e.g. target 128 -> 16, target 126 -> 16 (trim later)\n",
    "         init_w = (n_frames + 7) // 8 # Ceiling division equivalent for width\n",
    "\n",
    "    nodes = init_h * init_w * init_c\n",
    "\n",
    "    model = Sequential(name='generator')\n",
    "    model.add(Input(shape=(latent_dim,)))\n",
    "\n",
    "    # Dense layer and reshape\n",
    "    model.add(Dense(nodes))\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2)) # Use negative_slope\n",
    "    model.add(Reshape((init_h, init_w, init_c))) # e.g., (10, 16, 128)\n",
    "\n",
    "    # Upsample 1: (10, 16, 128) -> (20, 32, 64)\n",
    "    model.add(Conv2DTranspose(init_c // 2, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False)) # Typically use_bias=False with Norm\n",
    "    model.add(LayerNormalization()) # Using LayerNorm instead of BatchNorm\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "    # Upsample 2: (20, 32, 64) -> (40, 64, 32)\n",
    "    model.add(Conv2DTranspose(init_c // 4, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "    # Add Self-Attention Layer Here (applied to 40x64 feature map)\n",
    "    # Note: Attention can be computationally expensive. Apply strategically.\n",
    "    model.add(SelfAttention(channels_out=init_c // 4)) # Keep channels the same\n",
    "    # model.add(LayerNormalization()) # Optional normalization after attention\n",
    "    # model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2)) # Optional activation after attention\n",
    "\n",
    "    # Upsample 3: (40, 64, 32) -> (80, 128, 1)\n",
    "    model.add(Conv2DTranspose(1, kernel_size=(4, 4), strides=(2, 2), padding='same', activation='tanh'))\n",
    "\n",
    "    # Final adjustment layer if needed (e.g., width 128 -> 126)\n",
    "    current_width = init_w * 8\n",
    "    if current_width != n_frames:\n",
    "        print(f\"Generator adding final Conv2D to adjust width from {current_width} to {n_frames}\")\n",
    "        # Calculate kernel size needed for 'valid' padding: K = W_in - W_out + 1\n",
    "        kernel_w = current_width - n_frames + 1\n",
    "        if kernel_w > 0:\n",
    "             model.add(Conv2D(1, kernel_size=(1, kernel_w), padding='valid', activation='tanh'))\n",
    "        else:\n",
    "            # This case shouldn't happen with the init_w calculation but handle just in case\n",
    "             print(f\"Warning: Could not adjust width with Conv2D. Current {current_width}, Target {n_frames}\")\n",
    "             # May need padding='same' and cropping layer if kernel_w is not positive\n",
    "\n",
    "    # Ensure final output shape is correct\n",
    "    # model.add(Reshape((n_mels, n_frames, 1))) # Add reshape just to be certain, though last Conv should handle it\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Critic (Discriminator) Model (with Self-Attention) - NO DROPOUT\n",
    "\n",
    "def create_critic(input_shape): # input_shape (n_mels, n_frames) e.g., (80, 126)\n",
    "    \"\"\"Creates the Critic model for WGAN-GP with Self-Attention. NO DROPOUT.\"\"\"\n",
    "    n_mels, n_frames = input_shape\n",
    "    model_input_shape = (n_mels, n_frames, 1) # Expects (80, 126, 1)\n",
    "\n",
    "    model = Sequential(name='critic')\n",
    "    model.add(Input(shape=model_input_shape))\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "    # model.add(Dropout(0.25)) # REMOVED\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Conv2D(128, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "    # model.add(Dropout(0.25)) # REMOVED\n",
    "\n",
    "    # Add Self-Attention Layer Here\n",
    "    model.add(SelfAttention(channels_out=128))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(Conv2D(256, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "    # model.add(Dropout(0.25)) # REMOVED (If you had one here)\n",
    "\n",
    "    # Flatten and Output Score\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Define the GAN Model (Not used for WGAN-GP training loop)\n",
    "# def create_gan(generator, discriminator, latent_dim):\n",
    "#     \"\"\"Creates the combined GAN model.\"\"\"\n",
    "#     # Make discriminator non-trainable\n",
    "#     discriminator.trainable = False\n",
    "#\n",
    "#     # Stack generator and discriminator\n",
    "#     gan_input = Input(shape=(latent_dim,))\n",
    "#     gan_output = discriminator(generator(gan_input))\n",
    "#     gan = Model(gan_input, gan_output)\n",
    "#\n",
    "#     return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744191566.120307 1164916 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2143 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Output shape (80, 126) might not be perfectly reached with 3 strides of 2. Adjusting initial size or layers.\n",
      "Generator adding final Conv2D to adjust width from 128 to 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744191566.812452 1164916 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of generated image (Generator Output): (1, 80, 126, 1)\n",
      "Shape of critic output: (1, 1)\n",
      "Expected shape for Critic Input: (80, 126, 1)\n",
      "\n",
      "--- Generator Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"generator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"generator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20480</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,068,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20480</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttention</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,377</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20480\u001b[0m)          │     \u001b[38;5;34m2,068,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20480\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m131,072\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m32,768\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention (\u001b[38;5;33mSelfAttention\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m2,377\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │           \u001b[38;5;34m513\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │             \u001b[38;5;34m4\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,235,406</span> (8.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,235,406\u001b[0m (8.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,235,406</span> (8.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,235,406\u001b[0m (8.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Critic Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"critic\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"critic\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,153</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttention</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40960</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,961</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m1,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m131,072\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m37,153\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttention\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m524,288\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40960\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m40,961\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">735,330</span> (2.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m735,330\u001b[0m (2.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">735,330</span> (2.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m735,330\u001b[0m (2.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 11: Data Path and Parameters\n",
    "\n",
    "# Data Paths\n",
    "train_data_path = 'datasetNEW/train'\n",
    "dev_data_path = 'datasetNEW/dev'\n",
    "eval_data_path = 'datasetNEW/eval'\n",
    "\n",
    "# Define the fixed number of frames\n",
    "TARGET_FRAMES = 126\n",
    "N_MELS = 80\n",
    "mel_spectrogram_shape = (N_MELS, TARGET_FRAMES)\n",
    "\n",
    "# WGAN-GP specific parameters\n",
    "latent_dim = 100\n",
    "n_critic = 5         # Train critic 5 times per generator update\n",
    "gp_weight = 1.0     # Gradient penalty weight\n",
    "gan_epochs = 75      # WGAN often needs more epochs, adjust as needed\n",
    "gan_batch_size = 8  # Adjust based on GPU memory (WGAN-GP can be memory intensive)\n",
    "\n",
    "# Optimizers (Typical WGAN settings: lower LR, betas=(0.5, 0.9))\n",
    "critic_optimizer = Adam(learning_rate=1e-5, beta_1=0.5, beta_2=0.9) # TRY LOWER LR\n",
    "generator_optimizer = Adam(learning_rate=1e-5, beta_1=0.5, beta_2=0.9)\n",
    "# critic_optimizer = RMSprop(learning_rate=0.00005) # Alternative optimizer\n",
    "# generator_optimizer = RMSprop(learning_rate=0.00005)\n",
    "\n",
    "# Create instances\n",
    "generator = create_generator(latent_dim, mel_spectrogram_shape)\n",
    "critic = create_critic(mel_spectrogram_shape)\n",
    "\n",
    "# Diagnostic Code: Verify Output Shape\n",
    "test_noise = tf.random.normal((1, latent_dim))\n",
    "generated_image = generator(test_noise, training=False) # Use tf.random and call model directly\n",
    "critic_output = critic(generated_image, training=False)\n",
    "print(\"Shape of generated image (Generator Output):\", generated_image.shape)\n",
    "print(\"Shape of critic output:\", critic_output.shape)\n",
    "\n",
    "critic_input_shape_expected = (mel_spectrogram_shape[0], mel_spectrogram_shape[1], 1)\n",
    "print(\"Expected shape for Critic Input:\", critic_input_shape_expected)\n",
    "assert generated_image.shape[1:] == critic_input_shape_expected, \"Generator output shape mismatch!\"\n",
    "assert len(critic_output.shape) == 2 and critic_output.shape[1] == 1, \"Critic output shape mismatch!\"\n",
    "\n",
    "\n",
    "# Report the models\n",
    "print(\"\\n--- Generator Summary ---\")\n",
    "generator.summary()\n",
    "print(\"\\n--- Critic Summary ---\")\n",
    "critic.summary()\n",
    "\n",
    "# Parameters for standalone classifier training (can be different)\n",
    "classifier_batch_size = 8 # Keep smaller for classifier fine-tuning?\n",
    "classifier_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated 2850 GAN steps per epoch.\n",
      "\n",
      "Begin WGAN-GP training!\n",
      "\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f51601e202448c5bae0bfdedb5b0130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 152\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tf.shape(real_spoof_samples)[\u001b[32m0\u001b[39m] == \u001b[32m0\u001b[39m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# print(f\"Warning: Skipped empty batch at step {batch_idx}\") # Optional print\u001b[39;00m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m c_loss, g_loss = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_spoof_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# Check for NaN or Inf losses (important for stability)\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.isnan(c_loss.numpy()) \u001b[38;5;129;01mor\u001b[39;00m np.isinf(c_loss.numpy()) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[32m    156\u001b[39m    np.isnan(g_loss.numpy()) \u001b[38;5;129;01mor\u001b[39;00m np.isinf(g_loss.numpy()):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:906\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[32m    903\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    904\u001b[39m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[32m    905\u001b[39m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    910\u001b[39m   bound_args = \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn.function_type.bind(\n\u001b[32m    911\u001b[39m       *args, **kwds\n\u001b[32m    912\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:132\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    130\u001b[39m args = args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[32m    131\u001b[39m kwargs = kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m function = \u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[39m, in \u001b[36mtrace_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    175\u001b[39m     args = tracing_options.input_signature\n\u001b[32m    176\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m   concrete_function = \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options.bind_graph_to_function:\n\u001b[32m    183\u001b[39m   concrete_function._garbage_collector.release()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[39m, in \u001b[36m_maybe_define_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    282\u001b[39m   target_func_type = lookup_func_type\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m concrete_function = \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tracing_options.function_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    288\u001b[39m   tracing_options.function_cache.add(\n\u001b[32m    289\u001b[39m       concrete_function, current_func_context\n\u001b[32m    290\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[39m, in \u001b[36m_create_concrete_function\u001b[39m\u001b[34m(function_type, type_context, func_graph, tracing_options)\u001b[39m\n\u001b[32m    303\u001b[39m   placeholder_bound_args = function_type.placeholder_arguments(\n\u001b[32m    304\u001b[39m       placeholder_context\n\u001b[32m    305\u001b[39m   )\n\u001b[32m    307\u001b[39m disable_acd = tracing_options.attributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options.attributes.get(\n\u001b[32m    308\u001b[39m     attributes_lib.DISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    309\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m traced_func_graph = \u001b[43mfunc_graph_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction_type_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m transform.apply_func_graph_transforms(traced_func_graph)\n\u001b[32m    324\u001b[39m graph_capture_container = traced_func_graph.function_captures\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/framework/func_graph.py:1060\u001b[39m, in \u001b[36mfunc_graph_from_py_func\u001b[39m\u001b[34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[39m\n\u001b[32m   1057\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m   1059\u001b[39m _, original_func = tf_decorator.unwrap(python_func)\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m func_outputs = \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[32m   1064\u001b[39m func_outputs = variable_utils.convert_variables_to_tensors(func_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:599\u001b[39m, in \u001b[36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m default_graph._variable_creator_scope(scope, priority=\u001b[32m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    596\u001b[39m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[32m    597\u001b[39m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[32m    598\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     out = \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:41\u001b[39m, in \u001b[36mpy_func_from_autograph.<locals>.autograph_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m     51\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mag_error_metadata\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[39m, in \u001b[36mconverted_call\u001b[39m\u001b[34m(f, args, kwargs, caller_fn_scope, options)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    438\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     result = \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    441\u001b[39m     result = converted_f(*effective_args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_filema5m2ibt.py:71\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[39m\u001b[34m(real_images)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ag__.ld(tf).GradientTape() \u001b[38;5;28;01mas\u001b[39;00m gen_tape:\n\u001b[32m     70\u001b[39m     fake_images_gen = ag__.converted_call(ag__.ld(generator), (ag__.ld(noise),), \u001b[38;5;28mdict\u001b[39m(training=\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     fake_output_gen = \u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_images_gen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     gen_loss = ag__.converted_call(ag__.ld(generator_loss), (ag__.ld(fake_output_gen),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     73\u001b[39m gen_gradients = ag__.converted_call(ag__.ld(gen_tape).gradient, (ag__.ld(gen_loss), ag__.ld(generator).trainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:331\u001b[39m, in \u001b[36mconverted_call\u001b[39m\u001b[34m(f, args, kwargs, caller_fn_scope, options)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conversion.is_in_allowlist_cache(f, options):\n\u001b[32m    330\u001b[39m   logging.log(\u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: from cache\u001b[39m\u001b[33m'\u001b[39m, f)\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx.control_status_ctx().status == ag_ctx.Status.DISABLED:\n\u001b[32m    334\u001b[39m   logging.log(\u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: AutoGraph is disabled in context\u001b[39m\u001b[33m'\u001b[39m, f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[39m, in \u001b[36m_call_unconverted\u001b[39m\u001b[34m(f, args, kwargs, options, update_cache)\u001b[39m\n\u001b[32m    456\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m f.\u001b[34m__self__\u001b[39m.call(args, kwargs)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m f(*args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/layers/layer.py:909\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    907\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    911\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    912\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    913\u001b[39m distribution = distribution_lib.distribution()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/ops/operation.py:52\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     48\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     49\u001b[39m         call_fn,\n\u001b[32m     50\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     51\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/utils/traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/models/sequential.py:221\u001b[39m, in \u001b[36mSequential.call\u001b[39m\u001b[34m(self, inputs, training, mask)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training=\u001b[38;5;28;01mNone\u001b[39;00m, mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._functional:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_functional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# Fallback: Just apply the layer sequence.\u001b[39;00m\n\u001b[32m    224\u001b[39m     \u001b[38;5;66;03m# This typically happens if `inputs` is a nested struct.\u001b[39;00m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m    226\u001b[39m         \u001b[38;5;66;03m# During each iteration, `inputs` are the inputs to `layer`, and\u001b[39;00m\n\u001b[32m    227\u001b[39m         \u001b[38;5;66;03m# `outputs` are the outputs of `layer` applied to `inputs`. At the\u001b[39;00m\n\u001b[32m    228\u001b[39m         \u001b[38;5;66;03m# end of each iteration `inputs` is set to `outputs` to prepare for\u001b[39;00m\n\u001b[32m    229\u001b[39m         \u001b[38;5;66;03m# the next layer.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/models/functional.py:183\u001b[39m, in \u001b[36mFunctional.call\u001b[39m\u001b[34m(self, inputs, training, mask)\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    182\u001b[39m             backend.set_keras_mask(x, mask)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_through_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/ops/function.py:171\u001b[39m, in \u001b[36mFunction._run_through_graph\u001b[39m\u001b[34m(self, inputs, operation_fn, call_fn)\u001b[39m\n\u001b[32m    169\u001b[39m     outputs = call_fn(op, *args, **kwargs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     outputs = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node.outputs, tree.flatten(outputs)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/models/functional.py:643\u001b[39m, in \u001b[36moperation_fn.<locals>.call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mhasattr\u001b[39m(operation, \u001b[33m\"\u001b[39m\u001b[33m_call_has_training_arg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    639\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m operation._call_has_training_arg\n\u001b[32m    640\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    641\u001b[39m ):\n\u001b[32m    642\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m] = training\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/layers/layer.py:909\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    907\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    911\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    912\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    913\u001b[39m distribution = distribution_lib.distribution()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/ops/operation.py:52\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     48\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     49\u001b[39m         call_fn,\n\u001b[32m     50\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     51\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/utils/traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/layers/core/dense.py:144\u001b[39m, in \u001b[36mDense.call\u001b[39m\u001b[34m(self, inputs, training)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     x = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    146\u001b[39m         x = ops.add(x, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/ops/numpy.py:3884\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m   3882\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x1, x2)):\n\u001b[32m   3883\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Matmul().symbolic_call(x1, x2)\n\u001b[32m-> \u001b[39m\u001b[32m3884\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/backend/tensorflow/numpy.py:570\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x1_shape.rank == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m x2_shape.rank == \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m         output = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    571\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m x2_shape.rank == \u001b[32m1\u001b[39m:\n\u001b[32m    572\u001b[39m         output = tf.tensordot(x1, x2, axes=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[39m, in \u001b[36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    141\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m   bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m    144\u001b[39m   bound_arguments.apply_defaults()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1258\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1262\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/tensorflow/python/ops/math_ops.py:3442\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, grad_a, grad_b, name)\u001b[39m\n\u001b[32m   3438\u001b[39m     x = ops.convert_to_tensor(x, name=\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce_sum(array_ops.matrix_diag_part(x), [-\u001b[32m1\u001b[39m], name=name)\n\u001b[32m-> \u001b[39m\u001b[32m3442\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlinalg.matmul\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmatmul\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3443\u001b[39m \u001b[38;5;129m@dispatch\u001b[39m.add_dispatch_support\n\u001b[32m   3444\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmatmul\u001b[39m(\n\u001b[32m   3445\u001b[39m     a,\n\u001b[32m   3446\u001b[39m     b,\n\u001b[32m   3447\u001b[39m     transpose_a=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   3448\u001b[39m     transpose_b=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   3449\u001b[39m     adjoint_a=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   3450\u001b[39m     adjoint_b=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   3451\u001b[39m     a_is_sparse=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   3452\u001b[39m     b_is_sparse=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   3453\u001b[39m     output_type=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3454\u001b[39m     grad_a=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   3455\u001b[39m     grad_b=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   3456\u001b[39m     name=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3457\u001b[39m ):\n\u001b[32m   3458\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\u001b[39;00m\n\u001b[32m   3459\u001b[39m \n\u001b[32m   3460\u001b[39m \u001b[33;03m  The inputs must, following any transpositions, be tensors of rank >= 2\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3573\u001b[39m \u001b[33;03m      `output_type` is not (u)int8, (u)int8 and int32.\u001b[39;00m\n\u001b[32m   3574\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m   3576\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(name, \u001b[33m\"\u001b[39m\u001b[33mMatMul\u001b[39m\u001b[33m\"\u001b[39m, [a, b]) \u001b[38;5;28;01mas\u001b[39;00m name:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 12: WGAN-GP Training Loop (Saving Critic Weights Only)\n",
    "\n",
    "# --- Imports ---\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for better Jupyter integration\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Assuming necessary variables (train_data_path, gan_batch_size, latent_dim, n_critic,\n",
    "# gp_weight, gan_epochs, critic_optimizer, generator_optimizer, generator, critic,\n",
    "# FIGURES_DIR) and functions (data_generator_gan) are defined and accessible\n",
    "# from previous cells.\n",
    "\n",
    "# --- Loss Functions ---\n",
    "def critic_loss(real_output, fake_output):\n",
    "    \"\"\"Wasserstein loss for the critic.\"\"\"\n",
    "    # Ensure outputs are float32 for stable loss calculation\n",
    "    real_output = tf.cast(real_output, tf.float32)\n",
    "    fake_output = tf.cast(fake_output, tf.float32)\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    \"\"\"Wasserstein loss for the generator.\"\"\"\n",
    "    # Ensure output is float32\n",
    "    fake_output = tf.cast(fake_output, tf.float32)\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "# --- Gradient Penalty Function (Corrected) ---\n",
    "def gradient_penalty(batch_size, real_images, fake_images):\n",
    "    \"\"\" Calculates the gradient penalty loss for WGAN GP, handling mixed precision. \"\"\"\n",
    "    # Ensure both images have the same dtype before interpolation\n",
    "    if real_images.dtype != fake_images.dtype:\n",
    "        real_images = tf.cast(real_images, fake_images.dtype)\n",
    "\n",
    "    # Generate interpolation alpha with the correct dtype\n",
    "    alpha_shape = [tf.shape(real_images)[0]] + [1] * (len(real_images.shape) - 1)\n",
    "    alpha = tf.random.uniform(shape=alpha_shape, minval=0., maxval=1., dtype=real_images.dtype)\n",
    "\n",
    "    # Interpolate images\n",
    "    interpolated = real_images + alpha * (fake_images - real_images)\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        pred = critic(interpolated, training=True)\n",
    "        pred = tf.cast(pred, tf.float32) # Cast prediction to float32 for stable GP calculation\n",
    "\n",
    "    grads = gp_tape.gradient(pred, [interpolated])\n",
    "    if grads is None or grads[0] is None:\n",
    "        logging.warning(\"Gradients are None in gradient_penalty. Returning 0 penalty.\")\n",
    "        # print(\"Warning: Gradients are None in gradient_penalty. Returning 0 penalty.\") # Optional print\n",
    "        return tf.constant(0.0, dtype=tf.float32)\n",
    "    grads = grads[0]\n",
    "    grads = tf.cast(grads, tf.float32) # Cast gradients to float32 before norm\n",
    "\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=tf.range(1, tf.rank(grads))))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "\n",
    "    return gp * gp_weight # gp_weight should be float\n",
    "\n",
    "\n",
    "# --- Data Generator and Steps Calculation ---\n",
    "train_gen_gan = data_generator_gan(train_data_path, batch_size=gan_batch_size)\n",
    "\n",
    "fake_files_count = 0\n",
    "try:\n",
    "    fake_dir = os.path.join(train_data_path, 'fake')\n",
    "    if os.path.exists(fake_dir):\n",
    "        fake_files_count = len([f for f in os.listdir(fake_dir) if f.endswith('.wav')])\n",
    "except FileNotFoundError:\n",
    "    fake_files_count = 0\n",
    "\n",
    "if gan_batch_size <= 0:\n",
    "    raise ValueError(\"GAN Batch size must be positive.\")\n",
    "if fake_files_count == 0:\n",
    "    print(\"Warning: No fake training files found for GAN. Setting GAN steps to 0.\")\n",
    "    gan_steps_per_epoch = 0\n",
    "else:\n",
    "    gan_steps_per_epoch = int(np.ceil(fake_files_count / float(gan_batch_size)))\n",
    "    print(f\"Calculated {gan_steps_per_epoch} GAN steps per epoch.\")\n",
    "\n",
    "\n",
    "# --- Training Step Function (Decorated with tf.function) ---\n",
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    current_batch_size = tf.shape(real_images)[0]\n",
    "    # Use float32 for noise if mixed precision is enabled, otherwise default might be okay\n",
    "    noise = tf.random.normal([current_batch_size, latent_dim], dtype=tf.float32)\n",
    "\n",
    "    # Train Critic (n_critic times)\n",
    "    total_crit_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "    for _ in tf.range(n_critic):\n",
    "        with tf.GradientTape() as crit_tape:\n",
    "            fake_images = generator(noise, training=True)\n",
    "            real_output = critic(real_images, training=True)\n",
    "            fake_output = critic(fake_images, training=True)\n",
    "            crit_loss = critic_loss(real_output, fake_output)\n",
    "            gp = gradient_penalty(current_batch_size, real_images, fake_images)\n",
    "            total_crit_loss = crit_loss + gp\n",
    "\n",
    "        crit_gradients = crit_tape.gradient(total_crit_loss, critic.trainable_variables)\n",
    "        valid_grads_and_vars = [(g, v) for g, v in zip(crit_gradients, critic.trainable_variables) if g is not None]\n",
    "        if len(valid_grads_and_vars) < len(critic.trainable_variables):\n",
    "             tf.print(\"Warning: Some critic gradients are None.\")\n",
    "        if len(valid_grads_and_vars) > 0:\n",
    "            critic_optimizer.apply_gradients(valid_grads_and_vars)\n",
    "\n",
    "    # Train Generator\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_images_gen = generator(noise, training=True)\n",
    "        fake_output_gen = critic(fake_images_gen, training=True)\n",
    "        gen_loss = generator_loss(fake_output_gen)\n",
    "\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    valid_grads_and_vars_gen = [(g, v) for g, v in zip(gen_gradients, generator.trainable_variables) if g is not None]\n",
    "    if len(valid_grads_and_vars_gen) < len(generator.trainable_variables):\n",
    "         tf.print(\"Warning: Some generator gradients are None.\")\n",
    "    if len(valid_grads_and_vars_gen) > 0:\n",
    "        generator_optimizer.apply_gradients(valid_grads_and_vars_gen)\n",
    "\n",
    "    return total_crit_loss, gen_loss\n",
    "\n",
    "\n",
    "# --- WGAN-GP Training Loop ---\n",
    "print(\"\\nBegin WGAN-GP training!\")\n",
    "\n",
    "if train_gen_gan is None or gan_steps_per_epoch == 0:\n",
    "    print(\"Skipping WGAN-GP training: Generator not initialized or no steps per epoch.\")\n",
    "else:\n",
    "    c_loss_history = []\n",
    "    g_loss_history = []\n",
    "\n",
    "    for epoch in range(gan_epochs): # Use gan_epochs defined in Cell 11\n",
    "        print(f\"\\nEpoch {epoch+1}/{gan_epochs}\")\n",
    "        epoch_pbar = tqdm(range(gan_steps_per_epoch), desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "        epoch_c_loss = 0.0\n",
    "        epoch_g_loss = 0.0\n",
    "        batches_processed = 0\n",
    "\n",
    "        for batch_idx in epoch_pbar:\n",
    "            try:\n",
    "                real_spoof_samples_np = next(train_gen_gan)\n",
    "                # Ensure input to train_step is float32\n",
    "                real_spoof_samples = tf.convert_to_tensor(real_spoof_samples_np, dtype=tf.float32)\n",
    "\n",
    "                if tf.shape(real_spoof_samples)[0] == 0:\n",
    "                    # print(f\"Warning: Skipped empty batch at step {batch_idx}\") # Optional print\n",
    "                    continue\n",
    "\n",
    "                c_loss, g_loss = train_step(real_spoof_samples)\n",
    "\n",
    "                # Check for NaN or Inf losses (important for stability)\n",
    "                if np.isnan(c_loss.numpy()) or np.isinf(c_loss.numpy()) or \\\n",
    "                   np.isnan(g_loss.numpy()) or np.isinf(g_loss.numpy()):\n",
    "                    print(f\"\\nError: NaN or Inf loss detected at epoch {epoch+1}, batch {batch_idx}. Stopping training.\")\n",
    "                    logging.error(f\"NaN/Inf loss detected: C Loss={c_loss.numpy()}, G Loss={g_loss.numpy()}. Epoch {epoch+1}, Batch {batch_idx}\")\n",
    "                    # Optional: Save current state before breaking if needed\n",
    "                    # generator.save_weights('generator_nan_inf.weights.h5')\n",
    "                    # critic.save_weights('critic_nan_inf.weights.h5')\n",
    "                    raise ValueError(\"NaN or Inf loss detected, stopping training.\") # Stop execution\n",
    "\n",
    "                epoch_c_loss += c_loss.numpy()\n",
    "                epoch_g_loss += g_loss.numpy()\n",
    "                batches_processed += 1\n",
    "\n",
    "                epoch_pbar.set_postfix({\"C Loss\": f\"{c_loss.numpy():.4f}\", \"G Loss\": f\"{g_loss.numpy():.4f}\"})\n",
    "\n",
    "            except StopIteration:\n",
    "                print(f\"\\nGAN Generator exhausted prematurely at batch {batch_idx}. Moving to next epoch.\")\n",
    "                train_gen_gan = data_generator_gan(train_data_path, batch_size=gan_batch_size)\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch NaN/Inf error from the check above or other errors\n",
    "                logging.error(f\"Error during WGAN training epoch {epoch+1}, batch {batch_idx}: {e}\", exc_info=True)\n",
    "                print(f\"\\nError during WGAN training epoch {epoch+1}, batch {batch_idx}: {e}\")\n",
    "                # Decide whether to continue or break based on the error type\n",
    "                if isinstance(e, ValueError) and \"NaN or Inf loss\" in str(e):\n",
    "                     break # Stop the outer loop for NaN/Inf\n",
    "                continue # Skip other problematic batches\n",
    "\n",
    "\n",
    "        # Check if loop was broken due to NaN/Inf\n",
    "        if np.isnan(epoch_c_loss) or np.isinf(epoch_c_loss) or np.isnan(epoch_g_loss) or np.isinf(epoch_g_loss):\n",
    "             print(\"Training stopped due to NaN/Inf loss.\")\n",
    "             break # Exit the epoch loop\n",
    "\n",
    "        # --- End-of-epoch actions ---\n",
    "        if batches_processed > 0:\n",
    "             avg_c_loss = epoch_c_loss / batches_processed\n",
    "             avg_g_loss = epoch_g_loss / batches_processed\n",
    "             c_loss_history.append(avg_c_loss)\n",
    "             g_loss_history.append(avg_g_loss)\n",
    "             print(f\"Epoch {epoch+1} finished. Avg C Loss: {avg_c_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}\")\n",
    "        else:\n",
    "             print(f\"Epoch {epoch+1} finished. No batches processed.\")\n",
    "             c_loss_history.append(np.nan)\n",
    "             g_loss_history.append(np.nan)\n",
    "\n",
    "\n",
    "        # --- Save Models/Weights Periodically ---\n",
    "        # Save every 5 epochs and at the very last epoch\n",
    "        if (epoch + 1) % 5 == 0 or (epoch + 1) == gan_epochs:\n",
    "             try:\n",
    "                 # Save generator as full model (assuming it doesn't have loading issues)\n",
    "                 generator.save(f'generator_wgan_sa_epoch_{epoch+1}.keras')\n",
    "\n",
    "                 # Save critic weights ONLY\n",
    "                 critic_weights_filename = f'critic_wgan_sa_epoch_{epoch+1}.weights.h5' # Define filename\n",
    "                 critic.save_weights(critic_weights_filename) # Use save_weights\n",
    "\n",
    "                 print(f\"Saved generator model and critic weights ({critic_weights_filename}) for epoch {epoch+1}\")\n",
    "             except Exception as e:\n",
    "                 logging.error(f\"Error saving models/weights at epoch {epoch+1}: {e}\", exc_info=True)\n",
    "                 print(f\"Error saving models/weights at epoch {epoch+1}: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\nWGAN-GP training finished (or stopped early).\")\n",
    "\n",
    "    # Final loss plot\n",
    "    if c_loss_history and g_loss_history: # Plot only if history exists\n",
    "         plt.figure(figsize=(10, 5))\n",
    "         # Filter out NaN values for plotting if training stopped early\n",
    "         epochs_ran = range(1, len([loss for loss in c_loss_history if not np.isnan(loss)]) + 1)\n",
    "         plt.plot(epochs_ran, [loss for loss in c_loss_history if not np.isnan(loss)], label='Avg Critic Loss per Epoch')\n",
    "         plt.plot(epochs_ran, [loss for loss in g_loss_history if not np.isnan(loss)], label='Avg Generator Loss per Epoch')\n",
    "         plt.title('WGAN-GP Training Losses')\n",
    "         plt.xlabel('Epoch')\n",
    "         plt.ylabel('Average Loss')\n",
    "         plt.legend()\n",
    "         plt.grid(True)\n",
    "         plot_filename = os.path.join(FIGURES_DIR, 'gan_loss_final.png')\n",
    "         try:\n",
    "              plt.savefig(plot_filename)\n",
    "              print(f\"Saved final loss plot to {plot_filename}\")\n",
    "         except Exception as e:\n",
    "              logging.error(f\"Failed to save final loss plot: {e}\", exc_info=True)\n",
    "              print(f\"Failed to save final loss plot: {e}\")\n",
    "         plt.show()\n",
    "    else:\n",
    "         print(\"No loss history recorded, skipping final plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12.5: Continue WGAN-GP Training from Epoch 70 to 85 (loading weights - Build Fix Attempt 2)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import logging # Use logging setup from Cell 1\n",
    "# import fnmatch # Ensure fnmatch is imported if used later\n",
    "\n",
    "print(\"\\n--- Continuing WGAN-GP Training (Loading Weights) ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "load_epoch = 70\n",
    "start_epoch = load_epoch + 1\n",
    "end_epoch = 85\n",
    "continue_epochs = end_epoch - load_epoch\n",
    "\n",
    "# Parameters (should match Cell 11/12 setup)\n",
    "# Ensure these are correctly defined from previous cells:\n",
    "# latent_dim, train_data_path, gan_batch_size, gp_weight, n_critic\n",
    "# mel_spectrogram_shape (needed to recreate models)\n",
    "# Learning rates from Cell 11 (use the ones you found most stable)\n",
    "cont_critic_lr = 1e-5  # Example: Use the potentially lowered LR\n",
    "cont_gen_lr = 1e-5     # Example: Use the potentially lowered LR\n",
    "\n",
    "# --- Corrected File Paths for .weights.h5 ---\n",
    "generator_weights_load_path = f'generator_wgan_sa_epoch_{load_epoch}.weights.h5'\n",
    "critic_weights_load_path = f'critic_wgan_sa_epoch_{load_epoch}.weights.h5'\n",
    "\n",
    "# --- Ensure Model Creation Functions and SelfAttention Class are Defined ---\n",
    "# Make sure Cell 7 (SelfAttention), Cell 8 (create_generator), Cell 9 (create_critic)\n",
    "# definitions were executed earlier in the notebook.\n",
    "if 'SelfAttention' not in locals(): raise NameError(\"SelfAttention class definition not found. Ensure Cell 7 was executed.\")\n",
    "if 'create_generator' not in locals(): raise NameError(\"create_generator function definition not found. Ensure Cell 8 was executed.\")\n",
    "if 'create_critic' not in locals(): raise NameError(\"create_critic function definition not found. Ensure Cell 9 was executed.\")\n",
    "if 'mel_spectrogram_shape' not in locals(): raise NameError(\"mel_spectrogram_shape variable not found. Ensure Cell 11 was executed.\")\n",
    "if 'N_MELS' not in locals() or 'TARGET_FRAMES' not in locals(): raise NameError(\"N_MELS or TARGET_FRAMES not defined. Ensure Cell 11 was executed.\")\n",
    "if 'latent_dim' not in locals(): raise NameError(\"latent_dim not defined. Ensure Cell 11 was executed.\")\n",
    "\n",
    "\n",
    "# --- Recreate Model Structures ---\n",
    "print(\"Recreating model structures...\")\n",
    "generator_cont = create_generator(latent_dim, mel_spectrogram_shape)\n",
    "critic_cont = create_critic(mel_spectrogram_shape)\n",
    "print(\"Model structures recreated.\")\n",
    "\n",
    "# ---> BUILD MODELS VIA DUMMY FORWARD PASS <---\n",
    "print(\"Building models via dummy forward pass...\")\n",
    "# Create dummy input tensors with batch size 1\n",
    "dummy_critic_input = tf.zeros((1, N_MELS, TARGET_FRAMES, 1), dtype=tf.float32)\n",
    "dummy_generator_input = tf.zeros((1, latent_dim), dtype=tf.float32)\n",
    "\n",
    "try:\n",
    "    # Call the models with the dummy inputs\n",
    "    _ = critic_cont(dummy_critic_input, training=False) # training=False might be safer here\n",
    "    _ = generator_cont(dummy_generator_input, training=False)\n",
    "    print(\"Models built successfully via dummy pass.\")\n",
    "    # You can optionally print summary again to confirm layers are built\n",
    "    # print(\"Critic Summary after build:\")\n",
    "    # critic_cont.summary()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error building models via dummy pass: {e}\")\n",
    "\n",
    "\n",
    "# --- Load Weights ---\n",
    "# Load generator weights (optional, if file exists)\n",
    "# ... (loading logic remains the same as before) ...\n",
    "if os.path.exists(generator_weights_load_path):\n",
    "    print(f\"Loading generator weights from: {generator_weights_load_path}\")\n",
    "    try:\n",
    "        generator_cont.load_weights(generator_weights_load_path)\n",
    "        print(\"Generator weights loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error loading generator weights: {e}. Generator starts with initial weights.\")\n",
    "else:\n",
    "    print(f\"Generator weights file not found: {generator_weights_load_path}. Generator starts with initial weights.\")\n",
    "\n",
    "# Load critic weights (Mandatory)\n",
    "if not os.path.exists(critic_weights_load_path):\n",
    "    raise FileNotFoundError(f\"Critic weights file not found: {critic_weights_load_path}\")\n",
    "\n",
    "print(f\"Loading critic weights from: {critic_weights_load_path}\")\n",
    "try:\n",
    "    critic_cont.load_weights(critic_weights_load_path) # Now this *really* should work\n",
    "    print(\"Critic weights loaded successfully.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error loading critic weights after build: {e}\")\n",
    "\n",
    "\n",
    "# --- Define Optimizers (Redefine for clarity/safety) ---\n",
    "# ... (optimizer definitions remain the same) ...\n",
    "critic_optimizer_cont = tf.keras.optimizers.Adam(learning_rate=cont_critic_lr, beta_1=0.5, beta_2=0.9)\n",
    "generator_optimizer_cont = tf.keras.optimizers.Adam(learning_rate=cont_gen_lr, beta_1=0.5, beta_2=0.9)\n",
    "print(\"Optimizers redefined.\")\n",
    "\n",
    "# --- Redefine Training Functions (to use loaded models/optimizers) ---\n",
    "# ... (training function definitions remain the same) ...\n",
    "def critic_loss_cont(real_output, fake_output):\n",
    "    real_output = tf.cast(real_output, tf.float32); fake_output = tf.cast(fake_output, tf.float32)\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "def generator_loss_cont(fake_output):\n",
    "    fake_output = tf.cast(fake_output, tf.float32)\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "def gradient_penalty_cont(batch_size, real_images, fake_images):\n",
    "    if real_images.dtype != fake_images.dtype: real_images = tf.cast(real_images, fake_images.dtype)\n",
    "    alpha_shape = [tf.shape(real_images)[0]] + [1] * (len(real_images.shape) - 1)\n",
    "    alpha = tf.random.uniform(shape=alpha_shape, minval=0., maxval=1., dtype=real_images.dtype)\n",
    "    interpolated = real_images + alpha * (fake_images - real_images)\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated); pred = critic_cont(interpolated, training=True); pred = tf.cast(pred, tf.float32)\n",
    "    grads = gp_tape.gradient(pred, [interpolated])\n",
    "    if grads is None or grads[0] is None: logging.warning(\"Gradients are None in gradient_penalty_cont.\"); return tf.constant(0.0, dtype=tf.float32)\n",
    "    grads = grads[0]; grads = tf.cast(grads, tf.float32)\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=tf.range(1, tf.rank(grads)))); gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    return gp * gp_weight\n",
    "@tf.function\n",
    "def train_step_cont(real_images):\n",
    "    current_batch_size = tf.shape(real_images)[0]; noise = tf.random.normal([current_batch_size, latent_dim]); total_crit_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "    for _ in tf.range(n_critic):\n",
    "        with tf.GradientTape() as crit_tape:\n",
    "            fake_images = generator_cont(noise, training=True); real_output = critic_cont(real_images, training=True); fake_output = critic_cont(fake_images, training=True)\n",
    "            crit_loss = critic_loss_cont(real_output, fake_output); gp = gradient_penalty_cont(current_batch_size, real_images, fake_images); total_crit_loss = crit_loss + gp\n",
    "        crit_gradients = crit_tape.gradient(total_crit_loss, critic_cont.trainable_variables); valid_grads_and_vars = [(g, v) for g, v in zip(crit_gradients, critic_cont.trainable_variables) if g is not None]\n",
    "        if len(valid_grads_and_vars) < len(critic_cont.trainable_variables): tf.print(\"Warning: Some critic gradients are None in cont step.\")\n",
    "        if len(valid_grads_and_vars) > 0: critic_optimizer_cont.apply_gradients(valid_grads_and_vars)\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_images_gen = generator_cont(noise, training=True); fake_output_gen = critic_cont(fake_images_gen, training=True); gen_loss = generator_loss_cont(fake_output_gen)\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, generator_cont.trainable_variables); valid_grads_and_vars_gen = [(g, v) for g, v in zip(gen_gradients, generator_cont.trainable_variables) if g is not None]\n",
    "    if len(valid_grads_and_vars_gen) < len(generator_cont.trainable_variables): tf.print(\"Warning: Some generator gradients are None in cont step.\")\n",
    "    if len(valid_grads_and_vars_gen) > 0: generator_optimizer_cont.apply_gradients(valid_grads_and_vars_gen)\n",
    "    return total_crit_loss, gen_loss\n",
    "\n",
    "\n",
    "# --- Data Generator ---\n",
    "# ... (generator initialization and step calculation remain the same) ...\n",
    "print(\"Initializing data generator for continued training...\")\n",
    "train_gen_gan_cont = data_generator_gan(train_data_path, batch_size=gan_batch_size)\n",
    "if train_gen_gan_cont is None: raise RuntimeError(\"Failed to initialize GAN data generator.\")\n",
    "# Use function from Cell 13 or define count_total_files here if needed\n",
    "# This counts both real and fake, data_generator_gan only uses fake\n",
    "# We need the count of fake files only for the GAN generator steps\n",
    "fake_files_count_cont = 0\n",
    "try:\n",
    "    fake_dir_cont = os.path.join(train_data_path, 'fake')\n",
    "    if os.path.exists(fake_dir_cont):\n",
    "        fake_files_count_cont = len([f for f in os.listdir(fake_dir_cont) if f.endswith('.wav')])\n",
    "except Exception as e:\n",
    "     print(f\"Error counting fake files for continued training: {e}\")\n",
    "\n",
    "if gan_batch_size <= 0: raise ValueError(\"GAN Batch size must be positive.\")\n",
    "if fake_files_count_cont == 0: raise ValueError(\"No fake training files found for GAN.\")\n",
    "gan_steps_per_epoch_cont = int(np.ceil(fake_files_count_cont / float(gan_batch_size))) # Corrected count source\n",
    "print(f\"Using {gan_steps_per_epoch_cont} GAN steps per epoch for continued training.\") # Corrected count source\n",
    "\n",
    "\n",
    "# --- Continue Training Loop ---\n",
    "# ... (training loop remains the same) ...\n",
    "print(f\"\\nContinuing WGAN-GP training from epoch {start_epoch} to {end_epoch}...\")\n",
    "cont_c_loss_history = []; cont_g_loss_history = []\n",
    "if train_gen_gan_cont is None or gan_steps_per_epoch_cont == 0:\n",
    "    print(\"Skipping continued WGAN-GP training: Generator not initialized or no steps per epoch.\")\n",
    "else:\n",
    "    for epoch in range(start_epoch, end_epoch + 1): # Loop from 71 to 85\n",
    "        print(f\"\\nEpoch {epoch}/{end_epoch}\")\n",
    "        epoch_pbar_cont = tqdm(range(gan_steps_per_epoch_cont), desc=f\"Epoch {epoch}\")\n",
    "        epoch_c_loss = 0.0; epoch_g_loss = 0.0; batches_processed = 0\n",
    "        for batch_idx in epoch_pbar_cont:\n",
    "            try:\n",
    "                real_spoof_samples_np = next(train_gen_gan_cont); real_spoof_samples = tf.convert_to_tensor(real_spoof_samples_np, dtype=tf.float32)\n",
    "                if tf.shape(real_spoof_samples)[0] == 0: print(f\"Warning: Skipped empty batch at step {batch_idx} (cont)\"); continue\n",
    "                c_loss, g_loss = train_step_cont(real_spoof_samples)\n",
    "                epoch_c_loss += c_loss.numpy(); epoch_g_loss += g_loss.numpy(); batches_processed += 1\n",
    "                epoch_pbar_cont.set_postfix({\"C Loss\": f\"{c_loss.numpy():.4f}\", \"G Loss\": f\"{g_loss.numpy():.4f}\"})\n",
    "            except StopIteration: print(f\"\\nGAN Generator exhausted prematurely at batch {batch_idx} (cont). Re-initializing.\"); train_gen_gan_cont = data_generator_gan(train_data_path, batch_size=gan_batch_size); break\n",
    "            except Exception as e: logging.error(f\"Error during continued WGAN training epoch {epoch}, batch {batch_idx}: {e}\", exc_info=True); print(f\"\\nError during continued WGAN training epoch {epoch}, batch {batch_idx}: {e}\"); continue\n",
    "        if batches_processed > 0: avg_c_loss = epoch_c_loss / batches_processed; avg_g_loss = epoch_g_loss / batches_processed; cont_c_loss_history.append(avg_c_loss); cont_g_loss_history.append(avg_g_loss); print(f\"Epoch {epoch} finished. Avg C Loss: {avg_c_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}\")\n",
    "        else: print(f\"Epoch {epoch} finished. No batches processed.\"); cont_c_loss_history.append(np.nan); cont_g_loss_history.append(np.nan)\n",
    "        if (epoch % 5 == 0) or (epoch == end_epoch):\n",
    "             try: generator_cont.save_weights(f'generator_wgan_sa_epoch_{epoch}.weights.h5'); critic_cont.save_weights(f'critic_wgan_sa_epoch_{epoch}.weights.h5'); print(f\"Saved generator and critic weights for epoch {epoch}\")\n",
    "             except Exception as e: print(f\"Error saving weights at epoch {epoch} (cont): {e}\")\n",
    "    print(f\"\\nContinued WGAN-GP training finished at epoch {end_epoch}.\")\n",
    "    plt.figure(figsize=(10, 5)); epoch_range = range(start_epoch, start_epoch + len(cont_c_loss_history)); plt.plot(epoch_range, cont_c_loss_history, label='Avg Critic Loss (Continuation)'); plt.plot(epoch_range, cont_g_loss_history, label='Avg Generator Loss (Continuation)'); plt.title(f'WGAN-GP Continued Training Losses (Epochs {start_epoch}-{end_epoch})'); plt.xlabel('Epoch'); plt.ylabel('Average Loss'); plt.xticks(list(epoch_range)); plt.legend(); plt.grid(True); plt.savefig(os.path.join(FIGURES_DIR, f'gan_loss_cont_{start_epoch}_to_{end_epoch}.png')); plt.show()\n",
    "    critic = critic_cont; generator = generator_cont\n",
    "    print(\"Updated 'critic' and 'generator' variables with weights from continued training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up Standalone Classifier training...\n",
      "Classifier Train Steps/Epoch: 3173, Validation Steps: 3106\n",
      "Attempting to load critic weights from epoch 200: critic_wgan_sa_epoch_200.weights.h5\n",
      "Recreating base critic structure...\n",
      "Building critic structure...\n",
      "Critic structure built.\n",
      "Loading critic weights from: critic_wgan_sa_epoch_200.weights.h5\n",
      "Critic weights loaded successfully.\n",
      "Using input shape for classifier model: (80, 126, 1)\n",
      "Copied 10 layers from the pre-trained critic.\n",
      "\n",
      "--- Spoof Detector (Classifier) Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"spoof_detector\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"spoof_detector\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,153</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttention</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40960</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classifier_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,961</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m1,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_9 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m131,072\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_10 (\u001b[38;5;33mLeakyReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m37,153\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttention\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m524,288\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_11 (\u001b[38;5;33mLeakyReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40960\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classifier_output (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m40,961\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">735,330</span> (2.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m735,330\u001b[0m (2.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">735,330</span> (2.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m735,330\u001b[0m (2.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up callbacks monitoring 'val_auc'...\n",
      "PlotTrainingHistory initialized to monitor 'val_auc' (max) for best checkpoints.\n",
      "Compiling classifier model...\n",
      "\n",
      "Starting Standalone Classifier fine-tuning...\n",
      "Using class weights: {1: 4.9186046511627906, 0: 0.5565789473684211} for path datasetNEW/train\n",
      "Epoch 1/60\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7917 - auc: 0.8781 - loss: 0.4107Using class weights: {1: 4.875196232339089, 0: 0.557140294223179} for path datasetNEW/dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 15:28:34.143289: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189', 248 bytes spill stores, 248 bytes spill loads\n",
      "\n",
      "2025-04-09 15:28:34.321981: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-04-09 15:28:34.586236: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2025-04-09 15:28:34.599977: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189_0', 1648 bytes spill stores, 2076 bytes spill loads\n",
      "\n",
      "2025-04-09 15:28:34.634035: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189', 220 bytes spill stores, 184 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_auc improved from -inf to 0.93192, saving model to ./training_checkpoints_spoof_detector_wgan_sa/clf_ep01-auc0.9319.weights.h5\n",
      "\n",
      "Metric 'val_auc' improved from -inf to 0.93192 at epoch 1. Generating checkpoint plot.\n",
      "\n",
      "Checkpoint performance plot saved to training_figures_wgan_sa/spoof_detector_wgan_sa_ep200_finetune_checkpoint_epoch_1.png\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 148ms/step - accuracy: 0.7917 - auc: 0.8781 - loss: 0.4107 - val_accuracy: 0.8585 - val_auc: 0.9319 - val_loss: 0.2984 - learning_rate: 5.0000e-05\n",
      "Epoch 2/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8776 - auc: 0.9559 - loss: 0.2499\n",
      "Epoch 2: val_auc improved from 0.93192 to 0.94524, saving model to ./training_checkpoints_spoof_detector_wgan_sa/clf_ep02-auc0.9452.weights.h5\n",
      "\n",
      "Metric 'val_auc' improved from 0.93192 to 0.94524 at epoch 2. Generating checkpoint plot.\n",
      "\n",
      "Checkpoint performance plot saved to training_figures_wgan_sa/spoof_detector_wgan_sa_ep200_finetune_checkpoint_epoch_2.png\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 143ms/step - accuracy: 0.8776 - auc: 0.9559 - loss: 0.2498 - val_accuracy: 0.9039 - val_auc: 0.9452 - val_loss: 0.3794 - learning_rate: 5.0000e-05\n",
      "Epoch 3/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9076 - auc: 0.9730 - loss: 0.1943\n",
      "Epoch 3: val_auc improved from 0.94524 to 0.95531, saving model to ./training_checkpoints_spoof_detector_wgan_sa/clf_ep03-auc0.9553.weights.h5\n",
      "\n",
      "Metric 'val_auc' improved from 0.94524 to 0.95531 at epoch 3. Generating checkpoint plot.\n",
      "\n",
      "Checkpoint performance plot saved to training_figures_wgan_sa/spoof_detector_wgan_sa_ep200_finetune_checkpoint_epoch_3.png\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 147ms/step - accuracy: 0.9076 - auc: 0.9730 - loss: 0.1943 - val_accuracy: 0.9195 - val_auc: 0.9553 - val_loss: 0.3742 - learning_rate: 5.0000e-05\n",
      "Epoch 4/60\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9234 - auc: 0.9811 - loss: 0.1648\n",
      "Epoch 4: val_auc did not improve from 0.95531\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m470s\u001b[0m 148ms/step - accuracy: 0.9234 - auc: 0.9811 - loss: 0.1648 - val_accuracy: 0.9270 - val_auc: 0.9497 - val_loss: 0.5598 - learning_rate: 5.0000e-05\n",
      "Epoch 5/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9387 - auc: 0.9875 - loss: 0.1349\n",
      "Epoch 5: val_auc improved from 0.95531 to 0.95632, saving model to ./training_checkpoints_spoof_detector_wgan_sa/clf_ep05-auc0.9563.weights.h5\n",
      "\n",
      "Metric 'val_auc' improved from 0.95531 to 0.95632 at epoch 5. Generating checkpoint plot.\n",
      "\n",
      "Checkpoint performance plot saved to training_figures_wgan_sa/spoof_detector_wgan_sa_ep200_finetune_checkpoint_epoch_5.png\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m470s\u001b[0m 148ms/step - accuracy: 0.9387 - auc: 0.9875 - loss: 0.1349 - val_accuracy: 0.9249 - val_auc: 0.9563 - val_loss: 0.3841 - learning_rate: 5.0000e-05\n",
      "Epoch 6/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9407 - auc: 0.9868 - loss: 0.1379\n",
      "Epoch 6: val_auc did not improve from 0.95632\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 150ms/step - accuracy: 0.9407 - auc: 0.9868 - loss: 0.1379 - val_accuracy: 0.9321 - val_auc: 0.9513 - val_loss: 0.4903 - learning_rate: 5.0000e-05\n",
      "Epoch 7/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9478 - auc: 0.9883 - loss: 0.1226\n",
      "Epoch 7: val_auc improved from 0.95632 to 0.95855, saving model to ./training_checkpoints_spoof_detector_wgan_sa/clf_ep07-auc0.9586.weights.h5\n",
      "\n",
      "Metric 'val_auc' improved from 0.95632 to 0.95855 at epoch 7. Generating checkpoint plot.\n",
      "\n",
      "Checkpoint performance plot saved to training_figures_wgan_sa/spoof_detector_wgan_sa_ep200_finetune_checkpoint_epoch_7.png\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 149ms/step - accuracy: 0.9478 - auc: 0.9883 - loss: 0.1226 - val_accuracy: 0.8615 - val_auc: 0.9586 - val_loss: 0.2908 - learning_rate: 5.0000e-05\n",
      "Epoch 8/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9527 - auc: 0.9914 - loss: 0.1111\n",
      "Epoch 8: val_auc did not improve from 0.95855\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 156ms/step - accuracy: 0.9527 - auc: 0.9914 - loss: 0.1111 - val_accuracy: 0.9427 - val_auc: 0.9468 - val_loss: 0.7307 - learning_rate: 5.0000e-05\n",
      "Epoch 9/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9534 - auc: 0.9913 - loss: 0.1146\n",
      "Epoch 9: val_auc improved from 0.95855 to 0.96153, saving model to ./training_checkpoints_spoof_detector_wgan_sa/clf_ep09-auc0.9615.weights.h5\n",
      "\n",
      "Metric 'val_auc' improved from 0.95855 to 0.96153 at epoch 9. Generating checkpoint plot.\n",
      "\n",
      "Checkpoint performance plot saved to training_figures_wgan_sa/spoof_detector_wgan_sa_ep200_finetune_checkpoint_epoch_9.png\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 156ms/step - accuracy: 0.9534 - auc: 0.9913 - loss: 0.1146 - val_accuracy: 0.9426 - val_auc: 0.9615 - val_loss: 0.4382 - learning_rate: 5.0000e-05\n",
      "Epoch 10/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9668 - auc: 0.9952 - loss: 0.0785\n",
      "Epoch 10: val_auc improved from 0.96153 to 0.96857, saving model to ./training_checkpoints_spoof_detector_wgan_sa/clf_ep10-auc0.9686.weights.h5\n",
      "\n",
      "Metric 'val_auc' improved from 0.96153 to 0.96857 at epoch 10. Generating checkpoint plot.\n",
      "\n",
      "Checkpoint performance plot saved to training_figures_wgan_sa/spoof_detector_wgan_sa_ep200_finetune_checkpoint_epoch_10.png\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 158ms/step - accuracy: 0.9668 - auc: 0.9952 - loss: 0.0785 - val_accuracy: 0.9425 - val_auc: 0.9686 - val_loss: 0.3532 - learning_rate: 5.0000e-05\n",
      "Epoch 11/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9627 - auc: 0.9942 - loss: 0.0872\n",
      "Epoch 11: val_auc did not improve from 0.96857\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m512s\u001b[0m 162ms/step - accuracy: 0.9627 - auc: 0.9942 - loss: 0.0872 - val_accuracy: 0.9416 - val_auc: 0.9655 - val_loss: 0.3689 - learning_rate: 5.0000e-05\n",
      "Epoch 12/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9722 - auc: 0.9964 - loss: 0.0651\n",
      "Epoch 12: val_auc improved from 0.96857 to 0.96899, saving model to ./training_checkpoints_spoof_detector_wgan_sa/clf_ep12-auc0.9690.weights.h5\n",
      "\n",
      "Metric 'val_auc' improved from 0.96857 to 0.96899 at epoch 12. Generating checkpoint plot.\n",
      "\n",
      "Checkpoint performance plot saved to training_figures_wgan_sa/spoof_detector_wgan_sa_ep200_finetune_checkpoint_epoch_12.png\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 136ms/step - accuracy: 0.9722 - auc: 0.9964 - loss: 0.0652 - val_accuracy: 0.9456 - val_auc: 0.9690 - val_loss: 0.3511 - learning_rate: 5.0000e-05\n",
      "Epoch 13/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9708 - auc: 0.9953 - loss: 0.0739\n",
      "Epoch 13: val_auc improved from 0.96899 to 0.97169, saving model to ./training_checkpoints_spoof_detector_wgan_sa/clf_ep13-auc0.9717.weights.h5\n",
      "\n",
      "Metric 'val_auc' improved from 0.96899 to 0.97169 at epoch 13. Generating checkpoint plot.\n",
      "\n",
      "Checkpoint performance plot saved to training_figures_wgan_sa/spoof_detector_wgan_sa_ep200_finetune_checkpoint_epoch_13.png\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 132ms/step - accuracy: 0.9708 - auc: 0.9953 - loss: 0.0739 - val_accuracy: 0.9434 - val_auc: 0.9717 - val_loss: 0.3154 - learning_rate: 5.0000e-05\n",
      "Epoch 14/60\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9722 - auc: 0.9954 - loss: 0.0697\n",
      "Epoch 14: val_auc did not improve from 0.97169\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 131ms/step - accuracy: 0.9722 - auc: 0.9955 - loss: 0.0697 - val_accuracy: 0.9528 - val_auc: 0.9658 - val_loss: 0.4520 - learning_rate: 5.0000e-05\n",
      "Epoch 15/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9743 - auc: 0.9966 - loss: 0.0641\n",
      "Epoch 15: val_auc improved from 0.97169 to 0.97692, saving model to ./training_checkpoints_spoof_detector_wgan_sa/clf_ep15-auc0.9769.weights.h5\n",
      "\n",
      "Metric 'val_auc' improved from 0.97169 to 0.97692 at epoch 15. Generating checkpoint plot.\n",
      "\n",
      "Checkpoint performance plot saved to training_figures_wgan_sa/spoof_detector_wgan_sa_ep200_finetune_checkpoint_epoch_15.png\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 184ms/step - accuracy: 0.9743 - auc: 0.9966 - loss: 0.0641 - val_accuracy: 0.9452 - val_auc: 0.9769 - val_loss: 0.2546 - learning_rate: 5.0000e-05\n",
      "Epoch 16/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9718 - auc: 0.9949 - loss: 0.0742\n",
      "Epoch 16: val_auc did not improve from 0.97692\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 131ms/step - accuracy: 0.9718 - auc: 0.9949 - loss: 0.0742 - val_accuracy: 0.9449 - val_auc: 0.9755 - val_loss: 0.2757 - learning_rate: 5.0000e-05\n",
      "Epoch 17/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9783 - auc: 0.9976 - loss: 0.0540\n",
      "Epoch 17: val_auc did not improve from 0.97692\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 133ms/step - accuracy: 0.9783 - auc: 0.9976 - loss: 0.0540 - val_accuracy: 0.9557 - val_auc: 0.9691 - val_loss: 0.3950 - learning_rate: 5.0000e-05\n",
      "Epoch 18/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9801 - auc: 0.9976 - loss: 0.0526\n",
      "Epoch 18: val_auc did not improve from 0.97692\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 138ms/step - accuracy: 0.9801 - auc: 0.9976 - loss: 0.0526 - val_accuracy: 0.9562 - val_auc: 0.9692 - val_loss: 0.4029 - learning_rate: 5.0000e-05\n",
      "Epoch 19/60\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9812 - auc: 0.9979 - loss: 0.0495\n",
      "Epoch 19: val_auc did not improve from 0.97692\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 130ms/step - accuracy: 0.9812 - auc: 0.9979 - loss: 0.0495 - val_accuracy: 0.9565 - val_auc: 0.9580 - val_loss: 0.6255 - learning_rate: 5.0000e-05\n",
      "Epoch 20/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9813 - auc: 0.9969 - loss: 0.0526\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 20: val_auc did not improve from 0.97692\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 131ms/step - accuracy: 0.9813 - auc: 0.9969 - loss: 0.0526 - val_accuracy: 0.9599 - val_auc: 0.9680 - val_loss: 0.4561 - learning_rate: 5.0000e-05\n",
      "Epoch 21/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9863 - auc: 0.9986 - loss: 0.0337\n",
      "Epoch 21: val_auc did not improve from 0.97692\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 137ms/step - accuracy: 0.9863 - auc: 0.9986 - loss: 0.0337 - val_accuracy: 0.9609 - val_auc: 0.9670 - val_loss: 0.4573 - learning_rate: 2.0000e-05\n",
      "Epoch 22/60\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9873 - auc: 0.9982 - loss: 0.0353\n",
      "Epoch 22: val_auc did not improve from 0.97692\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 130ms/step - accuracy: 0.9873 - auc: 0.9982 - loss: 0.0353 - val_accuracy: 0.9608 - val_auc: 0.9565 - val_loss: 0.6209 - learning_rate: 2.0000e-05\n",
      "Epoch 23/60\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9897 - auc: 0.9993 - loss: 0.0276\n",
      "Epoch 23: val_auc improved from 0.97692 to 0.97991, saving model to ./training_checkpoints_spoof_detector_wgan_sa/clf_ep23-auc0.9799.weights.h5\n",
      "\n",
      "Metric 'val_auc' improved from 0.97692 to 0.97991 at epoch 23. Generating checkpoint plot.\n",
      "\n",
      "Checkpoint performance plot saved to training_figures_wgan_sa/spoof_detector_wgan_sa_ep200_finetune_checkpoint_epoch_23.png\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 130ms/step - accuracy: 0.9897 - auc: 0.9993 - loss: 0.0276 - val_accuracy: 0.9618 - val_auc: 0.9799 - val_loss: 0.2709 - learning_rate: 2.0000e-05\n",
      "Epoch 24/60\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9898 - auc: 0.9987 - loss: 0.0262\n",
      "Epoch 24: val_auc did not improve from 0.97991\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 128ms/step - accuracy: 0.9898 - auc: 0.9987 - loss: 0.0262 - val_accuracy: 0.9620 - val_auc: 0.9693 - val_loss: 0.4321 - learning_rate: 2.0000e-05\n",
      "Epoch 25/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9859 - auc: 0.9983 - loss: 0.0369\n",
      "Epoch 25: val_auc did not improve from 0.97991\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 127ms/step - accuracy: 0.9859 - auc: 0.9983 - loss: 0.0369 - val_accuracy: 0.9653 - val_auc: 0.9769 - val_loss: 0.3553 - learning_rate: 2.0000e-05\n",
      "Epoch 26/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9905 - auc: 0.9992 - loss: 0.0242\n",
      "Epoch 26: val_auc improved from 0.97991 to 0.98005, saving model to ./training_checkpoints_spoof_detector_wgan_sa/clf_ep26-auc0.9800.weights.h5\n",
      "\n",
      "Metric 'val_auc' improved from 0.97991 to 0.98005 at epoch 26. Generating checkpoint plot.\n",
      "\n",
      "Checkpoint performance plot saved to training_figures_wgan_sa/spoof_detector_wgan_sa_ep200_finetune_checkpoint_epoch_26.png\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 128ms/step - accuracy: 0.9905 - auc: 0.9992 - loss: 0.0242 - val_accuracy: 0.9682 - val_auc: 0.9800 - val_loss: 0.3320 - learning_rate: 2.0000e-05\n",
      "Epoch 27/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9910 - auc: 0.9994 - loss: 0.0245\n",
      "Epoch 27: val_auc did not improve from 0.98005\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 130ms/step - accuracy: 0.9910 - auc: 0.9994 - loss: 0.0245 - val_accuracy: 0.9678 - val_auc: 0.9788 - val_loss: 0.3263 - learning_rate: 2.0000e-05\n",
      "Epoch 28/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9909 - auc: 0.9987 - loss: 0.0255\n",
      "Epoch 28: val_auc improved from 0.98005 to 0.98099, saving model to ./training_checkpoints_spoof_detector_wgan_sa/clf_ep28-auc0.9810.weights.h5\n",
      "\n",
      "Metric 'val_auc' improved from 0.98005 to 0.98099 at epoch 28. Generating checkpoint plot.\n",
      "\n",
      "Checkpoint performance plot saved to training_figures_wgan_sa/spoof_detector_wgan_sa_ep200_finetune_checkpoint_epoch_28.png\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 131ms/step - accuracy: 0.9909 - auc: 0.9987 - loss: 0.0255 - val_accuracy: 0.9688 - val_auc: 0.9810 - val_loss: 0.3138 - learning_rate: 2.0000e-05\n",
      "Epoch 29/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9911 - auc: 0.9994 - loss: 0.0219\n",
      "Epoch 29: val_auc did not improve from 0.98099\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 126ms/step - accuracy: 0.9911 - auc: 0.9994 - loss: 0.0219 - val_accuracy: 0.9643 - val_auc: 0.9638 - val_loss: 0.5614 - learning_rate: 2.0000e-05\n",
      "Epoch 30/60\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9907 - auc: 0.9991 - loss: 0.0245\n",
      "Epoch 30: val_auc did not improve from 0.98099\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 126ms/step - accuracy: 0.9907 - auc: 0.9991 - loss: 0.0245 - val_accuracy: 0.9664 - val_auc: 0.9799 - val_loss: 0.3121 - learning_rate: 2.0000e-05\n",
      "Epoch 31/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9922 - auc: 0.9991 - loss: 0.0210\n",
      "Epoch 31: val_auc did not improve from 0.98099\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 126ms/step - accuracy: 0.9922 - auc: 0.9991 - loss: 0.0210 - val_accuracy: 0.9661 - val_auc: 0.9705 - val_loss: 0.4719 - learning_rate: 2.0000e-05\n",
      "Epoch 32/60\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9895 - auc: 0.9994 - loss: 0.0266\n",
      "Epoch 32: val_auc did not improve from 0.98099\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 126ms/step - accuracy: 0.9895 - auc: 0.9994 - loss: 0.0266 - val_accuracy: 0.9666 - val_auc: 0.9728 - val_loss: 0.4658 - learning_rate: 2.0000e-05\n",
      "Epoch 33/60\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9924 - auc: 0.9993 - loss: 0.0186\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 7.999999797903002e-06.\n",
      "\n",
      "Epoch 33: val_auc did not improve from 0.98099\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 126ms/step - accuracy: 0.9924 - auc: 0.9993 - loss: 0.0186 - val_accuracy: 0.9644 - val_auc: 0.9786 - val_loss: 0.3010 - learning_rate: 2.0000e-05\n",
      "Epoch 34/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9914 - auc: 0.9996 - loss: 0.0221\n",
      "Epoch 34: val_auc did not improve from 0.98099\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 126ms/step - accuracy: 0.9914 - auc: 0.9996 - loss: 0.0221 - val_accuracy: 0.9651 - val_auc: 0.9676 - val_loss: 0.5447 - learning_rate: 8.0000e-06\n",
      "Epoch 35/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9929 - auc: 0.9994 - loss: 0.0224\n",
      "Epoch 35: val_auc did not improve from 0.98099\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 127ms/step - accuracy: 0.9929 - auc: 0.9994 - loss: 0.0224 - val_accuracy: 0.9680 - val_auc: 0.9739 - val_loss: 0.4272 - learning_rate: 8.0000e-06\n",
      "Epoch 36/60\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9940 - auc: 0.9997 - loss: 0.0167\n",
      "Epoch 36: val_auc did not improve from 0.98099\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 134ms/step - accuracy: 0.9940 - auc: 0.9997 - loss: 0.0167 - val_accuracy: 0.9703 - val_auc: 0.9786 - val_loss: 0.3618 - learning_rate: 8.0000e-06\n",
      "Epoch 37/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9940 - auc: 0.9997 - loss: 0.0166\n",
      "Epoch 37: val_auc did not improve from 0.98099\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 133ms/step - accuracy: 0.9940 - auc: 0.9997 - loss: 0.0166 - val_accuracy: 0.9705 - val_auc: 0.9763 - val_loss: 0.3925 - learning_rate: 8.0000e-06\n",
      "Epoch 38/60\n",
      "\u001b[1m3172/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9930 - auc: 0.9984 - loss: 0.0215\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 3.199999991920777e-06.\n",
      "\n",
      "Epoch 38: val_auc did not improve from 0.98099\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 132ms/step - accuracy: 0.9930 - auc: 0.9984 - loss: 0.0215 - val_accuracy: 0.9719 - val_auc: 0.9803 - val_loss: 0.3439 - learning_rate: 8.0000e-06\n",
      "Epoch 39/60\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9944 - auc: 0.9997 - loss: 0.0149\n",
      "Epoch 39: val_auc did not improve from 0.98099\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 132ms/step - accuracy: 0.9944 - auc: 0.9997 - loss: 0.0149 - val_accuracy: 0.9697 - val_auc: 0.9752 - val_loss: 0.4302 - learning_rate: 3.2000e-06\n",
      "Epoch 40/60\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9959 - auc: 0.9999 - loss: 0.0114\n",
      "Epoch 40: val_auc did not improve from 0.98099\n",
      "\u001b[1m3173/3173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 131ms/step - accuracy: 0.9959 - auc: 0.9999 - loss: 0.0114 - val_accuracy: 0.9702 - val_auc: 0.9755 - val_loss: 0.4295 - learning_rate: 3.2000e-06\n",
      "Epoch 40: early stopping\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "\n",
      "Saving final best classifier weights (restored based on val_auc) to: spoof_detector_ep200_finetuned_best_val_auc.weights.h5\n",
      "Final best weights saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Train Standalone Classifier (loading Critic Epoch 180 - Reverted to Monitor val_auc)\n",
    "\n",
    "# Required Imports (ensure they are loaded from Cell 1 or here)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import fnmatch # Needed if finding latest checkpoint dynamically, but not for specific load\n",
    "import matplotlib.pyplot as plt # Needed for PlotTrainingHistory\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint # Needed Callbacks\n",
    "from tensorflow.keras.models import Sequential # Needed for building detector\n",
    "from tensorflow.keras.layers import Dense, Input # Added Input layer\n",
    "from tensorflow.keras.optimizers import Adam # Needed for compiling\n",
    "\n",
    "# --- PlotTrainingHistory CLASS DEFINITION HERE ---\n",
    "# (Ensure the version with checkpoint plotting logic is included)\n",
    "class PlotTrainingHistory(Callback):\n",
    "    def __init__(self, model_name='model', monitor='val_loss', mode='min'):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name; self.monitor = monitor; self.mode = mode\n",
    "        self.acc = []; self.val_acc = []; self.loss = []; self.val_loss = []\n",
    "        self.auc = []; self.val_auc = []\n",
    "        if self.mode == 'min': self.best_value = np.inf; self.monitor_op = np.less\n",
    "        elif self.mode == 'max': self.best_value = -np.inf; self.monitor_op = np.greater\n",
    "        else: raise ValueError(f\"Unsupported mode: {self.mode}\")\n",
    "        print(f\"PlotTrainingHistory initialized to monitor '{self.monitor}' ({self.mode}) for best checkpoints.\")\n",
    "    def plot_current_history(self, epoch, filename_suffix=\"\"):\n",
    "        plt.figure(figsize=(18, 5))\n",
    "        plt.subplot(1, 3, 1);\n",
    "        if any(v is not None for v in self.acc): plt.plot(self.acc, label='Training Accuracy')\n",
    "        if any(v is not None for v in self.val_acc): plt.plot(self.val_acc, label='Validation Accuracy')\n",
    "        plt.title(f'Model Accuracy (Epoch {epoch+1}{filename_suffix})'); plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "        if any(v is not None for v in self.acc) or any(v is not None for v in self.val_acc): plt.legend()\n",
    "        plt.subplot(1, 3, 2);\n",
    "        if any(v is not None for v in self.loss): plt.plot(self.loss, label='Training Loss')\n",
    "        if any(v is not None for v in self.val_loss): plt.plot(self.val_loss, label='Validation Loss')\n",
    "        plt.title(f'Model Loss (Epoch {epoch+1}{filename_suffix})'); plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "        if any(v is not None for v in self.loss) or any(v is not None for v in self.val_loss): plt.legend()\n",
    "        plt.subplot(1, 3, 3);\n",
    "        if any(v is not None for v in self.auc): plt.plot(self.auc, label='Training AUC')\n",
    "        if any(v is not None for v in self.val_auc): plt.plot(self.val_auc, label='Validation AUC')\n",
    "        plt.title(f'Model AUC (Epoch {epoch+1}{filename_suffix})'); plt.xlabel('Epoch'); plt.ylabel('AUC')\n",
    "        if any(v is not None for v in self.auc) or any(v is not None for v in self.val_auc): plt.legend()\n",
    "        plt.tight_layout(); filepath = os.path.join(FIGURES_DIR, f'{self.model_name}{filename_suffix}_epoch_{epoch+1}.png')\n",
    "        saved_successfully = False\n",
    "        try: plt.savefig(filepath); plt.close(); saved_successfully = True\n",
    "        except Exception as e: print(f\"\\nError saving plot to {filepath}: {e}\"); plt.close()\n",
    "        if saved_successfully and filename_suffix == \"_checkpoint\": print(f'\\nCheckpoint performance plot saved to {filepath}')\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}; self.loss.append(logs.get('loss')); self.val_loss.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('accuracy')); self.val_acc.append(logs.get('val_accuracy'))\n",
    "        self.auc.append(logs.get('auc')); self.val_auc.append(logs.get('val_auc'))\n",
    "        self.plot_current_history(epoch, filename_suffix=\"_history\")\n",
    "        current_value = logs.get(self.monitor)\n",
    "        if current_value is None: print(f\"Warning: Monitored metric '{self.monitor}' not found in logs for epoch {epoch+1}. Cannot check for checkpoint plot.\"); return\n",
    "        if self.monitor_op(current_value, self.best_value):\n",
    "            print(f\"\\nMetric '{self.monitor}' improved from {self.best_value:.5f} to {current_value:.5f} at epoch {epoch+1}. Generating checkpoint plot.\")\n",
    "            self.best_value = current_value; self.plot_current_history(epoch, filename_suffix=\"_checkpoint\")\n",
    "# --- END OF PlotTrainingHistory CLASS DEFINITION ---\n",
    "\n",
    "# --- ADD count_total_files FUNCTION DEFINITION HERE ---\n",
    "def count_total_files(path):\n",
    "    # ... (function definition as provided before) ...\n",
    "    real_count = 0; fake_count = 0; real_dir = os.path.join(path, 'real'); fake_dir = os.path.join(path, 'fake')\n",
    "    if os.path.exists(real_dir) and os.path.isdir(real_dir):\n",
    "        try: real_count = len([f for f in os.listdir(real_dir) if f.endswith('.wav')])\n",
    "        except Exception as e: print(f\"Warning: Error listing files in {real_dir}: {e}\")\n",
    "    if os.path.exists(fake_dir) and os.path.isdir(fake_dir):\n",
    "        try: fake_count = len([f for f in os.listdir(fake_dir) if f.endswith('.wav')])\n",
    "        except Exception as e: print(f\"Warning: Error listing files in {fake_dir}: {e}\")\n",
    "    total_count = real_count + fake_count; return total_count\n",
    "# --- END OF count_total_files FUNCTION DEFINITION ---\n",
    "\n",
    "# --- Ensure necessary functions/classes from previous cells are available ---\n",
    "if 'data_generator_classifier' not in locals(): raise NameError(\"data_generator_classifier function not defined.\")\n",
    "if 'SelfAttention' not in locals(): raise NameError(\"SelfAttention class definition not found.\")\n",
    "if 'create_critic' not in locals(): raise NameError(\"create_critic function not found.\")\n",
    "\n",
    "print(\"\\nSetting up Standalone Classifier training...\")\n",
    "\n",
    "# --- Get Parameters ---\n",
    "if 'classifier_batch_size' not in locals(): classifier_batch_size = 8\n",
    "if 'train_data_path' not in locals(): train_data_path = 'datasetNEW/train'\n",
    "if 'dev_data_path' not in locals(): dev_data_path = 'datasetNEW/dev'\n",
    "if 'N_MELS' not in locals(): N_MELS = 80\n",
    "if 'TARGET_FRAMES' not in locals(): TARGET_FRAMES = 126\n",
    "if 'mel_spectrogram_shape' not in locals(): mel_spectrogram_shape = (N_MELS, TARGET_FRAMES)\n",
    "if 'FIGURES_DIR' not in locals(): FIGURES_DIR = 'training_figures_wgan_sa'; os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "if 'classifier_epochs' not in locals(): classifier_epochs = 50\n",
    "\n",
    "# --- Create Data Generators ---\n",
    "train_gen_clf = data_generator_classifier(train_data_path, batch_size=classifier_batch_size)\n",
    "dev_gen_clf = data_generator_classifier(dev_data_path, batch_size=classifier_batch_size, shuffle=False)\n",
    "\n",
    "# --- Calculate steps ---\n",
    "train_samples_count = count_total_files(train_data_path)\n",
    "dev_samples_count = count_total_files(dev_data_path)\n",
    "if classifier_batch_size <= 0: raise ValueError(\"Classifier batch size must be positive.\")\n",
    "clf_steps_per_epoch = int(np.ceil(train_samples_count / float(classifier_batch_size))) if train_samples_count > 0 else 0\n",
    "clf_validation_steps = int(np.ceil(dev_samples_count / float(classifier_batch_size))) if dev_samples_count > 0 else 0\n",
    "print(f\"Classifier Train Steps/Epoch: {clf_steps_per_epoch}, Validation Steps: {clf_validation_steps}\")\n",
    "\n",
    "\n",
    "# --- Build the Spoof Detector Model FROM SPECIFIC SAVED CRITIC WEIGHTS ---\n",
    "load_epoch = 200 # Still loading critic from epoch 180\n",
    "critic_weights_load_path = f'critic_wgan_sa_epoch_{load_epoch}.weights.h5'\n",
    "print(f\"Attempting to load critic weights from epoch {load_epoch}: {critic_weights_load_path}\")\n",
    "\n",
    "# Recreate critic structure, build, and load weights\n",
    "# ... (Structure recreation, build, and load logic remains the same as previous correct versions) ...\n",
    "print(\"Recreating base critic structure...\")\n",
    "critic_base = create_critic(mel_spectrogram_shape)\n",
    "print(\"Building critic structure...\")\n",
    "dummy_critic_input = tf.zeros((1, N_MELS, TARGET_FRAMES, 1), dtype=tf.float32)\n",
    "try: _ = critic_base(dummy_critic_input, training=False); print(\"Critic structure built.\")\n",
    "except Exception as e: raise RuntimeError(f\"Error building critic structure: {e}\")\n",
    "if os.path.exists(critic_weights_load_path):\n",
    "    print(f\"Loading critic weights from: {critic_weights_load_path}\")\n",
    "    try: critic_base.load_weights(critic_weights_load_path); print(\"Critic weights loaded successfully.\")\n",
    "    except Exception as e: print(f\"ERROR loading weights: {e}. Training from scratch.\")\n",
    "else: print(f\"Weights file not found: {critic_weights_load_path}. Training from scratch.\")\n",
    "\n",
    "\n",
    "# --- Create the final classifier ---\n",
    "critic_base.trainable = True\n",
    "spoof_detector = Sequential(name='spoof_detector')\n",
    "spoof_detector.add(Input(shape=critic_base.input_shape[1:], name='classifier_input'))\n",
    "# ... (Copy layers logic remains the same) ...\n",
    "print(f\"Using input shape for classifier model: {critic_base.input_shape[1:]}\")\n",
    "layers_copied_count = 0\n",
    "for layer in critic_base.layers[:-1]: spoof_detector.add(layer); layers_copied_count += 1\n",
    "print(f\"Copied {layers_copied_count} layers from the pre-trained critic.\")\n",
    "spoof_detector.add(Dense(1, activation='sigmoid', name='classifier_output'))\n",
    "print(\"\\n--- Spoof Detector (Classifier) Summary ---\")\n",
    "spoof_detector.summary()\n",
    "\n",
    "\n",
    "# --- Callbacks for Classifier Training (Reverted to Monitor val_auc) ---\n",
    "print(\"Setting up callbacks monitoring 'val_auc'...\")\n",
    "monitor_metric = 'val_auc' # <-- MONITOR AUC\n",
    "monitor_mode = 'max'        # <-- MODE MAX\n",
    "\n",
    "# Original patience values might be suitable for AUC\n",
    "reduce_lr_patience = 5\n",
    "early_stopping_patience = 12\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor=monitor_metric, mode=monitor_mode, factor=0.4, patience=reduce_lr_patience, min_lr=5e-8, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor=monitor_metric, mode=monitor_mode, patience=early_stopping_patience, restore_best_weights=True, verbose=1)\n",
    "\n",
    "checkpoint_dir_clf = './training_checkpoints_spoof_detector_wgan_sa'\n",
    "os.makedirs(checkpoint_dir_clf, exist_ok=True)\n",
    "# Checkpoint filename format to include AUC score\n",
    "checkpoint_filepath_clf = os.path.join(checkpoint_dir_clf, f\"clf_ep{{epoch:02d}}-auc{{{monitor_metric}:.4f}}.weights.h5\") # <-- FILENAME FORMAT REVERTED\n",
    "checkpoint_callback_clf = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_clf,\n",
    "    save_weights_only=True,\n",
    "    monitor=monitor_metric, # MONITOR VAL_AUC\n",
    "    mode=monitor_mode,    # MODE MAX\n",
    "    save_best_only=True,  # Will save files like clf_ep28-auc0.9800.weights.h5 if that's the best\n",
    "    verbose=1\n",
    ")\n",
    "# Plotting callback - PASS THE MONITORING INFO TO IT\n",
    "plot_training_callback = PlotTrainingHistory(\n",
    "    model_name=f'spoof_detector_wgan_sa_ep{load_epoch}_finetune', # Name remains descriptive\n",
    "    monitor=monitor_metric, # Pass val_auc\n",
    "    mode=monitor_mode      # Pass max\n",
    ")\n",
    "\n",
    "\n",
    "# --- Compile and Train the Classifier ---\n",
    "print(\"Compiling classifier model...\")\n",
    "classifier_optimizer = Adam(learning_rate=5e-5) # Revert to original fine-tune LR or adjust as needed\n",
    "spoof_detector.compile(optimizer=classifier_optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "print(\"\\nStarting Standalone Classifier fine-tuning...\")\n",
    "\n",
    "if clf_steps_per_epoch > 0 and clf_validation_steps > 0 and train_gen_clf is not None and dev_gen_clf is not None:\n",
    "    history = spoof_detector.fit(\n",
    "        train_gen_clf,\n",
    "        steps_per_epoch=clf_steps_per_epoch,\n",
    "        epochs=classifier_epochs,\n",
    "        validation_data=dev_gen_clf,\n",
    "        validation_steps=clf_validation_steps,\n",
    "        callbacks=[reduce_lr, early_stopping, checkpoint_callback_clf, plot_training_callback],\n",
    "    )\n",
    "\n",
    "    # --- Save the final BEST weights (restored by EarlyStopping based on val_auc) ---\n",
    "    final_weights_path = f'spoof_detector_ep{load_epoch}_finetuned_best_{monitor_metric}.weights.h5' # <-- FILENAME REVERTED\n",
    "    print(f\"\\nSaving final best classifier weights (restored based on {monitor_metric}) to: {final_weights_path}\")\n",
    "    try:\n",
    "        spoof_detector.save_weights(final_weights_path)\n",
    "        print(\"Final best weights saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving final best weights: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping classifier training: steps are zero or generators are None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Spoof Detector using ep28-auc0.9800 checkpoint...\n",
      "Recreating model structure for evaluation...\n",
      "Building model structure...\n",
      "Model structure built.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"spoof_detector_eval\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"spoof_detector_eval\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,153</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttention</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40960</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classifier_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,961</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m1,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_12 (\u001b[38;5;33mLeakyReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m131,072\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_13 (\u001b[38;5;33mLeakyReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m37,153\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttention\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m524,288\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_14 (\u001b[38;5;33mLeakyReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40960\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classifier_output (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m40,961\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">735,330</span> (2.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m735,330\u001b[0m (2.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">735,330</span> (2.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m735,330\u001b[0m (2.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading specific weights for evaluation: training_checkpoints_spoof_detector_wgan_sa/clf_ep28-auc0.9800.weights.h5\n",
      "Weights loaded successfully.\n",
      "Compiling model...\n",
      "Model compiled.\n",
      "Initializing evaluation data generator...\n",
      "Using 8905 steps for evaluation.\n",
      "Running model.evaluate...\n",
      "Using class weights: {1: 4.842760027192386, 0: 0.557567076797846} for path datasetNEW/eval\n",
      "\u001b[1m8904/8905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8552 - auc: 0.8317 - loss: 0.8839"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 20:22:26.919269: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189', 248 bytes spill stores, 248 bytes spill loads\n",
      "\n",
      "2025-04-09 20:22:27.163628: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-04-09 20:22:27.166655: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189', 220 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "2025-04-09 20:22:27.197166: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2025-04-09 20:22:27.394241: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_189_0', 1648 bytes spill stores, 2076 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8905/8905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m670s\u001b[0m 75ms/step - accuracy: 0.8552 - auc: 0.8317 - loss: 0.8838\n",
      "\n",
      "--- Evaluation Results (ep28-auc0.9800 Weights) ---\n",
      "Loss: 0.4814\n",
      "Accuracy: 0.8452\n",
      "AUC: 0.9273\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Evaluation (Loading ep28-auc0.9800 Checkpoint Weights)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Ensure Prerequisites are Met ---\n",
    "# ... (Ensure functions/classes/variables are defined) ...\n",
    "if 'SelfAttention' not in locals(): raise NameError(\"SelfAttention class definition not found.\")\n",
    "if 'create_critic' not in locals(): raise NameError(\"create_critic function not found.\")\n",
    "if 'mel_spectrogram_shape' not in locals(): raise NameError(\"mel_spectrogram_shape not defined.\")\n",
    "if 'N_MELS' not in locals() or 'TARGET_FRAMES' not in locals(): raise NameError(\"N_MELS/TARGET_FRAMES not defined.\")\n",
    "if 'classifier_batch_size' not in locals(): classifier_batch_size = 8\n",
    "if 'eval_data_path' not in locals(): eval_data_path = 'datasetNEW/eval'\n",
    "if 'data_generator_classifier' not in locals(): raise NameError(\"data_generator_classifier not defined.\")\n",
    "if 'count_total_files' not in locals(): raise NameError(\"count_total_files not defined.\")\n",
    "\n",
    "\n",
    "print(\"\\nEvaluating Spoof Detector using ep28-auc0.9800 checkpoint...\")\n",
    "\n",
    "# --- Define Path to the SPECIFIC AUC-based Weights File ---\n",
    "# This path MUST match the file saved by ModelCheckpoint when monitoring val_auc\n",
    "model_weights_path = os.path.join('training_checkpoints_spoof_detector_wgan_sa', 'clf_ep28-auc0.9800.weights.h5') # <-- REVERTED PATH\n",
    "spoof_detector_eval = None\n",
    "eval_loss, eval_accuracy, eval_auc = None, None, None\n",
    "\n",
    "# --- Recreate the Model Structure ---\n",
    "# ... (Structure recreation and build logic remains the same) ...\n",
    "print(\"Recreating model structure for evaluation...\")\n",
    "try:\n",
    "    critic_base_eval = create_critic(mel_spectrogram_shape)\n",
    "    spoof_detector_eval = tf.keras.models.Sequential(name='spoof_detector_eval')\n",
    "    spoof_detector_eval.add(tf.keras.layers.Input(shape=critic_base_eval.input_shape[1:], name='classifier_input_eval'))\n",
    "    for layer in critic_base_eval.layers[:-1]: spoof_detector_eval.add(layer)\n",
    "    spoof_detector_eval.add(tf.keras.layers.Dense(1, activation='sigmoid', name='classifier_output'))\n",
    "    print(\"Building model structure...\")\n",
    "    dummy_input_eval = tf.zeros((1, N_MELS, TARGET_FRAMES, 1), dtype=tf.float32)\n",
    "    _ = spoof_detector_eval(dummy_input_eval, training=False); print(\"Model structure built.\")\n",
    "    spoof_detector_eval.summary()\n",
    "except Exception as e: print(f\"Error recreating model structure: {e}\"); spoof_detector_eval = None\n",
    "\n",
    "# --- Load the Specific Weights ---\n",
    "# ... (Loading logic remains the same, uses the reverted path) ...\n",
    "if spoof_detector_eval is not None:\n",
    "    if os.path.exists(model_weights_path):\n",
    "        print(f\"Loading specific weights for evaluation: {model_weights_path}\")\n",
    "        try:\n",
    "            spoof_detector_eval.load_weights(model_weights_path); print(\"Weights loaded successfully.\")\n",
    "            print(\"Compiling model...\"); spoof_detector_eval.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]); print(\"Model compiled.\")\n",
    "        except Exception as e: print(f\"Error loading specified weights: {e}\\nEvaluation cannot proceed.\"); spoof_detector_eval = None\n",
    "    else: print(f\"Specified weights file not found: {model_weights_path}\\nEvaluation cannot proceed.\"); spoof_detector_eval = None\n",
    "\n",
    "# --- Perform Evaluation ---\n",
    "# ... (Evaluation logic remains the same) ...\n",
    "if spoof_detector_eval:\n",
    "    print(\"Initializing evaluation data generator...\")\n",
    "    eval_gen_clf = data_generator_classifier(eval_data_path, batch_size=classifier_batch_size, shuffle=False)\n",
    "    eval_samples_count = count_total_files(eval_data_path)\n",
    "    eval_steps = int(np.ceil(eval_samples_count / float(classifier_batch_size))) if eval_samples_count > 0 else 0\n",
    "    print(f\"Using {eval_steps} steps for evaluation.\")\n",
    "    if eval_steps > 0 and eval_gen_clf is not None:\n",
    "        print(\"Running model.evaluate...\")\n",
    "        try:\n",
    "            results = spoof_detector_eval.evaluate(eval_gen_clf, steps=eval_steps, verbose=1)\n",
    "            print(\"\\n--- Evaluation Results (ep28-auc0.9800 Weights) ---\");\n",
    "            eval_loss = results[0]; eval_accuracy = results[1]; eval_auc = results[2]\n",
    "            print(f\"Loss: {eval_loss:.4f}\"); print(f\"Accuracy: {eval_accuracy:.4f}\"); print(f\"AUC: {eval_auc:.4f}\")\n",
    "            print(\"--------------------------------------------------\")\n",
    "        except Exception as e: print(f\"An error occurred during model.evaluate: {e}\"); eval_loss, eval_accuracy, eval_auc = None, None, None\n",
    "    else: print(\"Skipping evaluation because eval_steps is zero or generator failed.\")\n",
    "else: print(\"Skipping evaluation because the model structure could not be created or weights failed to load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Final Reports & Analysis using ep28-auc0.9800 Checkpoint...\n",
      "Recreating model structure for reporting...\n",
      "Building model structure...\n",
      "Model structure built.\n",
      "Loading specific weights for reporting: training_checkpoints_spoof_detector_wgan_sa/clf_ep28-auc0.9800.weights.h5\n",
      "Weights loaded successfully.\n",
      "Generating predictions on evaluation set for detailed reports...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567c8d97707f418293dfa84ba4bf912e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting for Reports:   0%|          | 0/8905 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using class weights: {1: 4.842760027192386, 0: 0.557567076797846} for path datasetNEW/eval\n",
      "\n",
      "--- Basic Metrics (ep28-auc0.9800 Weights) ---\n",
      "F1 Score (at threshold 0.5): 0.5407\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Threshold Analysis (ep28-auc0.9800 Weights) ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Threshold Analysis (ep28-auc0.9800 Weights) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# ... (Calculation code) ...\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbest_f1\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at threshold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_f1_threshold\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# ... (Plotting code) ...\u001b[39;00m\n\u001b[32m     99\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mP/R/F1 vs. Threshold (ep28 Weights)\u001b[39m\u001b[33m'\u001b[39m); \u001b[38;5;66;03m# Reverted Title\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'best_f1' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 15: Reporting & Analysis (using Specific Checkpoint Weights - Imports Fixed)\n",
    "\n",
    "# --- Imports (ensure these are loaded) ---\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm # For progress bar\n",
    "\n",
    "# --- ADDED ALL NECESSARY SKLEARN IMPORTS ---\n",
    "from sklearn.metrics import (confusion_matrix, f1_score, roc_curve, auc,\n",
    "                             precision_recall_curve, average_precision_score,\n",
    "                             precision_score, recall_score) # Ensure ALL are here\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "# -------------------------------------------\n",
    "\n",
    "print(\"\\nGenerating Final Reports & Analysis using Specific Checkpoint...\")\n",
    "\n",
    "# --- Parameters ---\n",
    "# Ensure these are defined: FIGURES_DIR, classifier_batch_size, eval_data_path\n",
    "if 'FIGURES_DIR' not in locals(): FIGURES_DIR = 'training_figures_wgan_sa'; os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "if 'classifier_batch_size' not in locals(): classifier_batch_size = 8\n",
    "if 'eval_data_path' not in locals(): eval_data_path = 'datasetNEW/eval'\n",
    "\n",
    "# t-DCF parameters (adjust as needed)\n",
    "p_target = 0.05\n",
    "c_miss = 1\n",
    "c_false_alarm = 1\n",
    "\n",
    "# --- Define Path to the SPECIFIC Weights File (e.g., best loss from Cell 13) ---\n",
    "# ADJUST THIS FILENAME if Cell 13 saved differently\n",
    "model_weights_path = os.path.join('training_checkpoints_spoof_detector_wgan_sa', 'clf_best_loss_ep180_finetune.weights.h5')\n",
    "# Or use the fixed name if you saved that way:\n",
    "# model_weights_path = f'spoof_detector_ep180_finetuned_best_loss.weights.h5'\n",
    "\n",
    "spoof_detector_report = None # Use a different variable name for clarity\n",
    "\n",
    "# --- Check Prerequisites & Recreate Model Structure ---\n",
    "if 'SelfAttention' not in locals(): raise NameError(\"SelfAttention class definition not found.\")\n",
    "if 'create_critic' not in locals(): raise NameError(\"create_critic function not found.\")\n",
    "if 'mel_spectrogram_shape' not in locals(): raise NameError(\"mel_spectrogram_shape not defined.\")\n",
    "if 'N_MELS' not in locals() or 'TARGET_FRAMES' not in locals(): raise NameError(\"N_MELS/TARGET_FRAMES not defined.\")\n",
    "\n",
    "print(\"Recreating model structure for reporting...\")\n",
    "try:\n",
    "    critic_base_report = create_critic(mel_spectrogram_shape)\n",
    "    spoof_detector_report = tf.keras.models.Sequential(name='spoof_detector_report')\n",
    "    spoof_detector_report.add(tf.keras.layers.Input(shape=critic_base_report.input_shape[1:])) # Define input explicitly\n",
    "    for layer in critic_base_report.layers[:-1]:\n",
    "        spoof_detector_report.add(layer)\n",
    "    spoof_detector_report.add(tf.keras.layers.Dense(1, activation='sigmoid', name='classifier_output'))\n",
    "\n",
    "    # Build the model structure (redundant if Input layer added, but safe)\n",
    "    print(\"Building model structure...\")\n",
    "    dummy_input_report = tf.zeros((1, N_MELS, TARGET_FRAMES, 1), dtype=tf.float32)\n",
    "    _ = spoof_detector_report(dummy_input_report, training=False)\n",
    "    print(\"Model structure built.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error recreating model structure: {e}\")\n",
    "    spoof_detector_report = None # Prevent proceeding\n",
    "\n",
    "\n",
    "# --- Load the Specific Weights ---\n",
    "if spoof_detector_report is not None:\n",
    "    if os.path.exists(model_weights_path):\n",
    "        print(f\"Loading specific weights for reporting: {model_weights_path}\")\n",
    "        try:\n",
    "            spoof_detector_report.load_weights(model_weights_path)\n",
    "            print(\"Weights loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading specified weights: {e}\")\n",
    "            print(\"Reporting cannot proceed.\")\n",
    "            spoof_detector_report = None # Mark as failed\n",
    "    else:\n",
    "        print(f\"Specified weights file not found: {model_weights_path}\")\n",
    "        print(\"Reporting cannot proceed.\")\n",
    "        spoof_detector_report = None # Mark as failed\n",
    "\n",
    "\n",
    "# --- Proceed only if model is loaded ---\n",
    "if spoof_detector_report:\n",
    "    # --- Get Predictions on Evaluation Set ---\n",
    "    print(\"Generating predictions on evaluation set for detailed reports...\")\n",
    "    # Ensure data_generator_classifier and count_total_files are defined\n",
    "    if 'data_generator_classifier' not in locals(): raise NameError(\"data_generator_classifier not defined.\")\n",
    "    if 'count_total_files' not in locals(): raise NameError(\"count_total_files not defined.\")\n",
    "\n",
    "    eval_gen_report = data_generator_classifier(eval_data_path, batch_size=classifier_batch_size, shuffle=False)\n",
    "    eval_samples_count_report = count_total_files(eval_data_path)\n",
    "    eval_steps_report = int(np.ceil(eval_samples_count_report / float(classifier_batch_size))) if eval_samples_count_report > 0 else 0\n",
    "\n",
    "    y_pred_scores = []\n",
    "    y_true_labels = []\n",
    "\n",
    "    if eval_steps_report > 0 and eval_gen_report is not None:\n",
    "        for _ in tqdm(range(eval_steps_report), desc=\"Predicting for Reports\"):\n",
    "            try:\n",
    "                batch_x, batch_y, _ = next(eval_gen_report)\n",
    "                if batch_x.size == 0: continue\n",
    "                batch_pred = spoof_detector_report.predict(batch_x, verbose=0)\n",
    "                y_pred_scores.extend(batch_pred.flatten())\n",
    "                y_true_labels.extend(batch_y)\n",
    "            except StopIteration: break\n",
    "            except Exception as e: print(f\"Error during prediction generation: {e}\"); continue\n",
    "\n",
    "        y_pred_scores = np.array(y_pred_scores).astype(np.float32)\n",
    "        y_true_labels = np.array(y_true_labels).astype(np.int32)\n",
    "        min_len = min(len(y_pred_scores), len(y_true_labels))\n",
    "        if min_len == 0: print(\"No predictions generated.\"); y_pred_scores = np.array([])\n",
    "        elif min_len < eval_samples_count_report: print(f\"Warning: Processed {min_len}/{eval_samples_count_report} samples.\"); y_pred_scores=y_pred_scores[:min_len]; y_true_labels=y_true_labels[:min_len]\n",
    "    else: print(\"Cannot generate predictions.\"); y_pred_scores = np.array([])\n",
    "\n",
    "    # --- Perform Analysis only if predictions exist ---\n",
    "    if y_pred_scores.size > 0:\n",
    "\n",
    "        # --- 1. Basic Metrics ---\n",
    "        print(\"\\n--- Basic Metrics ---\")\n",
    "        threshold_05 = 0.5; y_pred_binary_05 = (y_pred_scores >= threshold_05).astype(int)\n",
    "        f1_at_05 = f1_score(y_true_labels, y_pred_binary_05, zero_division=0)\n",
    "        print(f\"F1 Score (at threshold {threshold_05}): {f1_at_05:.4f}\")\n",
    "        cm = confusion_matrix(y_true_labels, y_pred_binary_05)\n",
    "        plt.figure(figsize=(8, 6)); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake (0)', 'Real (1)'], yticklabels=['Fake (0)', 'Real (1)'])\n",
    "        plt.title(f'Confusion Matrix (Counts, Threshold = {threshold_05})'); plt.ylabel('True Label'); plt.xlabel('Predicted Label')\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, f'confusion_matrix_counts_{os.path.basename(model_weights_path)}.png')); plt.show()\n",
    "        # Percentage CM\n",
    "        cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'): cm_perc = cm / cm_sum.astype(float) * 100; cm_perc = np.nan_to_num(cm_perc)\n",
    "        plt.figure(figsize=(8, 6)); sns.heatmap(cm_perc, annot=True, fmt='.2f', cmap='Greens', xticklabels=['Fake (0)', 'Real (1)'], yticklabels=['Fake (0)', 'Real (1)'], annot_kws={\"size\": 12})\n",
    "        plt.title(f'Confusion Matrix (Row Percentages, Threshold = {threshold_05})'); plt.ylabel('True Label'); plt.xlabel('Predicted Label')\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, f'confusion_matrix_perc_{os.path.basename(model_weights_path)}.png')); plt.show()\n",
    "\n",
    "        # --- 2. Threshold Analysis ---\n",
    "        print(\"\\n--- Threshold Analysis ---\")\n",
    "        precision_scores, recall_scores, f1_scores_thr = [], [], []\n",
    "        best_f1, best_f1_threshold = 0.0, 0.5 # Default values\n",
    "\n",
    "        # Check if scores have variation for thresholding\n",
    "        if np.min(y_pred_scores) < np.max(y_pred_scores):\n",
    "            thresholds_metrics = np.linspace(np.min(y_pred_scores)+1e-6, np.max(y_pred_scores)-1e-6, 100)\n",
    "            for thr in thresholds_metrics:\n",
    "                y_pred_binary = (y_pred_scores >= thr).astype(int)\n",
    "                precision = precision_score(y_true_labels, y_pred_binary, zero_division=0)\n",
    "                recall = recall_score(y_true_labels, y_pred_binary, zero_division=0)\n",
    "                f1 = f1_score(y_true_labels, y_pred_binary, zero_division=0)\n",
    "                precision_scores.append(precision); recall_scores.append(recall); f1_scores_thr.append(f1)\n",
    "\n",
    "            if f1_scores_thr: # Ensure list is not empty\n",
    "                best_f1_idx = np.argmax(f1_scores_thr)\n",
    "                best_f1 = f1_scores_thr[best_f1_idx]\n",
    "                best_f1_threshold = thresholds_metrics[best_f1_idx]\n",
    "            else:\n",
    "                print(\"Warning: Could not calculate F1 scores across thresholds.\")\n",
    "\n",
    "            print(f\"Best F1 Score: {best_f1:.4f} at threshold {best_f1_threshold:.4f}\")\n",
    "            plt.figure(figsize=(10, 6)); plt.plot(thresholds_metrics, precision_scores, label='Precision', linestyle='--'); plt.plot(thresholds_metrics, recall_scores, label='Recall', linestyle=':')\n",
    "            plt.plot(thresholds_metrics, f1_scores_thr, label='F1 Score', linewidth=2); plt.scatter(best_f1_threshold, best_f1, color='red', zorder=5, label=f'Best F1 ({best_f1:.4f})')\n",
    "            plt.xlabel('Threshold'); plt.ylabel('Score'); plt.title('Precision, Recall, and F1 Score vs. Threshold')\n",
    "            plt.legend(); plt.grid(True); plt.ylim([0.0, 1.05])\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, f'prf1_vs_threshold_{os.path.basename(model_weights_path)}.png')); plt.show()\n",
    "        else:\n",
    "             print(\"Skipping threshold analysis plot: All prediction scores are identical.\")\n",
    "             best_f1 = f1_at_05 # Use F1 at 0.5 if no variation\n",
    "             best_f1_threshold = 0.5\n",
    "\n",
    "\n",
    "        # --- 3. Precision-Recall Curve ---\n",
    "        print(\"\\n--- Precision-Recall Curve ---\")\n",
    "        precision_pr, recall_pr, _ = precision_recall_curve(y_true_labels, y_pred_scores)\n",
    "        pr_auc_val = auc(recall_pr, precision_pr); avg_precision = average_precision_score(y_true_labels, y_pred_scores)\n",
    "        print(f\"Area Under PR Curve (PR AUC): {pr_auc_val:.4f}\"); print(f\"Average Precision (AP): {avg_precision:.4f}\")\n",
    "        plt.figure(figsize=(8, 6)); plt.plot(recall_pr, precision_pr, label=f'PR curve (AUC = {pr_auc_val:.4f}, AP = {avg_precision:.4f})')\n",
    "        plt.xlabel('Recall (Sensitivity)'); plt.ylabel('Precision'); plt.title('Precision-Recall Curve')\n",
    "        plt.legend(); plt.grid(True); plt.ylim([0.0, 1.05]); plt.xlim([0.0, 1.0])\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, f'precision_recall_curve_{os.path.basename(model_weights_path)}.png')); plt.show()\n",
    "\n",
    "        # --- 4. Score Calibration Plot ---\n",
    "        print(\"\\n--- Score Calibration ---\")\n",
    "        print(\"Plotting calibration curve (based on evaluation data)...\")\n",
    "        plt.figure(figsize=(8, 8)); disp = CalibrationDisplay.from_predictions(y_true_labels, y_pred_scores, n_bins=15, name=f'Model ({os.path.basename(model_weights_path)})', strategy='uniform')\n",
    "        plt.title('Reliability Diagram (Calibration Curve)'); plt.grid(True)\n",
    "        plt.savefig(os.path.join(FIGURES_DIR, f'calibration_curve_{os.path.basename(model_weights_path)}.png')); plt.show()\n",
    "\n",
    "        # --- 5. EER & t-DCF ---\n",
    "        print(\"\\n--- EER & t-DCF Analysis ---\")\n",
    "        fpr, tpr, thresholds_roc = roc_curve(y_true_labels, y_pred_scores, pos_label=1)\n",
    "        fnr = 1 - tpr\n",
    "        eer_threshold, eer = 0.5, 0.5 # Defaults\n",
    "        if len(fpr) > 0 and len(fnr) > 0: # Check if roc_curve returned valid results\n",
    "             eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
    "             # Ensure index is valid for thresholds array\n",
    "             if eer_index < len(thresholds_roc):\n",
    "                  eer_threshold = thresholds_roc[eer_index]\n",
    "             else: # Handle edge case where threshold might be missing (e.g., perfect separation)\n",
    "                  eer_threshold = thresholds_roc[-1] if len(thresholds_roc) > 0 else 0.5\n",
    "             eer = fpr[eer_index]\n",
    "        else:\n",
    "            print(\"Warning: Could not calculate EER reliably.\")\n",
    "        print(f\"EER: {eer:.4f} at threshold {eer_threshold:.4f}\")\n",
    "\n",
    "        def calculate_tdcf_min(y_true, y_scores, p_target, c_miss, c_fa):\n",
    "            # Check for trivial cases\n",
    "            if len(np.unique(y_true)) < 2: return np.inf, 0.5 # Need both classes\n",
    "            if len(np.unique(y_scores)) < 2: # Handle constant scores\n",
    "                 const_score = y_scores[0]\n",
    "                 thr = const_score # Only one threshold makes sense\n",
    "                 y_pred_binary = (y_scores >= thr).astype(int)\n",
    "                 tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary, labels=[0, 1]).ravel()\n",
    "                 num_real = tp + fn; num_fake = tn + fp\n",
    "                 p_miss = fn / num_real if num_real > 0 else 0\n",
    "                 p_fa = fp / num_fake if num_fake > 0 else 0\n",
    "                 cost = (c_miss * p_miss * p_target) + (c_fa * p_fa * (1 - p_target))\n",
    "                 c_def = min(c_miss * p_target, c_fa * (1 - p_target))\n",
    "                 return (cost / c_def if c_def > 0 else cost), thr\n",
    "\n",
    "            # Normal case\n",
    "            fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "            fnr = 1 - tpr; min_c_det = float(\"inf\"); min_c_det_threshold = thresholds[0] if len(thresholds) > 0 else 0.0\n",
    "            c_def = min(c_miss * p_target, c_fa * (1 - p_target))\n",
    "            if c_def <= 0: return np.inf, min_c_det_threshold\n",
    "            # Iterate thresholds, including potential endpoints if needed\n",
    "            for i in range(len(thresholds)):\n",
    "                c_det = c_miss * fnr[i] * p_target + c_fa * fpr[i] * (1 - p_target)\n",
    "                # Ensure cost is non-negative\n",
    "                c_det = max(0, c_det)\n",
    "                if c_det < min_c_det: min_c_det = c_det; min_c_det_threshold = thresholds[i]\n",
    "            # Handle potential threshold edge cases if min_c_det is still inf\n",
    "            if min_c_det == float(\"inf\"): min_c_det = 0.0\n",
    "\n",
    "            return min_c_det / c_def, min_c_det_threshold\n",
    "\n",
    "\n",
    "        min_tdcf, min_tdcf_thresh = calculate_tdcf_min(y_true_labels, y_pred_scores, p_target, c_miss, c_false_alarm)\n",
    "        print(f\"Min t-DCF: {min_tdcf:.4f} at threshold {min_tdcf_thresh:.4f}\")\n",
    "\n",
    "        # --- Final Summary ---\n",
    "        print(\"\\n--- Final Summary Metrics ---\")\n",
    "        final_roc_auc = auc(fpr, tpr) if len(fpr)>0 and len(tpr)>0 else np.nan\n",
    "        acc_at_eer_thr = np.mean(y_true_labels == (y_pred_scores >= eer_threshold).astype(int)) if y_pred_scores.size > 0 else np.nan\n",
    "        print(f\"Weights File:        {os.path.basename(model_weights_path)}\")\n",
    "        print(f\"Accuracy (@EER thr): {acc_at_eer_thr:.4f}\")\n",
    "        print(f\"AUC (recalculated):  {final_roc_auc:.4f}\")\n",
    "        print(f\"F1 Score (@0.5):     {f1_at_05:.4f}\")\n",
    "        print(f\"Best F1 Score:     {best_f1:.4f} (@ thr={best_f1_threshold:.4f})\")\n",
    "        print(f\"EER:                 {eer:.4f} (@ thr={eer_threshold:.4f})\")\n",
    "        print(f\"Min t-DCF:           {min_tdcf:.4f} (@ thr={min_tdcf_thresh:.4f})\")\n",
    "        print(\"---------------------------------\")\n",
    "\n",
    "    else: print(\"No predictions available.\")\n",
    "else: print(\"Classifier model not loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
