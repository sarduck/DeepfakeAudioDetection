{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 22:22:08.138790: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-03 22:22:08.149608: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743699128.163480  120890 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743699128.167670  120890 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743699128.178689  120890 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743699128.178713  120890 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743699128.178714  120890 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743699128.178715  120890 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-03 22:22:08.182311: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, GRU, Dense, Dropout, BatchNormalization, LayerNormalization, Reshape, Permute, Bidirectional, Add, Attention, Flatten, TimeDistributed, Conv2DTranspose, Conv2D, Layer, Concatenate, Multiply, AdditiveAttention # Added Multiply, AdditiveAttention for potential SelfAttention implementation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop # Added RMSprop as an option for WGAN\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve # Moved confusion_matrix here\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "# import noisereduce as nr # Consider if still needed/effective\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, sosfilt\n",
    "import logging\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for better Jupyter integration\n",
    "import random\n",
    "import seaborn as sns # Added for confusion matrix plotting\n",
    "from scipy.interpolate import interp1d # Added for EER/t-DCF\n",
    "\n",
    "# WGAN-GP specific\n",
    "from tensorflow import GradientTape\n",
    "\n",
    "# Create directory for saving figures\n",
    "FIGURES_DIR = 'training_figures_wgan_sa' # Changed dir name\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='audio_errors_wgan_sa.log', level=logging.ERROR,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and configured.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Force GPU usage\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"GPU is available and configured.\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(f\"Error configuring GPU: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices found. Falling back to CPU.\")\n",
    "\n",
    "# Set mixed precision policy if desired (can speed up training on compatible GPUs)\n",
    "#from tensorflow.keras import mixed_precision\n",
    "#policy = mixed_precision.Policy('mixed_float16')\n",
    "#mixed_precision.set_global_policy(policy)\n",
    "#print('Mixed precision enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Audio Loading and Preprocessing Function\n",
    "def load_and_preprocess_audio(file_path, sr=16000, duration=4):\n",
    "    try:\n",
    "        # Load audio, potentially reducing noise first if beneficial\n",
    "        audio, current_sr = librosa.load(file_path, sr=None, duration=duration) # Load native SR first\n",
    "\n",
    "        # Optional: Noise Reduction (experiment if needed)\n",
    "        # audio = nr.reduce_noise(y=audio, sr=current_sr)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if current_sr != sr:\n",
    "            audio = librosa.resample(audio, orig_sr=current_sr, target_sr=sr)\n",
    "\n",
    "        # Pad or truncate to fixed duration *before* augmentation/normalization\n",
    "        target_len = sr * duration\n",
    "        if len(audio) < target_len:\n",
    "            audio = np.pad(audio, (0, target_len - len(audio)), mode='constant')\n",
    "        else:\n",
    "            audio = audio[:target_len]\n",
    "\n",
    "        # Data Augmentation (applied *before* normalization)\n",
    "        if np.random.random() < 0.5: # 50% chance\n",
    "            augmentation_type = np.random.choice(['noise', 'pitch', 'speed'])\n",
    "            if augmentation_type == 'noise':\n",
    "                noise_amp = 0.005 * np.random.uniform(0.1, 1.0) * np.max(np.abs(audio)) # Scale noise relative to audio\n",
    "                noise = np.random.randn(len(audio)) * noise_amp\n",
    "                audio = audio + noise\n",
    "            elif augmentation_type == 'pitch':\n",
    "                pitch_shift_steps = np.random.uniform(-2.5, 2.5)\n",
    "                audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=pitch_shift_steps)\n",
    "            else: # speed (time stretch)\n",
    "                speed_rate = np.random.uniform(0.85, 1.15)\n",
    "                audio = librosa.effects.time_stretch(audio, rate=speed_rate)\n",
    "                 # Time stretching changes length, re-pad/truncate\n",
    "                if len(audio) < target_len:\n",
    "                    audio = np.pad(audio, (0, target_len - len(audio)), mode='constant')\n",
    "                else:\n",
    "                    audio = audio[:target_len]\n",
    "\n",
    "        # Normalize audio (peak normalization) - crucial for consistency\n",
    "        max_amp = np.max(np.abs(audio))\n",
    "        if max_amp > 1e-6: # Avoid division by zero\n",
    "             audio = audio / max_amp\n",
    "        # Optional: RMS normalization instead\n",
    "        # rms = np.sqrt(np.mean(audio**2))\n",
    "        # if rms > 1e-6:\n",
    "        #    audio = audio / rms * 0.5 # Scale to a target RMS\n",
    "\n",
    "        return audio\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading/preprocessing {file_path}: {e}\")\n",
    "        print(f\"Error loading/preprocessing {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Feature Extraction Function\n",
    "def extract_features(audio, sr=16000, n_mels=80, n_fft=2048, hop_length=512):\n",
    "    if audio is None:\n",
    "        return None\n",
    "    try:\n",
    "        # Extract mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio,\n",
    "            sr=sr,\n",
    "            n_mels=n_mels,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length\n",
    "        )\n",
    "        # Convert to log scale (dB)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        # Normalize features (per spectrogram) - Standard Scaling\n",
    "        mean = np.mean(log_mel_spec)\n",
    "        std = np.std(log_mel_spec)\n",
    "        if std > 1e-6: # Avoid division by zero\n",
    "            log_mel_spec = (log_mel_spec - mean) / std\n",
    "        else:\n",
    "            log_mel_spec = log_mel_spec - mean # Just center if std is near zero\n",
    "\n",
    "        # Ensure the shape is consistent (should be handled by fixed duration loading)\n",
    "        # Expected frames: int(np.ceil(target_len / hop_length)) -> int(ceil(16000*4/512)) = 125? Check calculation\n",
    "        # target_len = 16000 * 4 = 64000\n",
    "        # expected_frames = 64000 // hop_length + 1 if 64000 % hop_length != 0 else 64000 // hop_length\n",
    "        # expected_frames = 64000 / 512 = 125. Check librosa padding. It often adds a frame. Let's stick to TARGET_FRAMES=126 based on previous findings.\n",
    "\n",
    "        return log_mel_spec # Shape (n_mels, n_frames) e.g. (80, 126)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting features: {e}\")\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Class Distribution Analysis Function\n",
    "def analyze_class_distribution(data_path):\n",
    "    try:\n",
    "        real_dir = os.path.join(data_path, 'real')\n",
    "        fake_dir = os.path.join(data_path, 'fake')\n",
    "        real_count = len([f for f in os.listdir(real_dir) if f.endswith('.wav')]) if os.path.exists(real_dir) else 0\n",
    "        fake_count = len([f for f in os.listdir(fake_dir) if f.endswith('.wav')]) if os.path.exists(fake_dir) else 0\n",
    "        total = real_count + fake_count\n",
    "        if total == 0:\n",
    "            print(f\"\\nNo .wav files found in {data_path}\")\n",
    "            return {'real': 0, 'fake': 0}\n",
    "        print(f\"\\nClass Distribution for {data_path}:\")\n",
    "        print(f\"Real: {real_count} ({real_count/total*100:.2f}%)\")\n",
    "        print(f\"Fake: {fake_count} ({fake_count/total*100:.2f}%)\")\n",
    "        return {'real': real_count, 'fake': fake_count}\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nError: Data path not found - {data_path}\")\n",
    "        return {'real': 0, 'fake': 0}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error analyzing distribution for {data_path}: {e}\")\n",
    "        print(f\"Error analyzing distribution for {data_path}: {e}\")\n",
    "        return {'real': 0, 'fake': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Data Generators\n",
    "\n",
    "# Define the fixed number of frames for GAN/Classifier consistency\n",
    "TARGET_FRAMES = 126 # Recalculate based on sr=16000, duration=4, hop_length=512 if needed\n",
    "\n",
    "# Data generator for STANDALONE CLASSIFIER training (Yields X, y, sample_weights)\n",
    "def data_generator_classifier(data_path, batch_size=32, shuffle=True, target_frames=TARGET_FRAMES, sr=16000, duration=4, n_mels=80, n_fft=2048, hop_length=512):\n",
    "    real_files = []\n",
    "    fake_files = []\n",
    "    try:\n",
    "        real_dir = os.path.join(data_path, 'real')\n",
    "        fake_dir = os.path.join(data_path, 'fake')\n",
    "        if os.path.exists(real_dir):\n",
    "             real_files = [os.path.join(real_dir, f) for f in os.listdir(real_dir) if f.endswith('.wav')]\n",
    "        if os.path.exists(fake_dir):\n",
    "             fake_files = [os.path.join(fake_dir, f) for f in os.listdir(fake_dir) if f.endswith('.wav')]\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error finding directories in {data_path}: {e}\")\n",
    "        # Optionally raise the error or yield nothing\n",
    "        # raise e\n",
    "        return # Stop the generator\n",
    "\n",
    "    all_files = real_files + fake_files\n",
    "    labels = [1] * len(real_files) + [0] * len(fake_files) # Real=1, Fake=0\n",
    "\n",
    "    if not all_files:\n",
    "        print(f\"No WAV files found in {data_path}. Generator stopping.\")\n",
    "        return\n",
    "\n",
    "    total_samples = len(all_files)\n",
    "    # Calculate class weights (inverse frequency) - useful for imbalanced datasets\n",
    "    class_weights = {\n",
    "        1: total_samples / (2 * len(real_files)) if len(real_files) > 0 else 1.0, # Handle division by zero\n",
    "        0: total_samples / (2 * len(fake_files)) if len(fake_files) > 0 else 1.0,\n",
    "    }\n",
    "    print(f\"Using class weights: {class_weights} for path {data_path}\")\n",
    "\n",
    "\n",
    "    indices = np.arange(total_samples)\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            batch_files = [all_files[k] for k in batch_indices]\n",
    "            batch_labels = [labels[k] for k in batch_indices]\n",
    "\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            batch_sample_weights = []\n",
    "\n",
    "            for file_path, label in zip(batch_files, batch_labels):\n",
    "                audio = load_and_preprocess_audio(file_path, sr=sr, duration=duration)\n",
    "                if audio is None: continue # Skip failed loads\n",
    "\n",
    "                features = extract_features(audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length) # Shape (n_mels, n_frames)\n",
    "                if features is None: continue # Skip failed feature extraction\n",
    "\n",
    "                # Pad or truncate features to target_frames (redundant if load_and_preprocess handles length)\n",
    "                current_frames = features.shape[1]\n",
    "                if current_frames != target_frames:\n",
    "                     # This indicates an issue in load_and_preprocess or feature extraction settings\n",
    "                     # print(f\"Warning: Feature frames mismatch for {file_path}. Expected {target_frames}, got {current_frames}. Padding/Truncating.\") # Log this if it happens often\n",
    "                     if current_frames < target_frames:\n",
    "                        pad_width = target_frames - current_frames\n",
    "                        padded_features = np.pad(features, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                     else:\n",
    "                        padded_features = features[:, :target_frames]\n",
    "                else:\n",
    "                    padded_features = features\n",
    "\n",
    "                batch_x.append(padded_features)\n",
    "                batch_y.append(label)\n",
    "                batch_sample_weights.append(class_weights[label])\n",
    "\n",
    "            if batch_x: # Only yield if batch is not empty\n",
    "                # Add channel dimension for Conv2D models\n",
    "                batch_x_4d = np.expand_dims(np.array(batch_x), axis=-1).astype(np.float32)\n",
    "                batch_y_arr = np.array(batch_y).astype(np.float32)\n",
    "                batch_weights_arr = np.array(batch_sample_weights).astype(np.float32)\n",
    "                yield batch_x_4d, batch_y_arr, batch_weights_arr\n",
    "\n",
    "\n",
    "# Data generator for WGAN training (Yields real fake samples X only)\n",
    "def data_generator_gan(data_path, batch_size=32, shuffle=True, target_frames=TARGET_FRAMES, sr=16000, duration=4, n_mels=80, n_fft=2048, hop_length=512):\n",
    "    fake_files = []\n",
    "    try:\n",
    "        fake_dir = os.path.join(data_path, 'fake')\n",
    "        if os.path.exists(fake_dir):\n",
    "             fake_files = [os.path.join(fake_dir, f) for f in os.listdir(fake_dir) if f.endswith('.wav')]\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error finding fake directory in {data_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    if not fake_files:\n",
    "        print(f\"No fake WAV files found in {data_path}. GAN generator stopping.\")\n",
    "        return\n",
    "\n",
    "    total_samples = len(fake_files)\n",
    "    indices = np.arange(total_samples)\n",
    "\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            batch_files = [fake_files[k] for k in batch_indices]\n",
    "\n",
    "            batch_x = []\n",
    "\n",
    "            for file_path in batch_files:\n",
    "                audio = load_and_preprocess_audio(file_path, sr=sr, duration=duration)\n",
    "                if audio is None: continue\n",
    "\n",
    "                features = extract_features(audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "                if features is None: continue\n",
    "\n",
    "                current_frames = features.shape[1]\n",
    "                if current_frames != target_frames:\n",
    "                    # print(f\"Warning: Feature frames mismatch for {file_path} in GAN generator. Expected {target_frames}, got {current_frames}. Padding/Truncating.\")\n",
    "                    if current_frames < target_frames:\n",
    "                        pad_width = target_frames - current_frames\n",
    "                        padded_features = np.pad(features, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                    else:\n",
    "                        padded_features = features[:, :target_frames]\n",
    "                else:\n",
    "                     padded_features = features\n",
    "\n",
    "                batch_x.append(padded_features)\n",
    "\n",
    "            if batch_x:\n",
    "                # Add channel dimension\n",
    "                batch_x_4d = np.expand_dims(np.array(batch_x), axis=-1).astype(np.float32)\n",
    "                yield batch_x_4d # Yield only the features (4D array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Self-Attention Layer (SAGAN style)\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    \"\"\"\n",
    "    Self-attention layer based on SAGAN.\n",
    "    Input shape: (batch, height, width, channels)\n",
    "    Output shape: (batch, height, width, channels_out) where channels_out is typically channels\n",
    "    \"\"\"\n",
    "    def __init__(self, channels_out=None, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_channels = input_shape[-1]\n",
    "        if self.channels_out is None:\n",
    "            self.channels_out = self.input_channels\n",
    "\n",
    "        # Convolution layers for query, key, value\n",
    "        # Use 1x1 convolutions to reduce/transform channels\n",
    "        self.f = Conv2D(self.input_channels // 8, kernel_size=1, strides=1, padding='same', name='conv_f') # Query\n",
    "        self.g = Conv2D(self.input_channels // 8, kernel_size=1, strides=1, padding='same', name='conv_g') # Key\n",
    "        self.h = Conv2D(self.channels_out, kernel_size=1, strides=1, padding='same', name='conv_h')        # Value\n",
    "\n",
    "        # Final 1x1 convolution\n",
    "        self.out_conv = Conv2D(self.channels_out, kernel_size=1, strides=1, padding='same', name='conv_out')\n",
    "\n",
    "        # Learnable scale parameter\n",
    "        self.gamma = self.add_weight(name='gamma', shape=(1,), initializer='zeros', trainable=True)\n",
    "\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size, height, width, num_channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        location_num = height * width\n",
    "        downsampled_num = location_num\n",
    "\n",
    "        # Query (f), Key (g), Value (h) projections\n",
    "        f_proj = self.f(x) # Shape: (batch, h, w, c/8)\n",
    "        g_proj = self.g(x) # Shape: (batch, h, w, c/8)\n",
    "        h_proj = self.h(x) # Shape: (batch, h, w, c_out)\n",
    "\n",
    "        # Reshape for matrix multiplication\n",
    "        f_flat = tf.reshape(f_proj, shape=(batch_size, location_num, self.input_channels // 8)) # (batch, h*w, c/8)\n",
    "        g_flat = tf.reshape(g_proj, shape=(batch_size, location_num, self.input_channels // 8)) # (batch, h*w, c/8)\n",
    "        h_flat = tf.reshape(h_proj, shape=(batch_size, location_num, self.channels_out))       # (batch, h*w, c_out)\n",
    "\n",
    "        # Attention map calculation\n",
    "        # Transpose g for matmul: (batch, c/8, h*w)\n",
    "        g_flat_t = tf.transpose(g_flat, perm=[0, 2, 1])\n",
    "        # Attention score: (batch, h*w, c/8) x (batch, c/8, h*w) -> (batch, h*w, h*w)\n",
    "        attention_score = tf.matmul(f_flat, g_flat_t)\n",
    "        attention_prob = tf.nn.softmax(attention_score, axis=-1) # Apply softmax across locations\n",
    "\n",
    "        # Apply attention map to value projection\n",
    "        # (batch, h*w, h*w) x (batch, h*w, c_out) -> (batch, h*w, c_out)\n",
    "        attention_output = tf.matmul(attention_prob, h_flat)\n",
    "\n",
    "        # Reshape back to image format\n",
    "        attention_output_reshaped = tf.reshape(attention_output, shape=(batch_size, height, width, self.channels_out))\n",
    "\n",
    "        # Apply final 1x1 convolution and scale by gamma\n",
    "        o = self.out_conv(attention_output_reshaped)\n",
    "        y = self.gamma * o + x # Additive skip connection\n",
    "\n",
    "        return y\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (self.channels_out,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Generator Model (with Self-Attention)\n",
    "\n",
    "def create_generator(latent_dim, output_shape): # output_shape (n_mels, n_frames) e.g., (80, 126)\n",
    "    \"\"\"Creates the generator model with Self-Attention.\"\"\"\n",
    "    n_mels, n_frames = output_shape\n",
    "    init_h, init_w = n_mels // 8, n_frames // 8 # Calculate initial dimensions based on 3 upsamples (2*2*2=8)\n",
    "    init_c = 128 # Initial channels\n",
    "\n",
    "    if init_h * 8 != n_mels or init_w * 8 != n_frames:\n",
    "         print(f\"Warning: Output shape {output_shape} might not be perfectly reached with 3 strides of 2. Adjusting initial size or layers.\")\n",
    "         # Adjust init_w slightly if needed, e.g. target 128 -> 16, target 126 -> 16 (trim later)\n",
    "         init_w = (n_frames + 7) // 8 # Ceiling division equivalent for width\n",
    "\n",
    "    nodes = init_h * init_w * init_c\n",
    "\n",
    "    model = Sequential(name='generator')\n",
    "    model.add(Input(shape=(latent_dim,)))\n",
    "\n",
    "    # Dense layer and reshape\n",
    "    model.add(Dense(nodes))\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2)) # Use negative_slope\n",
    "    model.add(Reshape((init_h, init_w, init_c))) # e.g., (10, 16, 128)\n",
    "\n",
    "    # Upsample 1: (10, 16, 128) -> (20, 32, 64)\n",
    "    model.add(Conv2DTranspose(init_c // 2, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False)) # Typically use_bias=False with Norm\n",
    "    model.add(LayerNormalization()) # Using LayerNorm instead of BatchNorm\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "    # Upsample 2: (20, 32, 64) -> (40, 64, 32)\n",
    "    model.add(Conv2DTranspose(init_c // 4, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "    # Add Self-Attention Layer Here (applied to 40x64 feature map)\n",
    "    # Note: Attention can be computationally expensive. Apply strategically.\n",
    "    model.add(SelfAttention(channels_out=init_c // 4)) # Keep channels the same\n",
    "    # model.add(LayerNormalization()) # Optional normalization after attention\n",
    "    # model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2)) # Optional activation after attention\n",
    "\n",
    "    # Upsample 3: (40, 64, 32) -> (80, 128, 1)\n",
    "    model.add(Conv2DTranspose(1, kernel_size=(4, 4), strides=(2, 2), padding='same', activation='tanh'))\n",
    "\n",
    "    # Final adjustment layer if needed (e.g., width 128 -> 126)\n",
    "    current_width = init_w * 8\n",
    "    if current_width != n_frames:\n",
    "        print(f\"Generator adding final Conv2D to adjust width from {current_width} to {n_frames}\")\n",
    "        # Calculate kernel size needed for 'valid' padding: K = W_in - W_out + 1\n",
    "        kernel_w = current_width - n_frames + 1\n",
    "        if kernel_w > 0:\n",
    "             model.add(Conv2D(1, kernel_size=(1, kernel_w), padding='valid', activation='tanh'))\n",
    "        else:\n",
    "            # This case shouldn't happen with the init_w calculation but handle just in case\n",
    "             print(f\"Warning: Could not adjust width with Conv2D. Current {current_width}, Target {n_frames}\")\n",
    "             # May need padding='same' and cropping layer if kernel_w is not positive\n",
    "\n",
    "    # Ensure final output shape is correct\n",
    "    # model.add(Reshape((n_mels, n_frames, 1))) # Add reshape just to be certain, though last Conv should handle it\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Critic (Discriminator) Model (with Self-Attention)\n",
    "\n",
    "def create_critic(input_shape): # input_shape (n_mels, n_frames) e.g., (80, 126)\n",
    "    \"\"\"Creates the Critic model for WGAN-GP with Self-Attention.\"\"\"\n",
    "    n_mels, n_frames = input_shape\n",
    "    model_input_shape = (n_mels, n_frames, 1) # Expects (80, 126, 1)\n",
    "\n",
    "    model = Sequential(name='critic')\n",
    "    model.add(Input(shape=model_input_shape))\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same')) # Increased filters\n",
    "    # No Batch/Layer Norm typically in WGAN-GP critic input layer\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Conv2D(128, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    # WGAN-GP often avoids norm layers in critic, but LayerNorm can sometimes help stability\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Add Self-Attention Layer Here (applied to 20x32 feature map if input is 80x128->40x64->20x32)\n",
    "    # Check actual size based on input_shape and strides\n",
    "    # Example placement after layer 2\n",
    "    model.add(SelfAttention(channels_out=128)) # Keep channels the same\n",
    "    # model.add(LayerNormalization()) # Optional\n",
    "    # model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2)) # Optional\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(Conv2D(256, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False)) # Increased filters\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Layer 4 (Optional additional conv layer)\n",
    "    # model.add(Conv2D(512, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    # model.add(LayerNormalization())\n",
    "    # model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # Flatten and Output Score (Linear Activation for WGAN Critic)\n",
    "    model.add(Flatten())\n",
    "    # model.add(Dense(512)) # Optional dense layer before output\n",
    "    # model.add(LayerNormalization())\n",
    "    # model.add(tf.keras.layers.LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Dense(1)) # Output a single scalar score (no activation)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Define the GAN Model (Not used for WGAN-GP training loop)\n",
    "# def create_gan(generator, discriminator, latent_dim):\n",
    "#     \"\"\"Creates the combined GAN model.\"\"\"\n",
    "#     # Make discriminator non-trainable\n",
    "#     discriminator.trainable = False\n",
    "#\n",
    "#     # Stack generator and discriminator\n",
    "#     gan_input = Input(shape=(latent_dim,))\n",
    "#     gan_output = discriminator(generator(gan_input))\n",
    "#     gan = Model(gan_input, gan_output)\n",
    "#\n",
    "#     return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743699130.652512  120890 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1904 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Output shape (80, 126) might not be perfectly reached with 3 strides of 2. Adjusting initial size or layers.\n",
      "Generator adding final Conv2D to adjust width from 128 to 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743699131.351937  120890 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of generated image (Generator Output): (1, 80, 126, 1)\n",
      "Shape of critic output: (1, 1)\n",
      "Expected shape for Critic Input: (80, 126, 1)\n",
      "\n",
      "--- Generator Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"generator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"generator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20480</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,068,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20480</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttention</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,377</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20480\u001b[0m)          │     \u001b[38;5;34m2,068,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20480\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m131,072\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m32,768\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention (\u001b[38;5;33mSelfAttention\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m2,377\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │           \u001b[38;5;34m513\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │             \u001b[38;5;34m4\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,235,406</span> (8.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,235,406\u001b[0m (8.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,235,406</span> (8.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,235,406\u001b[0m (8.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Critic Summary ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"critic\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"critic\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,072</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,153</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttention</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40960</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,961</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m1,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m131,072\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m37,153\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttention\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m524,288\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40960\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m40,961\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">735,330</span> (2.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m735,330\u001b[0m (2.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">735,330</span> (2.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m735,330\u001b[0m (2.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 11: Data Path and Parameters\n",
    "\n",
    "# Data Paths\n",
    "train_data_path = 'datasetNEW/train'\n",
    "dev_data_path = 'datasetNEW/dev'\n",
    "eval_data_path = 'datasetNEW/eval'\n",
    "\n",
    "# Define the fixed number of frames\n",
    "TARGET_FRAMES = 126\n",
    "N_MELS = 80\n",
    "mel_spectrogram_shape = (N_MELS, TARGET_FRAMES)\n",
    "\n",
    "# WGAN-GP specific parameters\n",
    "latent_dim = 100\n",
    "n_critic = 5         # Train critic 5 times per generator update\n",
    "gp_weight = 10.0     # Gradient penalty weight\n",
    "gan_epochs = 75      # WGAN often needs more epochs, adjust as needed\n",
    "gan_batch_size = 8  # Adjust based on GPU memory (WGAN-GP can be memory intensive)\n",
    "\n",
    "# Optimizers (Typical WGAN settings: lower LR, betas=(0.5, 0.9))\n",
    "critic_optimizer = Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.9)\n",
    "generator_optimizer = Adam(learning_rate=0.0001, beta_1=0.5, beta_2=0.9)\n",
    "# critic_optimizer = RMSprop(learning_rate=0.00005) # Alternative optimizer\n",
    "# generator_optimizer = RMSprop(learning_rate=0.00005)\n",
    "\n",
    "# Create instances\n",
    "generator = create_generator(latent_dim, mel_spectrogram_shape)\n",
    "critic = create_critic(mel_spectrogram_shape)\n",
    "\n",
    "# Diagnostic Code: Verify Output Shape\n",
    "test_noise = tf.random.normal((1, latent_dim))\n",
    "generated_image = generator(test_noise, training=False) # Use tf.random and call model directly\n",
    "critic_output = critic(generated_image, training=False)\n",
    "print(\"Shape of generated image (Generator Output):\", generated_image.shape)\n",
    "print(\"Shape of critic output:\", critic_output.shape)\n",
    "\n",
    "critic_input_shape_expected = (mel_spectrogram_shape[0], mel_spectrogram_shape[1], 1)\n",
    "print(\"Expected shape for Critic Input:\", critic_input_shape_expected)\n",
    "assert generated_image.shape[1:] == critic_input_shape_expected, \"Generator output shape mismatch!\"\n",
    "assert len(critic_output.shape) == 2 and critic_output.shape[1] == 1, \"Critic output shape mismatch!\"\n",
    "\n",
    "\n",
    "# Report the models\n",
    "print(\"\\n--- Generator Summary ---\")\n",
    "generator.summary()\n",
    "print(\"\\n--- Critic Summary ---\")\n",
    "critic.summary()\n",
    "\n",
    "# Parameters for standalone classifier training (can be different)\n",
    "classifier_batch_size = 8 # Keep smaller for classifier fine-tuning?\n",
    "classifier_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated 2850 GAN steps per epoch.\n",
      "\n",
      "Begin WGAN-GP training!\n",
      "\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea55be0f1074f928d0b4902a75abcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1743699138.234367  120890 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inwhile/body/_1/while/critic_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: WGAN-GP Training Loop (Corrected for Mixed Precision Compatibility)\n",
    "\n",
    "# --- Loss Functions ---\n",
    "def critic_loss(real_output, fake_output):\n",
    "    \"\"\"Wasserstein loss for the critic.\"\"\"\n",
    "    # Ensure outputs are float32 for stable loss calculation\n",
    "    real_output = tf.cast(real_output, tf.float32)\n",
    "    fake_output = tf.cast(fake_output, tf.float32)\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    \"\"\"Wasserstein loss for the generator.\"\"\"\n",
    "    # Ensure output is float32\n",
    "    fake_output = tf.cast(fake_output, tf.float32)\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "# --- Gradient Penalty Function (Corrected) ---\n",
    "def gradient_penalty(batch_size, real_images, fake_images):\n",
    "    \"\"\" Calculates the gradient penalty loss for WGAN GP, handling mixed precision. \"\"\"\n",
    "    # Ensure both images have the same dtype before interpolation\n",
    "    # Typically, fake_images might be float16 if mixed precision is on.\n",
    "    # Cast real_images to match fake_images' dtype.\n",
    "    if real_images.dtype != fake_images.dtype:\n",
    "        # print(f\"Casting real_images from {real_images.dtype} to {fake_images.dtype} in gradient_penalty\") # Optional debug print\n",
    "        real_images = tf.cast(real_images, fake_images.dtype)\n",
    "\n",
    "    # Generate interpolation alpha with the correct dtype\n",
    "    # Ensure alpha shape matches rank of images for broadcasting\n",
    "    alpha_shape = [tf.shape(real_images)[0]] + [1] * (len(real_images.shape) - 1)\n",
    "    alpha = tf.random.uniform(shape=alpha_shape, minval=0., maxval=1., dtype=real_images.dtype)\n",
    "\n",
    "    # Interpolate images\n",
    "    interpolated = real_images + alpha * (fake_images - real_images)\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        # 1. Get the critic output for this interpolated image.\n",
    "        pred = critic(interpolated, training=True)\n",
    "        # If using mixed precision, the prediction itself might be float16.\n",
    "        # It's often safer to compute the final GP loss in float32.\n",
    "        pred = tf.cast(pred, tf.float32)\n",
    "\n",
    "    # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "    # Ensure pred is connected to interpolated in the graph\n",
    "    grads = gp_tape.gradient(pred, [interpolated]) # grads is a list\n",
    "    if grads is None or grads[0] is None:\n",
    "        # Handle case where gradients are None (e.g., if interpolated is not connected to pred)\n",
    "        logging.warning(\"Gradients are None in gradient_penalty. Returning 0 penalty.\")\n",
    "        print(\"Warning: Gradients are None in gradient_penalty. Returning 0 penalty.\")\n",
    "        return tf.constant(0.0, dtype=tf.float32)\n",
    "    grads = grads[0] # Get the tensor from the list\n",
    "\n",
    "    # Cast gradients to float32 before norm calculation for stability\n",
    "    grads = tf.cast(grads, tf.float32)\n",
    "\n",
    "    # 3. Calculate the norm of the gradients. Sum over H, W, C axes.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=tf.range(1, tf.rank(grads))))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "\n",
    "    return gp * gp_weight # gp_weight should be float\n",
    "\n",
    "\n",
    "# --- Data Generator and Steps Calculation ---\n",
    "# Create GAN data generator instance\n",
    "train_gen_gan = data_generator_gan(train_data_path, batch_size=gan_batch_size)\n",
    "\n",
    "# Calculate steps per epoch for GAN training\n",
    "fake_files_count = 0\n",
    "try:\n",
    "    fake_dir = os.path.join(train_data_path, 'fake')\n",
    "    if os.path.exists(fake_dir):\n",
    "        fake_files_count = len([f for f in os.listdir(fake_dir) if f.endswith('.wav')])\n",
    "except FileNotFoundError:\n",
    "    fake_files_count = 0\n",
    "\n",
    "if gan_batch_size <= 0:\n",
    "    raise ValueError(\"GAN Batch size must be positive.\")\n",
    "if fake_files_count == 0:\n",
    "    print(\"Warning: No fake training files found for GAN. Setting GAN steps to 0.\")\n",
    "    gan_steps_per_epoch = 0\n",
    "else:\n",
    "    # Use ceiling division\n",
    "    gan_steps_per_epoch = int(np.ceil(fake_files_count / float(gan_batch_size)))\n",
    "    print(f\"Calculated {gan_steps_per_epoch} GAN steps per epoch.\")\n",
    "\n",
    "\n",
    "# --- Training Step Function (Decorated with tf.function) ---\n",
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    current_batch_size = tf.shape(real_images)[0]\n",
    "    noise = tf.random.normal([current_batch_size, latent_dim])\n",
    "\n",
    "    # Train Critic (n_critic times)\n",
    "    # Use a loop *inside* tf.function (AutoGraph will handle it)\n",
    "    # We need to track the loss from the *last* critic iteration for reporting\n",
    "    total_crit_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "    for _ in tf.range(n_critic): # Use tf.range inside tf.function\n",
    "        with tf.GradientTape() as crit_tape:\n",
    "            # Generate fake images (potentially float16)\n",
    "            fake_images = generator(noise, training=True)\n",
    "            # Get scores for real (float32 input) and fake images\n",
    "            real_output = critic(real_images, training=True) # Critic output might be float16 or float32\n",
    "            fake_output = critic(fake_images, training=True) # Critic output might be float16 or float32\n",
    "\n",
    "            # Calculate base critic loss (handles casting inside)\n",
    "            crit_loss = critic_loss(real_output, fake_output)\n",
    "            # Calculate gradient penalty (handles casting inside)\n",
    "            gp = gradient_penalty(current_batch_size, real_images, fake_images)\n",
    "            # Total critic loss (ensure consistent dtype, should be float32 due to loss functions)\n",
    "            total_crit_loss = crit_loss + gp # Store loss from this iteration\n",
    "\n",
    "        # Calculate gradients for critic and apply them\n",
    "        crit_gradients = crit_tape.gradient(total_crit_loss, critic.trainable_variables)\n",
    "\n",
    "        # Filter out None gradients before applying\n",
    "        valid_grads_and_vars = [\n",
    "            (g, v) for g, v in zip(crit_gradients, critic.trainable_variables) if g is not None\n",
    "        ]\n",
    "        if len(valid_grads_and_vars) < len(critic.trainable_variables):\n",
    "             tf.print(\"Warning: Some critic gradients are None.\") # Use tf.print inside tf.function\n",
    "        if len(valid_grads_and_vars) > 0: # Check if there are any valid gradients\n",
    "            critic_optimizer.apply_gradients(valid_grads_and_vars)\n",
    "\n",
    "    # Train Generator (once per train_step call, after n_critic updates)\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # Generate fake images again (within generator's tape)\n",
    "        fake_images_gen = generator(noise, training=True) # Use a different variable name if needed\n",
    "        # Get critic's score on these fake images\n",
    "        fake_output_gen = critic(fake_images_gen, training=True)\n",
    "        # Calculate generator loss (handles casting inside)\n",
    "        gen_loss = generator_loss(fake_output_gen)\n",
    "\n",
    "    # Calculate gradients for generator and apply them\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "\n",
    "    # Filter out None gradients before applying\n",
    "    valid_grads_and_vars_gen = [\n",
    "        (g, v) for g, v in zip(gen_gradients, generator.trainable_variables) if g is not None\n",
    "    ]\n",
    "    if len(valid_grads_and_vars_gen) < len(generator.trainable_variables):\n",
    "         tf.print(\"Warning: Some generator gradients are None.\")\n",
    "    if len(valid_grads_and_vars_gen) > 0:\n",
    "        generator_optimizer.apply_gradients(valid_grads_and_vars_gen)\n",
    "\n",
    "    # Return the critic loss from the LAST iteration and the generator loss\n",
    "    return total_crit_loss, gen_loss\n",
    "\n",
    "\n",
    "# --- WGAN-GP Training Loop ---\n",
    "print(\"\\nBegin WGAN-GP training!\")\n",
    "\n",
    "# Check if generator exists before proceeding\n",
    "if train_gen_gan is None or gan_steps_per_epoch == 0:\n",
    "    print(\"Skipping WGAN-GP training: Generator not initialized or no steps per epoch.\")\n",
    "else:\n",
    "    c_loss_history = []\n",
    "    g_loss_history = []\n",
    "\n",
    "    for epoch in range(gan_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{gan_epochs}\")\n",
    "        epoch_pbar = tqdm(range(gan_steps_per_epoch), desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "        epoch_c_loss = 0.0\n",
    "        epoch_g_loss = 0.0\n",
    "        batches_processed = 0\n",
    "\n",
    "        for batch_idx in epoch_pbar:\n",
    "            try:\n",
    "                # Get REAL fake samples (samples from dataset's fake folder)\n",
    "                # Ensure the generator yields tf.float32 or handle conversion\n",
    "                real_spoof_samples_np = next(train_gen_gan)\n",
    "                real_spoof_samples = tf.convert_to_tensor(real_spoof_samples_np, dtype=tf.float32) # Explicitly ensure float32 input to train_step\n",
    "\n",
    "                if tf.shape(real_spoof_samples)[0] == 0:\n",
    "                    print(f\"Warning: Skipped empty batch at step {batch_idx}\")\n",
    "                    continue\n",
    "\n",
    "                # Run the training step\n",
    "                c_loss, g_loss = train_step(real_spoof_samples)\n",
    "\n",
    "                # Accumulate losses (use .numpy() only for display/logging outside tf.function)\n",
    "                epoch_c_loss += c_loss.numpy()\n",
    "                epoch_g_loss += g_loss.numpy()\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Update tqdm postfix with current batch losses\n",
    "                epoch_pbar.set_postfix({\"C Loss\": f\"{c_loss.numpy():.4f}\", \"G Loss\": f\"{g_loss.numpy():.4f}\"})\n",
    "\n",
    "            except StopIteration:\n",
    "                print(f\"\\nGAN Generator exhausted prematurely at batch {batch_idx}. Moving to next epoch.\")\n",
    "                # Re-initialize generator for the next epoch cycle\n",
    "                train_gen_gan = data_generator_gan(train_data_path, batch_size=gan_batch_size)\n",
    "                break # Exit the inner loop for this epoch\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during WGAN training epoch {epoch+1}, batch {batch_idx}: {e}\", exc_info=True)\n",
    "                print(f\"\\nError during WGAN training epoch {epoch+1}, batch {batch_idx}: {e}\")\n",
    "                # Decide whether to continue or break\n",
    "                continue # Skip this problematic batch\n",
    "\n",
    "        # --- End-of-epoch actions ---\n",
    "        if batches_processed > 0:\n",
    "             avg_c_loss = epoch_c_loss / batches_processed\n",
    "             avg_g_loss = epoch_g_loss / batches_processed\n",
    "             c_loss_history.append(avg_c_loss)\n",
    "             g_loss_history.append(avg_g_loss)\n",
    "             print(f\"Epoch {epoch+1} finished. Avg C Loss: {avg_c_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}\")\n",
    "        else:\n",
    "             print(f\"Epoch {epoch+1} finished. No batches processed.\")\n",
    "             # Append NaN or handle appropriately if needed for plotting\n",
    "             c_loss_history.append(np.nan)\n",
    "             g_loss_history.append(np.nan)\n",
    "\n",
    "\n",
    "        # Save models periodically\n",
    "        if (epoch + 1) % 5 == 0 or (epoch + 1) == gan_epochs: # Save every 5 epochs and at the end\n",
    "             try:\n",
    "                 generator.save(f'generator_wgan_sa_epoch_{epoch+1}.keras')\n",
    "                 critic.save(f'critic_wgan_sa_epoch_{epoch+1}.keras')\n",
    "                 print(f\"Saved generator and critic models for epoch {epoch+1}\")\n",
    "             except Exception as e:\n",
    "                 print(f\"Error saving models at epoch {epoch+1}: {e}\")\n",
    "\n",
    "        # Optional: Plot losses at the end of each epoch\n",
    "        # plt.figure()\n",
    "        # plt.plot(c_loss_history, label='Critic Loss')\n",
    "        # plt.plot(g_loss_history, label='Generator Loss')\n",
    "        # plt.title(f'Losses up to Epoch {epoch+1}')\n",
    "        # plt.xlabel('Epoch')\n",
    "        # plt.ylabel('Loss')\n",
    "        # plt.legend()\n",
    "        # plt.savefig(os.path.join(FIGURES_DIR, f'gan_loss_epoch_{epoch+1}.png'))\n",
    "        # plt.close()\n",
    "\n",
    "\n",
    "    print(\"\\nWGAN-GP training finished.\")\n",
    "\n",
    "    # Final loss plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(c_loss_history) + 1), c_loss_history, label='Avg Critic Loss per Epoch')\n",
    "    plt.plot(range(1, len(g_loss_history) + 1), g_loss_history, label='Avg Generator Loss per Epoch')\n",
    "    plt.title('WGAN-GP Training Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'gan_loss_final.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Train Standalone Classifier (using the trained Critic)\n",
    "\n",
    "print(\"\\nSetting up Standalone Classifier training...\")\n",
    "\n",
    "# --- Create Data Generators for Classifier ---\n",
    "train_gen_clf = data_generator_classifier(train_data_path, batch_size=classifier_batch_size)\n",
    "dev_gen_clf = data_generator_classifier(dev_data_path, batch_size=classifier_batch_size, shuffle=False) # No shuffle for dev\n",
    "\n",
    "# --- Calculate steps for Classifier Training ---\n",
    "def count_total_files(path):\n",
    "    # Uses the function defined in Cell 5 context if run sequentially, otherwise redefine:\n",
    "    real_dir = os.path.join(path, 'real')\n",
    "    fake_dir = os.path.join(path, 'fake')\n",
    "    real_count = len([f for f in os.listdir(real_dir) if f.endswith('.wav')]) if os.path.exists(real_dir) else 0\n",
    "    fake_count = len([f for f in os.listdir(fake_dir) if f.endswith('.wav')]) if os.path.exists(fake_dir) else 0\n",
    "    return real_count + fake_count\n",
    "\n",
    "train_samples_count = count_total_files(train_data_path)\n",
    "dev_samples_count = count_total_files(dev_data_path)\n",
    "\n",
    "if classifier_batch_size <= 0:\n",
    "    raise ValueError(\"Classifier batch size must be positive.\")\n",
    "\n",
    "clf_steps_per_epoch = int(np.ceil(train_samples_count / float(classifier_batch_size))) if train_samples_count > 0 else 0\n",
    "clf_validation_steps = int(np.ceil(dev_samples_count / float(classifier_batch_size))) if dev_samples_count > 0 else 0\n",
    "\n",
    "print(f\"Classifier Train Steps/Epoch: {clf_steps_per_epoch}, Validation Steps: {clf_validation_steps}\")\n",
    "\n",
    "\n",
    "# --- Build the Spoof Detector Model from the Critic ---\n",
    "# Load the latest critic weights if desired, otherwise it uses weights from end of training\n",
    "# latest_critic_ckpt = tf.train.latest_checkpoint(os.path.dirname('critic_wgan_sa_epoch_X.keras')) # Or specify path\n",
    "# if latest_critic_ckpt:\n",
    "#    print(f\"Loading critic weights from {latest_critic_ckpt}\")\n",
    "#    critic.load_weights(latest_critic_ckpt)\n",
    "\n",
    "# Create the final classifier by adding a sigmoid layer to the critic's base\n",
    "critic.trainable = True # Unfreeze critic layers for fine-tuning\n",
    "\n",
    "spoof_detector = Sequential(name='spoof_detector')\n",
    "for layer in critic.layers[:-1]: # Add all layers except the last Dense(1) output layer\n",
    "    spoof_detector.add(layer)\n",
    "\n",
    "# Add the final classification layer\n",
    "spoof_detector.add(Dense(1, activation='sigmoid', name='classifier_output'))\n",
    "\n",
    "print(\"\\n--- Spoof Detector (Classifier) Summary ---\")\n",
    "spoof_detector.summary()\n",
    "\n",
    "# --- Callbacks for Classifier Training ---\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1 # Lower min_lr\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True, verbose=1\n",
    ")\n",
    "\n",
    "# Checkpoint directory for the classifier\n",
    "checkpoint_dir_clf = './training_checkpoints_spoof_detector_wgan_sa'\n",
    "os.makedirs(checkpoint_dir_clf, exist_ok=True)\n",
    "checkpoint_prefix_clf = os.path.join(checkpoint_dir_clf, \"ckpt_clf_{epoch:02d}.weights.h5\")\n",
    "\n",
    "checkpoint_callback_clf = ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix_clf,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_auc', # Monitor validation AUC (or accuracy/loss)\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plotting callback (ensure class is defined - should be from original Cell 13)\n",
    "plot_training_callback = PlotTrainingHistory(model_name='spoof_detector_wgan_sa')\n",
    "\n",
    "\n",
    "# --- Compile and Train the Classifier ---\n",
    "classifier_optimizer = Adam(learning_rate=1e-5) # Use a smaller LR for fine-tuning\n",
    "spoof_detector.compile(\n",
    "    optimizer=classifier_optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Standalone Classifier training...\")\n",
    "\n",
    "if clf_steps_per_epoch > 0 and clf_validation_steps > 0:\n",
    "    history = spoof_detector.fit(\n",
    "        train_gen_clf,\n",
    "        steps_per_epoch=clf_steps_per_epoch,\n",
    "        epochs=classifier_epochs,\n",
    "        validation_data=dev_gen_clf,\n",
    "        validation_steps=clf_validation_steps,\n",
    "        callbacks=[reduce_lr, early_stopping, checkpoint_callback_clf, plot_training_callback],\n",
    "        # Use sample_weight argument if generator yields weights\n",
    "        # sample_weight= # Not directly supported by fit, need custom loop or weighted metrics if using sample_weights yield\n",
    "        # Note: Keras `fit` uses the third element of the generator yield as sample_weight if present\n",
    "        # Make sure data_generator_classifier yields (batch_x, batch_y, batch_weights)\n",
    "    )\n",
    "\n",
    "    # Save the final trained classifier model\n",
    "    spoof_detector.save('spoof_detector_final_wgan_sa.keras')\n",
    "    print(\"\\nClassifier training complete. Model saved as spoof_detector_final_wgan_sa.keras\")\n",
    "\n",
    "    # --- Optional: Load best weights based on checkpoint ---\n",
    "    best_checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir_clf)\n",
    "    if best_checkpoint_path:\n",
    "        print(f\"Loading best classifier weights from: {best_checkpoint_path}\")\n",
    "        spoof_detector.load_weights(best_checkpoint_path)\n",
    "        spoof_detector.save('spoof_detector_best_val_auc_wgan_sa.keras')\n",
    "        print(\"Saved classifier with best validation AUC weights.\")\n",
    "    else:\n",
    "        print(\"Could not find best checkpoint weights to load. Final weights saved.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping classifier training because steps_per_epoch or validation_steps is zero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Evaluation\n",
    "\n",
    "print(\"\\nEvaluating the final Spoof Detector...\")\n",
    "\n",
    "# --- Load the best or final classifier model ---\n",
    "try:\n",
    "    # Try loading the model saved with best weights first\n",
    "    detector_model_path = 'spoof_detector_best_val_auc_wgan_sa.keras'\n",
    "    if not os.path.exists(detector_model_path):\n",
    "         # Fallback to the final model if best wasn't saved or found\n",
    "         detector_model_path = 'spoof_detector_final_wgan_sa.keras'\n",
    "\n",
    "    print(f\"Loading model for evaluation: {detector_model_path}\")\n",
    "    # When loading custom objects like SelfAttention, provide them in custom_objects\n",
    "    custom_objects = {'SelfAttention': SelfAttention}\n",
    "    spoof_detector_eval = tf.keras.models.load_model(\n",
    "        detector_model_path,\n",
    "        custom_objects=custom_objects\n",
    "    )\n",
    "    # Re-compile is often needed after loading, especially if optimizer state isn't saved\n",
    "    spoof_detector_eval.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error loading spoof detector model: {e}\")\n",
    "    print(\"Evaluation cannot proceed.\")\n",
    "    spoof_detector_eval = None\n",
    "\n",
    "\n",
    "if spoof_detector_eval:\n",
    "    # --- Create Evaluation Generator ---\n",
    "    eval_gen_clf = data_generator_classifier(eval_data_path, batch_size=classifier_batch_size, shuffle=False) # No shuffle\n",
    "\n",
    "    # --- Calculate Evaluation Steps ---\n",
    "    eval_samples_count = count_total_files(eval_data_path)\n",
    "    if classifier_batch_size <= 0:\n",
    "        raise ValueError(\"Classifier batch size must be positive.\")\n",
    "    eval_steps = int(np.ceil(eval_samples_count / float(classifier_batch_size))) if eval_samples_count > 0 else 0\n",
    "    print(f\"Using {eval_steps} steps for evaluation.\")\n",
    "\n",
    "    # --- Evaluate ---\n",
    "    if eval_steps > 0:\n",
    "        results = spoof_detector_eval.evaluate(\n",
    "            eval_gen_clf,\n",
    "            steps=eval_steps,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Evaluation Results ---\")\n",
    "        if len(results) >= 3:\n",
    "             print(f\"Loss: {results[0]:.4f}\")\n",
    "             print(f\"Accuracy: {results[1]:.4f}\")\n",
    "             print(f\"AUC: {results[2]:.4f}\")\n",
    "        else:\n",
    "            print(f\"Results: {results}\") # Print raw results if format is unexpected\n",
    "        print(\"--------------------------\")\n",
    "    else:\n",
    "        print(\"Skipping evaluation because eval_steps is zero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Reporting (Confusion Matrix, EER, t-DCF)\n",
    "\n",
    "print(\"\\nGenerating Final Reports (F1, CM, EER, t-DCF)...\")\n",
    "\n",
    "# Ensure the evaluation model is loaded from Cell 14\n",
    "if 'spoof_detector_eval' not in locals() or spoof_detector_eval is None:\n",
    "    print(\"Spoof detector model not loaded. Cannot generate reports.\")\n",
    "else:\n",
    "    # --- Parameters for t-DCF ---\n",
    "    p_target = 0.05  # Prior probability of target (real=1) - Adjust based on ASVspoof challenge or use case\n",
    "    c_miss = 1       # Cost of miss (classifying real as fake - FN)\n",
    "    c_false_alarm = 1 # Cost of false alarm (classifying fake as real - FP)\n",
    "\n",
    "    # --- Regenerate Predictions ---\n",
    "    # Reset the eval generator\n",
    "    eval_gen_report = data_generator_classifier(eval_data_path, batch_size=classifier_batch_size, shuffle=False)\n",
    "\n",
    "    # Recalculate eval_steps if needed (should match Cell 14)\n",
    "    eval_samples_count_report = count_total_files(eval_data_path)\n",
    "    eval_steps_report = int(np.ceil(eval_samples_count_report / float(classifier_batch_size))) if eval_samples_count_report > 0 else 0\n",
    "\n",
    "    y_pred_scores = []\n",
    "    y_true_labels = []\n",
    "\n",
    "    if eval_steps_report > 0:\n",
    "        print(f\"Generating predictions using {eval_steps_report} steps...\")\n",
    "        for _ in tqdm(range(eval_steps_report), desc=\"Predicting\"):\n",
    "            try:\n",
    "                # Generator yields (batch_x, batch_y, batch_weights)\n",
    "                batch_x, batch_y, _ = next(eval_gen_report)\n",
    "                if batch_x.size == 0: continue # Skip empty batches if they occur\n",
    "\n",
    "                # Use predict, not predict_on_batch for potentially better performance over many batches\n",
    "                batch_pred = spoof_detector_eval.predict(batch_x, verbose=0)\n",
    "                y_pred_scores.extend(batch_pred.flatten())\n",
    "                y_true_labels.extend(batch_y)\n",
    "            except StopIteration:\n",
    "                print(\"Evaluation generator stopped.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error during prediction generation: {e}\")\n",
    "                continue\n",
    "\n",
    "        y_pred_scores = np.array(y_pred_scores).astype(np.float32)\n",
    "        y_true_labels = np.array(y_true_labels).astype(np.int32)\n",
    "\n",
    "        # Ensure lengths match (might be off if last batch was incomplete and generator didn't handle it perfectly)\n",
    "        min_len = min(len(y_pred_scores), len(y_true_labels))\n",
    "        if min_len == 0:\n",
    "            print(\"No predictions or labels collected. Cannot generate reports.\")\n",
    "        else:\n",
    "            y_pred_scores = y_pred_scores[:min_len]\n",
    "            y_true_labels = y_true_labels[:min_len]\n",
    "\n",
    "            # --- Calculations ---\n",
    "            # Binary predictions for F1 and CM\n",
    "            y_pred_binary = (y_pred_scores > 0.5).astype(int)\n",
    "\n",
    "            # F1 Score\n",
    "            f1 = f1_score(y_true_labels, y_pred_binary)\n",
    "            print(f\"\\nF1 Score (threshold 0.5): {f1:.4f}\")\n",
    "\n",
    "            # Confusion Matrix\n",
    "            cm = confusion_matrix(y_true_labels, y_pred_binary)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake (0)', 'Real (1)'], yticklabels=['Fake (0)', 'Real (1)'])\n",
    "            plt.title('Confusion Matrix (Counts)')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, 'confusion_matrix_counts_wgan_sa.png'))\n",
    "            plt.show()\n",
    "\n",
    "            # Confusion Matrix (Percentages)\n",
    "            cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "            cm_perc = cm / cm_sum.astype(float) * 100\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm_perc, annot=True, fmt='.2f', cmap='Greens', xticklabels=['Fake (0)', 'Real (1)'], yticklabels=['Fake (0)', 'Real (1)'])\n",
    "            plt.title('Confusion Matrix (Row Percentages)')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, 'confusion_matrix_perc_wgan_sa.png'))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            # EER Calculation\n",
    "            fpr, tpr, thresholds_roc = roc_curve(y_true_labels, y_pred_scores, pos_label=1)\n",
    "            fnr = 1 - tpr\n",
    "            eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
    "            eer_threshold = thresholds_roc[eer_index]\n",
    "            eer = fpr[eer_index] # Or use (fpr[eer_index] + fnr[eer_index]) / 2\n",
    "            print(f\"EER: {eer:.4f} at threshold {eer_threshold:.4f}\")\n",
    "\n",
    "            # Plot ROC Curve\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(fpr, tpr, label=f'ROC curve (AUC = {results[2]:.4f})') # Use AUC from evaluate\n",
    "            plt.plot(fpr, fnr, label='FN Rate')\n",
    "            plt.plot([0, 1], [0, 1], 'k--') # Random guess line\n",
    "            plt.scatter(fpr[eer_index], tpr[eer_index], color='red', zorder=5, label=f'EER = {eer:.4f}')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate (FPR)')\n",
    "            plt.ylabel('True Positive Rate (TPR)')\n",
    "            plt.title('Receiver Operating Characteristic (ROC)')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(FIGURES_DIR, 'roc_curve_wgan_sa.png'))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            # t-DCF Calculation\n",
    "            # Define function (can be moved to utils if used often)\n",
    "            def calculate_t_dcf(y_true, y_scores, p_target, c_miss, c_fa, thresholds):\n",
    "                \"\"\"Calculates normalized t-DCF for a range of thresholds.\"\"\"\n",
    "                num_thresholds = len(thresholds)\n",
    "                dcf_values = np.zeros(num_thresholds)\n",
    "\n",
    "                num_real = np.sum(y_true == 1)\n",
    "                num_fake = np.sum(y_true == 0)\n",
    "\n",
    "                if num_real == 0 or num_fake == 0:\n",
    "                     print(\"Warning: Cannot calculate t-DCF with zero samples in a class.\")\n",
    "                     return np.inf # Return infinity or handle as error\n",
    "\n",
    "                for i, thr in enumerate(thresholds):\n",
    "                    y_pred_binary = (y_scores >= thr).astype(int)\n",
    "                    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary, labels=[0, 1]).ravel()\n",
    "\n",
    "                    p_miss = fn / num_real if num_real > 0 else 0 # Miss Rate (FN / Total Real)\n",
    "                    p_fa = fp / num_fake if num_fake > 0 else 0   # False Alarm Rate (FP / Total Fake)\n",
    "\n",
    "                    cost = (c_miss * p_miss * p_target) + (c_fa * p_fa * (1 - p_target))\n",
    "\n",
    "                    # Normalize the cost\n",
    "                    dcf_norm = min(c_miss * p_target, c_fa * (1 - p_target))\n",
    "                    dcf_values[i] = cost / dcf_norm if dcf_norm > 0 else cost # Avoid division by zero\n",
    "\n",
    "                # Find the minimum t-DCF\n",
    "                min_dcf_index = np.argmin(dcf_values)\n",
    "                min_dcf = dcf_values[min_dcf_index]\n",
    "                min_dcf_threshold = thresholds[min_dcf_index]\n",
    "\n",
    "                return min_dcf, min_dcf_threshold\n",
    "\n",
    "            # Calculate min t-DCF over a range of thresholds (more robust than just EER threshold)\n",
    "            # Use ROC thresholds, but filter out +/- inf if present\n",
    "            valid_thresholds = thresholds_roc[np.isfinite(thresholds_roc)]\n",
    "            min_tdcf, min_tdcf_thresh = calculate_t_dcf(y_true_labels, y_pred_scores, p_target, c_miss, c_false_alarm, valid_thresholds)\n",
    "\n",
    "            print(f\"Minimum t-DCF: {min_tdcf:.4f} at threshold {min_tdcf_thresh:.4f}\")\n",
    "\n",
    "    else:\n",
    "         print(\"Skipping report generation because eval_steps_report is zero.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
