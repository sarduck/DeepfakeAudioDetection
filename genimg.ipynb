{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 00:11:12.249432: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-08 00:11:12.261471: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744051272.274885  302968 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744051272.278728  302968 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744051272.289575  302968 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744051272.289623  302968 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744051272.289624  302968 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744051272.289625  302968 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-08 00:11:12.293615: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports for Spectrogram Generation\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Imports needed for the custom layer and generator architecture\n",
    "from tensorflow.keras.layers import (Input, Dense, Reshape, Conv2DTranspose,\n",
    "                                     LayerNormalization, LeakyReLU, Conv2D, Layer)\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Custom Self-Attention Layer Definition\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    \"\"\"\n",
    "    Self-attention layer based on SAGAN.\n",
    "    Input shape: (batch, height, width, channels)\n",
    "    Output shape: (batch, height, width, channels_out) where channels_out is typically channels\n",
    "    \"\"\"\n",
    "    def __init__(self, channels_out=None, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_channels = input_shape[-1]\n",
    "        if self.channels_out is None:\n",
    "            self.channels_out = self.input_channels\n",
    "\n",
    "        # Convolution layers for query, key, value\n",
    "        self.f = Conv2D(self.input_channels // 8, kernel_size=1, strides=1, padding='same', name='conv_f') # Query\n",
    "        self.g = Conv2D(self.input_channels // 8, kernel_size=1, strides=1, padding='same', name='conv_g') # Key\n",
    "        self.h = Conv2D(self.channels_out, kernel_size=1, strides=1, padding='same', name='conv_h')        # Value\n",
    "        self.out_conv = Conv2D(self.channels_out, kernel_size=1, strides=1, padding='same', name='conv_out')\n",
    "        self.gamma = self.add_weight(name='gamma', shape=(1,), initializer='zeros', trainable=True)\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size, height, width, num_channels = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        location_num = height * width\n",
    "        f_proj = self.f(x)\n",
    "        g_proj = self.g(x)\n",
    "        h_proj = self.h(x)\n",
    "        f_flat = tf.reshape(f_proj, shape=(batch_size, location_num, self.input_channels // 8))\n",
    "        g_flat = tf.reshape(g_proj, shape=(batch_size, location_num, self.input_channels // 8))\n",
    "        h_flat = tf.reshape(h_proj, shape=(batch_size, location_num, self.channels_out))\n",
    "        g_flat_t = tf.transpose(g_flat, perm=[0, 2, 1])\n",
    "        attention_score = tf.matmul(f_flat, g_flat_t)\n",
    "        attention_prob = tf.nn.softmax(attention_score, axis=-1)\n",
    "        attention_output = tf.matmul(attention_prob, h_flat)\n",
    "        attention_output_reshaped = tf.reshape(attention_output, shape=(batch_size, height, width, self.channels_out))\n",
    "        o = self.out_conv(attention_output_reshaped)\n",
    "        y = self.gamma * o + x # Additive skip connection\n",
    "        return y\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (self.channels_out,)\n",
    "\n",
    "    # --- IMPORTANT: Add get_config if you were saving/loading full model ---\n",
    "    # Although not strictly needed for load_weights, it's best practice\n",
    "    def get_config(self):\n",
    "        config = super(SelfAttention, self).get_config()\n",
    "        config.update({\"channels_out\": self.channels_out})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Generator Model Definition\n",
    "\n",
    "def create_generator(latent_dim, output_shape): # output_shape (n_mels, n_frames) e.g., (80, 126)\n",
    "    \"\"\"Creates the generator model with Self-Attention.\"\"\"\n",
    "    n_mels, n_frames = output_shape\n",
    "    init_h, init_w = n_mels // 8, n_frames // 8\n",
    "    init_c = 128\n",
    "\n",
    "    if init_h * 8 != n_mels or init_w * 8 != n_frames:\n",
    "         print(f\"Note: Output shape {output_shape} might require final adjustment layer.\")\n",
    "         init_w = (n_frames + 7) // 8\n",
    "\n",
    "    nodes = init_h * init_w * init_c\n",
    "\n",
    "    model = Sequential(name='generator')\n",
    "    model.add(Input(shape=(latent_dim,)))\n",
    "    model.add(Dense(nodes))\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "    model.add(Reshape((init_h, init_w, init_c)))\n",
    "\n",
    "    # Upsample 1: -> (20, 32, 64) approx\n",
    "    model.add(Conv2DTranspose(init_c // 2, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "    # Upsample 2: -> (40, 64, 32) approx\n",
    "    model.add(Conv2DTranspose(init_c // 4, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "    # Add Self-Attention Layer (ensure placement matches trained model)\n",
    "    model.add(SelfAttention(channels_out=init_c // 4))\n",
    "\n",
    "    # Upsample 3: -> (80, 128, 1) approx\n",
    "    model.add(Conv2DTranspose(1, kernel_size=(4, 4), strides=(2, 2), padding='same', activation='tanh'))\n",
    "\n",
    "    # Final adjustment layer if needed (e.g., width 128 -> 126)\n",
    "    current_width = init_w * 8\n",
    "    if current_width != n_frames:\n",
    "        print(f\"Generator adding final Conv2D to adjust width from {current_width} to {n_frames}\")\n",
    "        kernel_w = current_width - n_frames + 1\n",
    "        if kernel_w > 0:\n",
    "             model.add(Conv2D(1, kernel_size=(1, kernel_w), padding='valid', activation='tanh'))\n",
    "        else:\n",
    "             print(f\"Warning: Could not adjust width with Conv2D.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base output directory: generator_comparison\n",
      "Epochs to check: [75, 80, 85, 100, 125]\n",
      "Images per epoch: 8\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Configuration for Generation (Multi-Epoch)\n",
    "\n",
    "# --- Essential Parameters (Match the training setup) ---\n",
    "latent_dim = 100\n",
    "N_MELS = 80\n",
    "TARGET_FRAMES = 126\n",
    "mel_spectrogram_shape = (N_MELS, TARGET_FRAMES)\n",
    "\n",
    "# --- Generation Parameters ---\n",
    "num_images_per_epoch = 8  # How many examples to generate PER epoch for comparison\n",
    "epochs_to_check = [ 75, 80, 85, 100, 125] # <--- LIST of epochs weights you have saved and want to compare\n",
    "output_dir_base = \"generator_comparison\" # Base directory to save comparison images\n",
    "\n",
    "# --- Ensure necessary modules are imported ---\n",
    "import os\n",
    "os.makedirs(output_dir_base, exist_ok=True)\n",
    "print(f\"Base output directory: {output_dir_base}\")\n",
    "print(f\"Epochs to check: {epochs_to_check}\")\n",
    "print(f\"Images per epoch: {num_images_per_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Generators and Generating Comparison Spectrograms ---\n",
      "\n",
      "--- Processing Epoch 75 ---\n",
      "Recreating generator structure...\n",
      "Note: Output shape (80, 126) might require final adjustment layer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744051274.555790  302968 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2143 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator adding final Conv2D to adjust width from 128 to 126\n",
      "Building generator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744051275.152090  302968 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure built.\n",
      "Loading weights from: generator_wgan_sa_epoch_75.weights.h5\n",
      "Weights loaded successfully.\n",
      "\n",
      "--- Processing Epoch 80 ---\n",
      "Recreating generator structure...\n",
      "Note: Output shape (80, 126) might require final adjustment layer.\n",
      "Generator adding final Conv2D to adjust width from 128 to 126\n",
      "Building generator...\n",
      "Structure built.\n",
      "Loading weights from: generator_wgan_sa_epoch_80.weights.h5\n",
      "Weights loaded successfully.\n",
      "\n",
      "--- Processing Epoch 85 ---\n",
      "Recreating generator structure...\n",
      "Note: Output shape (80, 126) might require final adjustment layer.\n",
      "Generator adding final Conv2D to adjust width from 128 to 126\n",
      "Building generator...\n",
      "Structure built.\n",
      "Loading weights from: generator_wgan_sa_epoch_85.weights.h5\n",
      "Weights loaded successfully.\n",
      "\n",
      "--- Processing Epoch 100 ---\n",
      "Recreating generator structure...\n",
      "Note: Output shape (80, 126) might require final adjustment layer.\n",
      "Generator adding final Conv2D to adjust width from 128 to 126\n",
      "Building generator...\n",
      "Structure built.\n",
      "Loading weights from: generator_wgan_sa_epoch_100.weights.h5\n",
      "Weights loaded successfully.\n",
      "\n",
      "--- Processing Epoch 125 ---\n",
      "Recreating generator structure...\n",
      "Note: Output shape (80, 126) might require final adjustment layer.\n",
      "Generator adding final Conv2D to adjust width from 128 to 126\n",
      "Building generator...\n",
      "Structure built.\n",
      "Loading weights from: generator_wgan_sa_epoch_125.weights.h5\n",
      "Weights loaded successfully.\n",
      "\n",
      "--- Generating Images for Loaded Epochs: [75, 80, 85, 100, 125] ---\n",
      "\n",
      "Generating 8 images for Epoch 75...\n",
      "\n",
      "Generating 8 images for Epoch 80...\n",
      "\n",
      "Generating 8 images for Epoch 85...\n",
      "\n",
      "Generating 8 images for Epoch 100...\n",
      "\n",
      "Generating 8 images for Epoch 125...\n",
      "\n",
      "Finished generating comparison images in 'generator_comparison'.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load Generators (Multiple Epochs), Generate, and Save Images\n",
    "\n",
    "print(\"\\n--- Loading Generators and Generating Comparison Spectrograms ---\")\n",
    "\n",
    "# --- Ensure prerequisite definitions are available ---\n",
    "if 'SelfAttention' not in locals(): raise NameError(\"SelfAttention class definition not found.\")\n",
    "if 'create_generator' not in locals(): raise NameError(\"create_generator function definition not found.\")\n",
    "\n",
    "loaded_generators = {} # Dictionary to hold loaded generators for each epoch\n",
    "\n",
    "# --- Loop through epochs to load weights ---\n",
    "for epoch in epochs_to_check:\n",
    "    generator_weights_path = f'generator_wgan_sa_epoch_{epoch}.weights.h5'\n",
    "    print(f\"\\n--- Processing Epoch {epoch} ---\")\n",
    "\n",
    "    if not os.path.exists(generator_weights_path):\n",
    "        print(f\"Weights file not found: {generator_weights_path}. Skipping this epoch.\")\n",
    "        continue\n",
    "\n",
    "    print(\"Recreating generator structure...\")\n",
    "    try:\n",
    "        gen = create_generator(latent_dim, mel_spectrogram_shape)\n",
    "        # Build generator via dummy pass\n",
    "        print(\"Building generator...\")\n",
    "        dummy_input = tf.zeros((1, latent_dim), dtype=tf.float32)\n",
    "        _ = gen(dummy_input, training=False)\n",
    "        print(\"Structure built.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error recreating/building generator for epoch {epoch}: {e}\")\n",
    "        continue # Skip to next epoch\n",
    "\n",
    "    print(f\"Loading weights from: {generator_weights_path}\")\n",
    "    try:\n",
    "        gen.load_weights(generator_weights_path)\n",
    "        print(\"Weights loaded successfully.\")\n",
    "        loaded_generators[epoch] = gen # Store the loaded generator\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading generator weights for epoch {epoch}: {e}.\")\n",
    "        # Don't add to dict if loading failed\n",
    "\n",
    "\n",
    "# --- Generate and Save Images ---\n",
    "print(f\"\\n--- Generating Images for Loaded Epochs: {list(loaded_generators.keys())} ---\")\n",
    "\n",
    "for epoch, generator in loaded_generators.items():\n",
    "    print(f\"\\nGenerating {num_images_per_epoch} images for Epoch {epoch}...\")\n",
    "    epoch_output_dir = os.path.join(output_dir_base, f\"epoch_{epoch}\")\n",
    "    os.makedirs(epoch_output_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Generate noise vectors (can do in batch for efficiency here if memory allows for num_images_per_epoch)\n",
    "        noise_vectors = tf.random.normal([num_images_per_epoch, latent_dim])\n",
    "        generated_spectrograms = generator(noise_vectors, training=False)\n",
    "        generated_spectrograms_np = generated_spectrograms.numpy()\n",
    "\n",
    "        # Plot and save each generated spectrogram\n",
    "        for i in range(num_images_per_epoch):\n",
    "            spec_to_plot = generated_spectrograms_np[i]\n",
    "            if spec_to_plot.shape[-1] == 1:\n",
    "                spec_to_plot = np.squeeze(spec_to_plot, axis=-1)\n",
    "            elif len(spec_to_plot.shape) > 2:\n",
    "                 spec_to_plot = spec_to_plot[:, :, 0] # Plot first channel\n",
    "\n",
    "            if len(spec_to_plot.shape) == 2:\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                plt.imshow(spec_to_plot, cmap='viridis', origin='lower', aspect='auto')\n",
    "                plt.colorbar(label='Log Amplitude (Normalized)')\n",
    "                plt.title(f'Generated Spectrogram (Epoch {epoch} - Sample {i+1})')\n",
    "                plt.xlabel('Time Frames'); plt.ylabel('Mel Bins')\n",
    "                plt.tight_layout()\n",
    "                save_path = os.path.join(epoch_output_dir, f'generated_spec_epoch{epoch}_sample{i+1}.png')\n",
    "                plt.savefig(save_path); plt.close()\n",
    "            else:\n",
    "                 print(f\"Skipping plot for epoch {epoch} sample {i+1} due to unexpected dimensions.\")\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"Error generating images for epoch {epoch}: {e}\")\n",
    "\n",
    "print(f\"\\nFinished generating comparison images in '{output_dir_base}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
