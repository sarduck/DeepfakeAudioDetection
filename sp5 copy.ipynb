{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 23:45:32.577586: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-31 23:45:32.633727: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743444932.679191  113978 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743444932.687672  113978 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743444932.744282  113978 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743444932.744305  113978 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743444932.744307  113978 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743444932.744308  113978 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-31 23:45:32.748275: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, GRU, Dense, Dropout, BatchNormalization, LayerNormalization, Reshape, Permute, Bidirectional, Add, Attention, Flatten, TimeDistributed, Conv2DTranspose, Conv2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.layers import Layer, Concatenate\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, sosfilt\n",
    "import logging  # Import the logging module\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for better Jupyter integration\n",
    "\n",
    "# Add these lines:\n",
    "import random\n",
    "\n",
    "# Add this to create a directory for saving figures\n",
    "FIGURES_DIR = 'training_figures'\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# Configure logging to a file (optional, but recommended)\n",
    "logging.basicConfig(filename='audio_errors.log', level=logging.ERROR,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\") # Check the TF version to verify install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and configured.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Force GPU usage\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"GPU is available and configured.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error configuring GPU: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices found.  Falling back to CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Audio Loading and Preprocessing Function\n",
    "def load_and_preprocess_audio(file_path, sr=16000, duration=4):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
    "\n",
    "        # Data Augmentation (increased probability and variety)\n",
    "        if np.random.random() < 0.5:  # 50% chance of applying augmentation\n",
    "            augmentation_type = np.random.choice(['noise', 'pitch', 'speed'])\n",
    "            if augmentation_type == 'noise':\n",
    "                noise = np.random.randn(len(audio)) * 0.005\n",
    "                audio = audio + noise\n",
    "            elif augmentation_type == 'pitch':\n",
    "                audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=np.random.uniform(-2, 2))\n",
    "            else:  # speed\n",
    "                audio = librosa.effects.time_stretch(audio, rate=np.random.uniform(0.8, 1.2))\n",
    "\n",
    "        # Normalize audio\n",
    "        audio = audio - np.mean(audio)\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "\n",
    "        # Pad if necessary\n",
    "        if len(audio) < sr * duration:\n",
    "            audio = np.pad(audio, (0, sr * duration - len(audio)))\n",
    "        return audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Feature Extraction Function\n",
    "def extract_features(audio, sr=16000, n_mels=80, n_fft=2048, hop_length=512):\n",
    "    if audio is None:\n",
    "        return None\n",
    "\n",
    "    # Extract mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio,\n",
    "        sr=sr,\n",
    "        n_mels=n_mels,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    # Normalize features\n",
    "    log_mel_spec = (log_mel_spec - np.mean(log_mel_spec)) / np.std(log_mel_spec)\n",
    "    return log_mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Class Distribution Analysis Function\n",
    "def analyze_class_distribution(data_path):\n",
    "    real_count = len([f for f in os.listdir(os.path.join(data_path, 'real')) if f.endswith('.wav')])\n",
    "    fake_count = len([f for f in os.listdir(os.path.join(data_path, 'fake')) if f.endswith('.wav')])\n",
    "    total = real_count + fake_count\n",
    "    print(f\"\\nClass Distribution for {data_path}:\")\n",
    "    print(f\"Real: {real_count} ({real_count/total*100:.2f}%)\")\n",
    "    print(f\"Fake: {fake_count} ({fake_count/total*100:.2f}%)\")\n",
    "    return {'real': real_count, 'fake': fake_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fixed number of frames for GAN consistency\n",
    "TARGET_FRAMES = 126 # Calculated based on sr=16000, duration=4, hop_length=512\n",
    "\n",
    "# Data generator with sample weights and FIXED padding\n",
    "def data_generator(data_path, batch_size=128, shuffle=True, target_frames=TARGET_FRAMES):\n",
    "    real_files = [os.path.join(data_path, 'real', f) for f in os.listdir(os.path.join(data_path, 'real')) if f.endswith('.wav')]\n",
    "    fake_files = [os.path.join(data_path, 'fake', f) for f in os.listdir(os.path.join(data_path, 'fake')) if f.endswith('.wav')]\n",
    "\n",
    "    all_files = real_files + fake_files\n",
    "    labels = [1] * len(real_files) + [0] * len(fake_files)\n",
    "\n",
    "    total_samples = len(all_files)\n",
    "    class_weights = {\n",
    "        1: total_samples / (2 * len(real_files)),\n",
    "        0: total_samples / (2 * len(fake_files))\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            temp = list(zip(all_files, labels))\n",
    "            np.random.shuffle(temp)\n",
    "            all_files, labels = zip(*temp)\n",
    "\n",
    "        for i in range(0, len(all_files), batch_size):\n",
    "            batch_files = all_files[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            batch_weights = []\n",
    "            # max_length = 0 # No longer needed for fixed padding\n",
    "\n",
    "            for file_path, label in zip(batch_files, batch_labels):\n",
    "                audio = load_and_preprocess_audio(file_path)\n",
    "                features = extract_features(audio) # Shape (80, n_frames)\n",
    "\n",
    "                if features is not None:\n",
    "                    # Pad or truncate features to target_frames\n",
    "                    current_frames = features.shape[1]\n",
    "                    if current_frames < target_frames:\n",
    "                        pad_width = target_frames - current_frames\n",
    "                        padded_features = np.pad(features, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                    else:\n",
    "                        padded_features = features[:, :target_frames] # Truncate if longer\n",
    "\n",
    "                    batch_x.append(padded_features) # Append the (80, target_frames) array\n",
    "                    batch_y.append(label)\n",
    "                    weight = class_weights[label]\n",
    "                    batch_weights.append(weight)\n",
    "                    # max_length = max(max_length, features.shape[1]) # No longer needed\n",
    "\n",
    "            # No need for secondary padding loop as all are now (80, target_frames)\n",
    "            if batch_x:\n",
    "                # Add channel dimension for Conv2D discriminator\n",
    "                batch_x_4d = np.expand_dims(np.array(batch_x), axis=-1)\n",
    "                yield batch_x_4d, np.array(batch_y), np.array(batch_weights)\n",
    "\n",
    "\n",
    "def data_generator_GAN(data_path, batch_size=128, shuffle=True, target_frames=TARGET_FRAMES):\n",
    "    \"\"\"Generates batches of audio features for SPOOF samples only, with FIXED padding.\"\"\"\n",
    "    fake_files = [os.path.join(data_path, 'fake', f) for f in os.listdir(os.path.join(data_path, 'fake')) if f.endswith('.wav')]\n",
    "\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(fake_files)\n",
    "\n",
    "        for i in range(0, len(fake_files), batch_size):\n",
    "            batch_files = fake_files[i:i+batch_size]\n",
    "            batch_x = []\n",
    "            # max_length = 0 # No longer needed\n",
    "\n",
    "            for file_path in batch_files:\n",
    "                audio = load_and_preprocess_audio(file_path)\n",
    "\n",
    "                if audio is None:\n",
    "                    print(f\"Skipping {file_path} due to loading failure.\")\n",
    "                    continue\n",
    "\n",
    "                features = extract_features(audio) # Shape (80, n_frames)\n",
    "\n",
    "                if features is None:\n",
    "                    print(f\"Skipping {file_path} due to feature extraction failure.\")\n",
    "                    continue\n",
    "\n",
    "                # Pad or truncate features to target_frames\n",
    "                current_frames = features.shape[1]\n",
    "                if current_frames < target_frames:\n",
    "                    pad_width = target_frames - current_frames\n",
    "                    padded_features = np.pad(features, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                else:\n",
    "                    padded_features = features[:, :target_frames] # Truncate if longer\n",
    "\n",
    "                batch_x.append(padded_features)\n",
    "                # max_length = max(max_length, features.shape[1]) # No longer needed\n",
    "\n",
    "            # No need for secondary padding loop\n",
    "            if batch_x:\n",
    "                # Add channel dimension before yielding\n",
    "                padded_batch_x_4d = np.expand_dims(np.array(batch_x), axis=-1)\n",
    "                yield padded_batch_x_4d # Yield 4D array (batch, 80, target_frames, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Modified MFM layer\n",
    "class MFM(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MFM, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return tf.reshape(tf.math.maximum(inputs[:,:,:shape[-1]//2], inputs[:,:,shape[-1]//2:]), (shape[0], shape[1], shape[-1]//2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(latent_dim, output_shape): # output_shape should be (n_mels, n_frames) e.g., (80, 126)\n",
    "    \"\"\"Creates the generator model designed to output the correct spectrogram shape.\"\"\"\n",
    "    n_mels, n_frames = output_shape\n",
    "    target_shape_with_channel = (n_mels, n_frames, 1)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Start dense layer - aim for dimensions that allow upsampling to target\n",
    "    # Let's target initial dimensions like (5, 8) before upsampling\n",
    "    # Strides of 2, 3 times: 2*2*2 = 8. Height: 80/8 = 10. Width: 126/8 approx 16.\n",
    "    # Let's try initial shape (10, 16) with 128 filters? -> 10*16*128 = 20480 nodes\n",
    "    nodes = 10 * 16 * 128\n",
    "    model.add(Dense(nodes, input_dim=latent_dim))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((10, 16, 128))) # H=10, W=16, C=128\n",
    "\n",
    "    # Upsample 1: (10, 16, 128) -> (20, 32, 64)\n",
    "    model.add(Conv2DTranspose(64, kernel_size=(4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Upsample 2: (20, 32, 64) -> (40, 64, 32)\n",
    "    model.add(Conv2DTranspose(32, kernel_size=(4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Upsample 3: (40, 64, 32) -> (80, 128, 1) - Width is too large (128 vs 126)\n",
    "    model.add(Conv2DTranspose(1, kernel_size=(4, 4), strides=(2, 2), padding='same', activation='tanh'))\n",
    "\n",
    "    # Add a Conv2D layer to adjust the width dimension precisely\n",
    "    # Input: (80, 128, 1) -> Output: (80, 126, 1)\n",
    "    # Kernel (1, 3) should work if padding='valid' or adjust padding\n",
    "    model.add(Conv2D(1, kernel_size=(1, 3), padding='valid', activation='tanh')) # 'valid' padding might reduce width by 2\n",
    "\n",
    "    # Final Reshape to remove the channel dimension if needed by discriminator,\n",
    "    # BUT our discriminator now expects the channel dim. Check final shape.\n",
    "    # The output of the last Conv2D with 'valid' padding and kernel (1,3) on (80, 128, 1)\n",
    "    # will be (80, 128-3+1, 1) = (80, 126, 1). This is the target!\n",
    "\n",
    "    # model.add(Reshape(output_shape)) # No need to reshape further if discriminator takes 4D\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Define the Discriminator Model\n",
    "def create_discriminator(input_shape): # input_shape should be (n_mels, n_frames) e.g., (80, 126)\n",
    "    \"\"\"Creates the discriminator model based on Conv2D accepting (H, W, C).\"\"\"\n",
    "    model_input_shape = (input_shape[0], input_shape[1], 1) # Expects (80, 126, 1)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Convolutional layers - specify the 4D input shape here\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), strides=(2, 2), padding='same', input_shape=model_input_shape))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Add more Conv layers if needed\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128)) # Reduced size\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Define the GAN Model\n",
    "def create_gan(generator, discriminator, latent_dim):\n",
    "    \"\"\"Creates the combined GAN model.\"\"\"\n",
    "    # Make discriminator non-trainable\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    # Stack generator and discriminator\n",
    "    gan_input = Input(shape=(latent_dim,))\n",
    "    gan_output = discriminator(generator(gan_input))\n",
    "    gan = Model(gan_input, gan_output)\n",
    "\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthakm/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1743444935.845863  113978 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2143 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "/home/sarthakm/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "/home/sarthakm/VIT/Capstone/Proj/.venvn/lib64/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743444936.509563  114552 service.cc:152] XLA service 0x7f8308003860 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1743444936.509578  114552 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Ti Laptop GPU, Compute Capability 8.6\n",
      "2025-03-31 23:45:36.519495: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1743444936.541652  114552 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 835ms/step\n",
      "Shape of generated image (Generator Output): (1, 80, 126, 1)\n",
      "Expected shape for Discriminator Input: (80, 126, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743444937.243731  114552 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20480</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,068,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20480</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,136</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,800</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20480\u001b[0m)          │     \u001b[38;5;34m2,068,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20480\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m131,136\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m32,800\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │           \u001b[38;5;34m513\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m1\u001b[0m)     │             \u001b[38;5;34m4\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,233,125</span> (8.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,233,125\u001b[0m (8.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,233,125</span> (8.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,233,125\u001b[0m (8.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20480</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,621,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20480\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m2,621,568\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_6 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,714,369</span> (10.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,714,369\u001b[0m (10.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,714,369</span> (10.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,714,369\u001b[0m (10.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 11: Data Path and Parameters (REVISED BATCH SIZE)\n",
    "# Data Paths\n",
    "train_data_path = 'datasetNEW/train'\n",
    "dev_data_path = 'datasetNEW/dev'\n",
    "eval_data_path = 'datasetNEW/eval'\n",
    "\n",
    "# Define the fixed number of frames\n",
    "TARGET_FRAMES = 126\n",
    "\n",
    "# GAN-specific parameters\n",
    "latent_dim = 100\n",
    "mel_spectrogram_shape = (80, TARGET_FRAMES) \n",
    "\n",
    "# Start low and increase if memory allows. Try 16, 8, or even 4 if needed.\n",
    "batch_size = 8 \n",
    "\n",
    "epochs = 7  # Adjust for GAN pre-training.\n",
    "\n",
    "# Create instances\n",
    "generator = create_generator(latent_dim, mel_spectrogram_shape)\n",
    "discriminator = create_discriminator(mel_spectrogram_shape)\n",
    "gan = create_gan(generator, discriminator, latent_dim)\n",
    "\n",
    "# Diagnostic Code: Verify Output Shape\n",
    "test_noise = np.random.normal(0, 1, (1, latent_dim))\n",
    "generated_image = generator.predict(test_noise)\n",
    "print(\"Shape of generated image (Generator Output):\", generated_image.shape)\n",
    "\n",
    "discriminator_input_shape_expected = (mel_spectrogram_shape[0], mel_spectrogram_shape[1], 1)\n",
    "print(\"Expected shape for Discriminator Input:\", discriminator_input_shape_expected)\n",
    "\n",
    "# Report the models\n",
    "generator.summary()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training Discriminator...\n",
      "Discriminator compiled for pre-training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279ded3b48be4a9aa550aabbc35dafb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pre-training D:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin GAN training!\n",
      "GAN model compiled for training.\n",
      "Discriminator re-compiled for epoch training.\n",
      "Calculated 2850 batches per epoch for GAN training.\n",
      "\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8b0ee5030f4effaa9aec9078a42d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished.\n",
      "\n",
      "Epoch 2/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d353a6774ef4a3ea3f4a825cf306cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/2850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished.\n",
      "Saved generator and discriminator models for epoch 2\n",
      "\n",
      "Epoch 3/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95a59a1abfd4afc86345bac0e116950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/2850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished.\n",
      "\n",
      "Epoch 4/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c24071f5b24b489b3305f25f5459e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/2850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished.\n",
      "Saved generator and discriminator models for epoch 4\n",
      "\n",
      "Epoch 5/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bdd5f39d9445979061e93f6020e67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/2850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished.\n",
      "\n",
      "Epoch 6/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7005774528f24bcdaa516dd9e7c29af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/2850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished.\n",
      "Saved generator and discriminator models for epoch 6\n",
      "\n",
      "Epoch 7/7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8447886fd47a492b878f0819fff8f3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/2850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: GAN Training Loop (Corrected Epoch Batch Limit)\n",
    "\n",
    "# Optimizer\n",
    "discriminator_optimizer = Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "generator_optimizer = Adam(learning_rate=0.0001, beta_1=0.5)\n",
    "\n",
    "# --- Pre-training Discriminator (No change needed here) ---\n",
    "print(\"Pre-training Discriminator...\")\n",
    "real_batch_size = batch_size\n",
    "num_pretrain_steps = 1000\n",
    "discriminator.trainable = True\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"Discriminator compiled for pre-training.\")\n",
    "train_gen_GAN_pre = data_generator_GAN(train_data_path, batch_size=real_batch_size)\n",
    "pretrain_pbar = tqdm(range(num_pretrain_steps), desc=\"Pre-training D\")\n",
    "for i in pretrain_pbar:\n",
    "    try:\n",
    "        real_spoof_samples = next(train_gen_GAN_pre)\n",
    "        current_batch_size = real_spoof_samples.shape[0]\n",
    "        if current_batch_size == 0: continue\n",
    "        noise = np.random.normal(0, 1, (current_batch_size, latent_dim))\n",
    "        generated_images = generator(noise, training=False)\n",
    "        y_real = np.ones((current_batch_size, 1)) * 0.9\n",
    "        y_fake = np.zeros((current_batch_size, 1))\n",
    "        d_loss_real, d_acc_real = discriminator.train_on_batch(real_spoof_samples, y_real)\n",
    "        d_loss_fake, d_acc_fake = discriminator.train_on_batch(generated_images, y_fake)\n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "        d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
    "        pretrain_pbar.set_postfix({\"D Loss\": f\"{d_loss:.4f}\", \"D Acc\": f\"{d_acc:.4f}\"})\n",
    "    except StopIteration:\n",
    "        print(\"Pre-training generator exhausted. Re-initializing.\")\n",
    "        train_gen_GAN_pre = data_generator_GAN(train_data_path, batch_size=real_batch_size)\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during pre-training step {i}: {e}\")\n",
    "        print(f\"Error during pre-training step {i}: {e}\")\n",
    "        continue\n",
    "# --- End of Pre-training ---\n",
    "\n",
    "print(\"\\nBegin GAN training!\")\n",
    "\n",
    "# Compile GAN and Discriminator before the epoch loop\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss='binary_crossentropy', optimizer=generator_optimizer)\n",
    "print(\"GAN model compiled for training.\")\n",
    "discriminator.trainable = True\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"Discriminator re-compiled for epoch training.\")\n",
    "\n",
    "# --- Calculate total batches per epoch accurately ---\n",
    "fake_files_count = len([f for f in os.listdir(os.path.join(train_data_path, 'fake')) if f.endswith('.wav')])\n",
    "if batch_size > 0:\n",
    "    # Use ceiling division to get the correct number of batches\n",
    "    batches_per_epoch = int(np.ceil(fake_files_count / float(batch_size)))\n",
    "else:\n",
    "    batches_per_epoch = 0\n",
    "print(f\"Calculated {batches_per_epoch} batches per epoch for GAN training.\")\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "# Combined training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    # Re-create data generator for each epoch - it will restart automatically\n",
    "    train_gen_GAN = data_generator_GAN(train_data_path, batch_size=batch_size)\n",
    "\n",
    "    # --- Wrap the range of batches with tqdm ---\n",
    "    epoch_pbar = tqdm(range(batches_per_epoch), desc=f\"Epoch {epoch+1}\")\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # --- Loop exactly batches_per_epoch times ---\n",
    "    for batch_idx in epoch_pbar:\n",
    "        try:\n",
    "            # Get the next batch from the generator\n",
    "            real_spoof_samples = next(train_gen_GAN)\n",
    "            current_batch_size = real_spoof_samples.shape[0]\n",
    "            if current_batch_size == 0:\n",
    "                 print(f\"Warning: Skipped empty batch at step {batch_idx}\")\n",
    "                 continue # Skip if the generator somehow yields empty\n",
    "\n",
    "            # --- 1: Train the Discriminator ---\n",
    "            noise = np.random.normal(0, 1, (current_batch_size, latent_dim))\n",
    "            generated_samples = generator(noise, training=False)\n",
    "            y_real = np.ones((current_batch_size, 1)) * 0.9\n",
    "            y_fake = np.zeros((current_batch_size, 1))\n",
    "\n",
    "            d_loss_real, d_acc_real = discriminator.train_on_batch(real_spoof_samples, y_real)\n",
    "            d_loss_fake, d_acc_fake = discriminator.train_on_batch(generated_samples, y_fake)\n",
    "            d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "            d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
    "\n",
    "            # --- 2: Train the Generator ---\n",
    "            noise = np.random.normal(0, 1, (current_batch_size, latent_dim)) # Use current_batch_size\n",
    "            valid_y = np.ones((current_batch_size, 1))\n",
    "\n",
    "            # Generator needs to be trained on a batch size it expects,\n",
    "            # or handle variable last batch size if architecture allows.\n",
    "            # For simplicity here, we assume GAN can handle the last smaller batch.\n",
    "            g_loss = gan.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Update tqdm postfix\n",
    "            epoch_pbar.set_postfix({\"D Loss\": f\"{d_loss:.4f}\", \"G Loss\": f\"{g_loss:.4f}\", \"D Acc\": f\"{d_acc:.4f}\"})\n",
    "\n",
    "        except StopIteration:\n",
    "            # This *shouldn't* happen if batches_per_epoch is calculated correctly,\n",
    "            # but adding safety break.\n",
    "            print(f\"Generator stopped unexpectedly at batch {batch_idx}. Moving to next epoch.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during GAN training epoch {epoch+1}, batch {batch_idx}: {e}\")\n",
    "            print(f\"Error during GAN training epoch {epoch+1}, batch {batch_idx}: {e}\")\n",
    "            # Decide whether to continue or break the epoch\n",
    "            continue # Skip this batch\n",
    "\n",
    "    # End-of-epoch actions\n",
    "    print(f\"Epoch {epoch+1} finished.\")\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "         generator.save(f'generator_epoch_{epoch+1}.keras')\n",
    "         discriminator.save(f'discriminator_epoch_{epoch+1}.keras')\n",
    "         print(f\"Saved generator and discriminator models for epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Steps/Epoch: 3172, Validation Steps: 3105, Eval Steps: 8904\n",
      "StandAlone DISCRIMINATOR training!\n",
      "\n",
      "Starting discriminator training for 50 epochs...\n",
      "Epoch 1/50\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.6612 - auc: 0.6757 - loss: 8.4413\n",
      "Epoch 1: val_accuracy improved from -inf to 0.81699, saving model to ./training_checkpoints_discriminator/ckpt_disc_1.weights.h5\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_1.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 134ms/step - accuracy: 0.6612 - auc: 0.6758 - loss: 8.4403 - val_accuracy: 0.8170 - val_auc: 0.8660 - val_loss: 1.2962 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7779 - auc: 0.8206 - loss: 1.9196\n",
      "Epoch 2: val_accuracy improved from 0.81699 to 0.82234, saving model to ./training_checkpoints_discriminator/ckpt_disc_2.weights.h5\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_2.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 136ms/step - accuracy: 0.7779 - auc: 0.8206 - loss: 1.9196 - val_accuracy: 0.8223 - val_auc: 0.9140 - val_loss: 0.6823 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8215 - auc: 0.8797 - loss: 1.0275\n",
      "Epoch 3: val_accuracy improved from 0.82234 to 0.89557, saving model to ./training_checkpoints_discriminator/ckpt_disc_3.weights.h5\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_3.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 144ms/step - accuracy: 0.8215 - auc: 0.8797 - loss: 1.0275 - val_accuracy: 0.8956 - val_auc: 0.9082 - val_loss: 0.7654 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8398 - auc: 0.9062 - loss: 0.7036\n",
      "Epoch 4: val_accuracy did not improve from 0.89557\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_4.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 135ms/step - accuracy: 0.8398 - auc: 0.9062 - loss: 0.7036 - val_accuracy: 0.8264 - val_auc: 0.9506 - val_loss: 0.3791 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8658 - auc: 0.9368 - loss: 0.4579\n",
      "Epoch 5: val_accuracy improved from 0.89557 to 0.93635, saving model to ./training_checkpoints_discriminator/ckpt_disc_5.weights.h5\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_5.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 132ms/step - accuracy: 0.8658 - auc: 0.9368 - loss: 0.4579 - val_accuracy: 0.9364 - val_auc: 0.9297 - val_loss: 0.8570 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8802 - auc: 0.9489 - loss: 0.3580\n",
      "Epoch 6: val_accuracy improved from 0.93635 to 0.93948, saving model to ./training_checkpoints_discriminator/ckpt_disc_6.weights.h5\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_6.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 131ms/step - accuracy: 0.8802 - auc: 0.9489 - loss: 0.3580 - val_accuracy: 0.9395 - val_auc: 0.9522 - val_loss: 0.6008 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m3171/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9027 - auc: 0.9642 - loss: 0.2689\n",
      "Epoch 7: val_accuracy did not improve from 0.93948\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_7.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 132ms/step - accuracy: 0.9027 - auc: 0.9642 - loss: 0.2689 - val_accuracy: 0.9308 - val_auc: 0.9558 - val_loss: 0.4450 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m3171/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9126 - auc: 0.9707 - loss: 0.2244\n",
      "Epoch 8: val_accuracy did not improve from 0.93948\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_8.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 131ms/step - accuracy: 0.9126 - auc: 0.9707 - loss: 0.2244 - val_accuracy: 0.8796 - val_auc: 0.9636 - val_loss: 0.2607 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m3171/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9160 - auc: 0.9758 - loss: 0.2108\n",
      "Epoch 9: val_accuracy improved from 0.93948 to 0.94677, saving model to ./training_checkpoints_discriminator/ckpt_disc_9.weights.h5\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_9.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 128ms/step - accuracy: 0.9160 - auc: 0.9758 - loss: 0.2108 - val_accuracy: 0.9468 - val_auc: 0.9584 - val_loss: 0.5927 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m3171/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9260 - auc: 0.9805 - loss: 0.1738\n",
      "Epoch 10: val_accuracy did not improve from 0.94677\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_10.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 127ms/step - accuracy: 0.9260 - auc: 0.9805 - loss: 0.1738 - val_accuracy: 0.9131 - val_auc: 0.9587 - val_loss: 0.3014 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m3171/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9307 - auc: 0.9840 - loss: 0.1656\n",
      "Epoch 11: val_accuracy did not improve from 0.94677\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_11.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 125ms/step - accuracy: 0.9307 - auc: 0.9840 - loss: 0.1656 - val_accuracy: 0.9125 - val_auc: 0.9669 - val_loss: 0.2657 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m3171/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9376 - auc: 0.9848 - loss: 0.1529\n",
      "Epoch 12: val_accuracy did not improve from 0.94677\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_12.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m396s\u001b[0m 125ms/step - accuracy: 0.9376 - auc: 0.9848 - loss: 0.1529 - val_accuracy: 0.9287 - val_auc: 0.9669 - val_loss: 0.2965 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m3171/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9412 - auc: 0.9870 - loss: 0.1414\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.94677 to 0.94834, saving model to ./training_checkpoints_discriminator/ckpt_disc_13.weights.h5\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_13.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 127ms/step - accuracy: 0.9412 - auc: 0.9870 - loss: 0.1414 - val_accuracy: 0.9483 - val_auc: 0.9711 - val_loss: 0.3200 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9522 - auc: 0.9925 - loss: 0.1074\n",
      "Epoch 14: val_accuracy improved from 0.94834 to 0.95188, saving model to ./training_checkpoints_discriminator/ckpt_disc_14.weights.h5\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_14.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 125ms/step - accuracy: 0.9522 - auc: 0.9925 - loss: 0.1074 - val_accuracy: 0.9519 - val_auc: 0.9730 - val_loss: 0.3189 - learning_rate: 2.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9565 - auc: 0.9931 - loss: 0.1005\n",
      "Epoch 15: val_accuracy did not improve from 0.95188\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_15.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 126ms/step - accuracy: 0.9565 - auc: 0.9931 - loss: 0.1005 - val_accuracy: 0.9425 - val_auc: 0.9706 - val_loss: 0.2874 - learning_rate: 2.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9562 - auc: 0.9921 - loss: 0.1061\n",
      "Epoch 16: val_accuracy improved from 0.95188 to 0.95470, saving model to ./training_checkpoints_discriminator/ckpt_disc_16.weights.h5\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_16.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 125ms/step - accuracy: 0.9562 - auc: 0.9921 - loss: 0.1061 - val_accuracy: 0.9547 - val_auc: 0.9721 - val_loss: 0.3642 - learning_rate: 2.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9599 - auc: 0.9926 - loss: 0.1002\n",
      "Epoch 17: val_accuracy improved from 0.95470 to 0.95611, saving model to ./training_checkpoints_discriminator/ckpt_disc_17.weights.h5\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_17.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 126ms/step - accuracy: 0.9599 - auc: 0.9926 - loss: 0.1002 - val_accuracy: 0.9561 - val_auc: 0.9763 - val_loss: 0.2875 - learning_rate: 2.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9616 - auc: 0.9942 - loss: 0.0947\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.95611\n",
      "\n",
      "Training history plot saved to training_figures/spoof_detector_discriminator_epoch_18.png\n",
      "\u001b[1m3172/3172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 126ms/step - accuracy: 0.9616 - auc: 0.9942 - loss: 0.0947 - val_accuracy: 0.9516 - val_auc: 0.9764 - val_loss: 0.2629 - learning_rate: 2.0000e-05\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Discriminator (Spoof Detector) training complete and saved as spoof_detector_final.keras\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Define Callbacks and Train Standalone Discriminator\n",
    "\n",
    "# --- ADD THIS CLASS DEFINITION BACK ---\n",
    "# Create a custom callback to save training history plots after each epoch\n",
    "class PlotTrainingHistory(Callback):\n",
    "    def __init__(self, model_name='model'):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.auc = [] # Added to track AUC\n",
    "        self.val_auc = [] # Added to track validation AUC\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('accuracy'))\n",
    "        self.val_acc.append(logs.get('val_accuracy'))\n",
    "        self.auc.append(logs.get('auc')) # Get AUC\n",
    "        self.val_auc.append(logs.get('val_auc')) # Get val_AUC\n",
    "\n",
    "\n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(18, 5)) # Adjusted figure size slightly\n",
    "\n",
    "        # Accuracy Plot\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.acc, label='Training Accuracy')\n",
    "        plt.plot(self.val_acc, label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        # Loss Plot\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.loss, label='Training Loss')\n",
    "        plt.plot(self.val_loss, label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # AUC Plot\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.auc, label='Training AUC')\n",
    "        plt.plot(self.val_auc, label='Validation AUC')\n",
    "        plt.title('Model AUC')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # Save the plot to a file\n",
    "        filepath = os.path.join(FIGURES_DIR, f'{self.model_name}_epoch_{epoch+1}.png')\n",
    "        plt.savefig(filepath)\n",
    "        plt.close() # Close the plot to avoid displaying it in the notebook output repeatedly\n",
    "        print(f'\\nTraining history plot saved to {filepath}')\n",
    "# --- END OF CLASS DEFINITION ---\n",
    "\n",
    "\n",
    "# After GAN training, train the DISCRIMINATOR as your spoof detector\n",
    "# First create all data generator.\n",
    "train_gen = data_generator(train_data_path, batch_size=batch_size)\n",
    "dev_gen = data_generator(dev_data_path, batch_size=batch_size)\n",
    "eval_gen = data_generator(eval_data_path, batch_size=batch_size)\n",
    "\n",
    "# Then calculate steps per epoch.\n",
    "def count_files(path):\n",
    "    # Check if 'real' and 'fake' directories exist\n",
    "    real_dir = os.path.join(path, 'real')\n",
    "    fake_dir = os.path.join(path, 'fake')\n",
    "    real_files = []\n",
    "    fake_files = []\n",
    "    if os.path.exists(real_dir) and os.path.isdir(real_dir):\n",
    "         real_files = [f for f in os.listdir(real_dir) if f.endswith('.wav')]\n",
    "    else:\n",
    "        print(f\"Warning: Directory not found - {real_dir}\")\n",
    "    if os.path.exists(fake_dir) and os.path.isdir(fake_dir):\n",
    "         fake_files = [f for f in os.listdir(fake_dir) if f.endswith('.wav')]\n",
    "    else:\n",
    "        print(f\"Warning: Directory not found - {fake_dir}\")\n",
    "    return len(real_files) + len(fake_files)\n",
    "\n",
    "\n",
    "train_samples_count = count_files(train_data_path)\n",
    "dev_samples_count = count_files(dev_data_path)\n",
    "eval_samples_count = count_files(eval_data_path)\n",
    "\n",
    "\n",
    "# Ensure counts are not zero before division\n",
    "if batch_size <= 0:\n",
    "    raise ValueError(\"Batch size must be positive.\")\n",
    "if train_samples_count == 0:\n",
    "     print(\"Warning: No training files found. Setting steps_per_epoch to 0.\")\n",
    "     steps_per_epoch = 0\n",
    "else:\n",
    "    steps_per_epoch = train_samples_count // batch_size\n",
    "\n",
    "\n",
    "if dev_samples_count == 0:\n",
    "     print(\"Warning: No development files found. Setting validation_steps to 0.\")\n",
    "     validation_steps = 0\n",
    "else:\n",
    "     validation_steps = dev_samples_count // batch_size\n",
    "\n",
    "\n",
    "if eval_samples_count == 0:\n",
    "     print(\"Warning: No evaluation files found. Setting eval_steps to 0.\")\n",
    "     eval_steps = 0\n",
    "else:\n",
    "     eval_steps = eval_samples_count // batch_size\n",
    "\n",
    "print(f\"Train Steps/Epoch: {steps_per_epoch}, Validation Steps: {validation_steps}, Eval Steps: {eval_steps}\")\n",
    "\n",
    "\n",
    "print(\"StandAlone DISCRIMINATOR training!\")\n",
    "\n",
    "discriminator.trainable = True #This may already be set to false from GAN training loop\n",
    "\n",
    "#Add Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10, # Increased patience slightly\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Create a checkpoint directory\n",
    "checkpoint_dir = './training_checkpoints_discriminator' # Separate dir for clarity\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_disc_{epoch}.weights.h5\") # Add .weights.h5 extension\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy', # Monitor validation accuracy\n",
    "    mode='max', # Save the one with max accuracy\n",
    "    save_best_only=True,  # Save only the best model based on monitor\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "plot_training_callback = PlotTrainingHistory(model_name='spoof_detector_discriminator') # create instance here\n",
    "\n",
    "\n",
    "# Recompile for standalone training\n",
    "discriminator_optimizer = Adam(learning_rate=0.0001) # Re-create optimizer or ensure state is appropriate\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]) # Named AUC\n",
    "\n",
    "\n",
    "# Ensure steps_per_epoch and validation_steps are > 0 before fitting\n",
    "if steps_per_epoch > 0 and validation_steps > 0:\n",
    "    # Train the model\n",
    "    print(f\"\\nStarting discriminator training for 50 epochs...\")\n",
    "    history = discriminator.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=50,  # Can increase this now\n",
    "        validation_data=dev_gen,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[reduce_lr, early_stopping, checkpoint_callback, plot_training_callback] # add the callbacks\n",
    "    )\n",
    "\n",
    "    discriminator.save('spoof_detector_final.keras') #Save the final discriminator\n",
    "    print(\"Discriminator (Spoof Detector) training complete and saved as spoof_detector_final.keras\")\n",
    "\n",
    "    # --- Optional: Load best weights ---\n",
    "    # print(\"Loading best weights based on validation accuracy...\")\n",
    "    # best_epoch_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    # if best_epoch_checkpoint:\n",
    "    #     discriminator.load_weights(best_epoch_checkpoint)\n",
    "    #     print(f\"Loaded weights from {best_epoch_checkpoint}\")\n",
    "    #     # Optional: Save the model with the best weights loaded\n",
    "    #     discriminator.save('spoof_detector_best_val_acc.keras')\n",
    "    #     print(\"Saved discriminator with best validation accuracy weights.\")\n",
    "    # else:\n",
    "    #     print(\"Could not find checkpoint weights to load.\")\n",
    "    # ------------------------------------\n",
    "\n",
    "else:\n",
    "    print(\"Skipping discriminator training because steps_per_epoch or validation_steps is zero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the final Discriminator (Spoof Detector)...\n",
      "Using 8905 steps for evaluation.\n",
      "\u001b[1m8905/8905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m599s\u001b[0m 67ms/step - accuracy: 0.8436 - auc: 0.8236 - loss: 0.6088\n",
      "\n",
      "--- Evaluation Results ---\n",
      "Loss: 0.5317\n",
      "Accuracy: 0.8070\n",
      "AUC: 0.9185\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Evaluation\n",
    "\n",
    "print(\"\\nEvaluating the final Discriminator (Spoof Detector)...\")\n",
    "\n",
    "# --- CORRECT calculation of eval_steps ---\n",
    "# Use the count_files function defined in Cell 13\n",
    "if eval_samples_count == 0:\n",
    "     print(\"Warning: No evaluation files found. Setting eval_steps to 0.\")\n",
    "     eval_steps = 0\n",
    "else:\n",
    "     eval_steps = eval_samples_count // batch_size\n",
    "     if eval_samples_count % batch_size != 0: # Add incomplete batch if needed\n",
    "         eval_steps += 1 # Ensure all samples are evaluated if generator loops\n",
    "\n",
    "print(f\"Using {eval_steps} steps for evaluation.\")\n",
    "# -----------------------------------------\n",
    "\n",
    "# Re-create the evaluation generator to ensure it starts from the beginning\n",
    "eval_gen = data_generator(eval_data_path, batch_size=batch_size, shuffle=False) # Turn shuffle OFF for consistent evaluation\n",
    "\n",
    "# Ensure eval_steps is valid before evaluating\n",
    "if eval_steps > 0:\n",
    "    # Evaluate the model\n",
    "    results = discriminator.evaluate(\n",
    "        eval_gen,\n",
    "        steps=eval_steps,\n",
    "        verbose=1 # Show progress bar during evaluation\n",
    "        )\n",
    "\n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    # Match the names with the compiled metrics (loss, accuracy, auc)\n",
    "    print(f\"Loss: {results[0]:.4f}\")\n",
    "    print(f\"Accuracy: {results[1]:.4f}\")\n",
    "    print(f\"AUC: {results[2]:.4f}\")\n",
    "    print(\"--------------------------\")\n",
    "else:\n",
    "    print(\"Skipping evaluation because eval_steps is zero (no evaluation data found or batch size issue).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.5030\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 0.1512\n",
      "t-DCF: 0.1511\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Reporting (Confusion Matrix, EER, t-DCF)\n",
    "# Load data into variables.\n",
    "discriminator.trainable = False #Freeze discriminator so it does change during the rest of the process\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# EER and t-DCF related imports\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Define t-DCF parameters (these should be set according to your task)\n",
    "p_target = 0.05  # Prior probability of target speaker\n",
    "c_miss = 1       # Cost of a miss (false negative)\n",
    "c_false_alarm = 1 # Cost of a false alarm (false positive)\n",
    "\n",
    "# Reset the generator to its initial state\n",
    "eval_gen = data_generator(eval_data_path, batch_size=batch_size)\n",
    "\n",
    "# Generate predictions and collect true labels\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for _ in range(eval_steps):\n",
    "    batch_x, batch_y, _ = next(eval_gen)\n",
    "    batch_pred = discriminator.predict(batch_x, verbose=0)\n",
    "    y_pred.extend(batch_pred.flatten())\n",
    "    y_true.extend(batch_y)\n",
    "\n",
    "# Convert to numpy arrays and ensure same length\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "min_len = min(len(y_pred), len(y_true))\n",
    "y_pred = y_pred[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(y_true, y_pred_binary)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_binary)\n",
    "\n",
    "# Convert confusion matrix to percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Visualize confusion matrix as percentages\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap='Blues', cbar_kws={'format': '%.0f%%' })\n",
    "plt.title('Confusion Matrix (Percentage)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig(os.path.join(FIGURES_DIR, 'confusion_matrix.png'))  # Save confusion matrix\n",
    "plt.show()\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# EER Calculation\n",
    "#---------------------------------------------------------------\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n",
    "\n",
    "# Find the EER threshold\n",
    "eer_threshold = thresholds[np.argmin(np.abs(fpr - (1-tpr)))]\n",
    "\n",
    "# Calculate EER\n",
    "eer = fpr[np.argmin(np.abs(fpr - (1-tpr)))]\n",
    "\n",
    "print(f\"EER: {eer:.4f}\")\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# t-DCF Calculation\n",
    "#---------------------------------------------------------------\n",
    "def calculate_t_dcf(y_true, y_pred, p_target, c_miss, c_false_alarm, threshold):\n",
    "    \"\"\"\n",
    "    Calculates the tuned Detection Cost Function (t-DCF).\n",
    "    \"\"\"\n",
    "    # Apply threshold to get binary predictions\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "    # Calculate confusion matrix elements\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_binary).ravel()\n",
    "\n",
    "    # Calculate False Alarm Rate (FAR) and Miss Rate (FR)\n",
    "    far = fp / (tn + fp)\n",
    "    fr = fn / (tp + fn)\n",
    "\n",
    "    # Calculate t-DCF\n",
    "    t_dcf = c_miss * p_target * fr + c_false_alarm * (1 - p_target) * far\n",
    "\n",
    "    return t_dcf\n",
    "\n",
    "# Calculate t-DCF using the EER threshold\n",
    "t_dcf = calculate_t_dcf(y_true, y_pred, p_target, c_miss, c_false_alarm, eer_threshold)\n",
    "print(f\"t-DCF: {t_dcf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
