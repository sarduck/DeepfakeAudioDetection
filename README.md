# Deepfake Audio Detection using WGAN-GP + Attention Pre-training

## Overview

This project implements a deep learning system for detecting deepfaked audio clips. It utilizes Mel Spectrograms as the input features derived from `.wav` audio files.

The core approach is a two-stage training process:

1.  **Pre-training Phase:** A Wasserstein GAN with Gradient Penalty (WGAN-GP) incorporating Self-Attention layers is trained.
    *   The **Generator** learns to create realistic fake Mel Spectrograms from random noise.
    *   The **Critic** (Discriminator in WGAN terms) learns to distinguish between fake spectrograms generated by the Generator and "real" fake spectrograms provided in the training dataset's `fake` folder. The goal of this phase is *not* to generate fakes, but to train the Critic to become a robust feature extractor sensitive to the nuances distinguishing authentic/fake audio characteristics. WGAN-GP is used for improved training stability compared to standard GANs. Self-Attention helps capture long-range dependencies in the spectrograms.
2.  **Classifier Fine-tuning Phase:** The pre-trained **Critic** from the WGAN-GP phase is taken, its final output layer is replaced with a sigmoid classification layer, and the entire network is then fine-tuned as a binary classifier on the full dataset (real vs. fake audio spectrograms). This leverages the features learned during the GAN pre-training.

The final output is a trained classifier (`spoof_detector`) capable of predicting whether a given audio spectrogram represents real or fake audio, along with detailed performance metrics.

## Features

*   Deepfake Audio Detection
*   Mel Spectrogram feature extraction
*   WGAN-GP for stable adversarial pre-training
*   Self-Attention mechanism for capturing global dependencies
*   Two-stage training: GAN pre-training followed by classifier fine-tuning
*   Detailed evaluation including Accuracy, AUC, F1-Score, EER, and t-DCF

## Dataset

The code expects the dataset to be organized as follows:
datasetNEW/
├── train/
│ ├── real/
│ │ └── *.wav
│ └── fake/
│ └── *.wav
├── dev/
│ ├── real/
│ │ └── *.wav
│ └── fake/
│ └── *.wav
└── eval/
├── real/
│ └── *.wav
└── fake/
└── *.wav

*   Audio files should be in `.wav` format.
*   The code assumes a target sample rate of **16000 Hz** and processes audio in **4-second** segments (padding or truncating as needed). These can be adjusted in the data loading/parameter cells.

## Setup and Installation

**1. Environment:**
It is highly recommended to use a virtual environment (like `venv` or `conda`) to manage dependencies.

*   Using `venv`:
    ```bash
    python -m venv .venvn         # Create virtual environment
    source .venvn/bin/activate    # Activate (Linux/macOS)
    # .\ .venvn\Scripts\activate # Activate (Windows)
    pip install --upgrade pip     # Upgrade pip
    ```
*   Using `conda`:
    ```bash
    conda create -n spoofenv python=3.10  # Or your preferred Python 3.8+ version
    conda activate spoofenv
    ```

**2. Python Version:**
The code was tested with Python 3.10+. Compatibility with other Python 3.x versions is likely but not guaranteed.

**3. GPU Requirements:**
An **NVIDIA GPU with CUDA support is strongly recommended** for reasonable training times. Ensure you have compatible NVIDIA drivers, CUDA Toolkit, and cuDNN installed for your OS and GPU. TensorFlow's GPU support handles the interaction.

**4. Install Dependencies:**
You can install the required packages using `pip`.

*   **Method 1: Direct Install**
    ```bash
    pip install tensorflow[and-cuda]>=2.15 numpy matplotlib seaborn scikit-learn librosa soundfile tqdm notebook
    ```
    *(Note: `tensorflow[and-cuda]` attempts to install TensorFlow with GPU support and its CUDA dependencies. Adjust the version `2.15` if needed based on your CUDA/cuDNN setup. Check TensorFlow documentation for compatibility.)*
    *(Note: `noisereduce` was commented out in later versions; install if you re-enable it: `pip install noisereduce`)*

*   **Method 2: Using `requirements.txt`**
    Create a file named `requirements.txt` in the project directory with the following content:

    ```
    # requirements.txt
    numpy
    tensorflow[and-cuda]>=2.15
    librosa
    soundfile
    matplotlib
    seaborn
    scikit-learn
    tqdm
    notebook
    # noisereduce # Optional, if used
    ```
    Then install using:
    ```bash
    pip install -r requirements.txt
    ```

## Usage / Running the Code

1.  **Activate** your virtual environment.
2.  **Launch Jupyter Notebook:**
    ```bash
    jupyter notebook
    ```
3.  **Open the Notebook:** Navigate to and open the `.ipynb` file (e.g., `wgan_sa_spoof_detection.ipynb`).
4.  **Run Cells Sequentially:** Execute the cells in order from top to bottom using "Run" or Shift+Enter.
    *   **Parameter Adjustment (Cell 11):** Pay attention to parameters like `classifier_batch_size`, `gan_batch_size`, `gan_epochs` (for initial WGAN training), learning rates (`cont_critic_lr`, `cont_gen_lr`, `classifier_optimizer` learning rate), `gp_weight`, `n_critic`. These may need tuning based on your hardware and dataset for stable training and optimal results.
    *   **WGAN Training (Cell 12/12.5):** This phase trains the Generator and Critic. Monitor the Critic (C Loss) and Generator (G Loss). Stable C Loss near zero or slightly positive and decreasing (less negative) G loss are generally good signs. This phase does *not* show accuracy. Weights (`.weights.h5`) are saved periodically.
    *   **Classifier Training (Cell 13):** This loads the best Critic weights, modifies the model, and fine-tunes it as a classifier. Monitor standard metrics (Loss, Accuracy, AUC) and watch for `ReduceLROnPlateau` and `EarlyStopping` activations. The best classifier weights based on `val_auc` are saved.
    *   **Evaluation (Cell 14):** Loads the best saved classifier and evaluates it on the `eval` set, printing Loss, Accuracy, and AUC.
    *   **Reporting (Cell 15):** Generates detailed reports including F1 scores, confusion matrices, threshold analysis plots, Precision-Recall curve, Calibration plot, EER, and t-DCF using the best classifier model on the `eval` set.

## File Structure (Expected)

.
├── datasetNEW/ # Root dataset directory (or adjust path in code)
│ ├── train/
│ ├── dev/
│ └── eval/
├── training_figures_wgan_sa/ # Directory for saved plots
│ └── *.png
├── training_checkpoints_spoof_detector_wgan_sa/ # Checkpoints for classifier
│ └── *.weights.h5
├── *.weights.h5 # Saved weights for Generator/Critic from WGAN phase
├── *.keras # Saved final classifier model(s)
├── wgan_sa_spoof_detection.ipynb # The main Jupyter Notebook (adjust name if different)
├── README.md # This file
└── requirements.txt # Optional requirements file

## Output / Results

*   **Console Output:** Training progress (losses, metrics per epoch), evaluation results (Loss, Acc, AUC), final report metrics (F1, EER, t-DCF).
*   **Saved Model Files:**
    *   `generator_wgan_sa_epoch_*.weights.h5`: Generator weights saved during WGAN training.
    *   `critic_wgan_sa_epoch_*.weights.h5`: Critic weights saved during WGAN training.
    *   `clf_ep*.weights.h5`: Best classifier weights saved during fine-tuning (inside `training_checkpoints...` dir).
    *   `spoof_detector_best_val_auc_wgan_sa.keras`: The final, best classifier model saved after training.
*   **Saved Plots:** Located in `training_figures_wgan_sa/`, including:
    *   GAN loss plots (`gan_loss_...png`)
    *   Classifier training history (`spoof_detector_...png`)
    *   Confusion matrices
    *   Precision/Recall/F1 vs. Threshold
    *   Precision-Recall Curve
    *   Calibration Plot

## Troubleshooting / Notes

*   **GPU Memory (OOM Errors):** WGAN-GP with Self-Attention can be memory-intensive. If you encounter Out-of-Memory errors, the primary solution is to **reduce `gan_batch_size` and/or `classifier_batch_size`** (in Cell 11/13). You might also need to simplify the model architecture (fewer filters in `Conv2D` layers) or apply Self-Attention to smaller feature maps. Note the relatively low reported VRAM (2143MB) on the 3050Ti in testing - this card might require small batch sizes.
*   **WGAN Instability:** If Critic Loss explodes or oscillates wildly during Cell 12/12.5, try **significantly reducing the learning rates** (`cont_critic_lr`, `cont_gen_lr` - e.g., to `1e-5` or lower). Adjusting `gp_weight` might also help.
*   **XLA Warnings (`layout failed`, `ptxas warning`):**
    *   `layout failed`: Often caused by empty batches being processed by the optimized graph. Ensure generators in Cell 6 correctly handle edge cases and don't yield empty data. If persists, try disabling XLA (`tf.config.optimizer.set_jit(False)` at the start) for diagnosis, but this may slow down training.
    *   `ptxas warning : Registers are spilled...`: This is a performance warning indicating the GPU compiler needed to use slower memory. Training will continue, but might be slower. Can often be ignored if training time is acceptable. Reducing model complexity can alleviate it.
*   **Low F1 / High t-DCF:** Even with good AUC, F1 and t-DCF can be poor due to class imbalance or poor score calibration. Analyze the threshold plots and calibration curve in Cell 15. Consider if the t-DCF parameters (`p_target`) match your actual application needs.
      

