{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, GRU, Dense, Dropout, BatchNormalization, LayerNormalization, Reshape, Permute, Bidirectional, Add, Attention, Flatten, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.layers import Layer, Concatenate\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, sosfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force GPU usage\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "else:\n",
    "    print(\"No GPU devices found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and preprocessing\n",
    "def load_and_preprocess_audio(file_path, sr=16000, duration=4):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
    "\n",
    "        # Data Augmentation (increased probability and variety)\n",
    "        if np.random.random() < 0.5:  # 50% chance of applying augmentation\n",
    "            augmentation_type = np.random.choice(['noise', 'pitch', 'speed'])\n",
    "            if augmentation_type == 'noise':\n",
    "                noise = np.random.randn(len(audio)) * 0.005\n",
    "                audio = audio + noise\n",
    "            elif augmentation_type == 'pitch':\n",
    "                audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=np.random.uniform(-2, 2))\n",
    "            else:  # speed\n",
    "                audio = librosa.effects.time_stretch(audio, rate=np.random.uniform(0.8, 1.2))\n",
    "\n",
    "        # Normalize audio\n",
    "        audio = audio - np.mean(audio)\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "\n",
    "        # Pad if necessary\n",
    "        if len(audio) < sr * duration:\n",
    "            audio = np.pad(audio, (0, sr * duration - len(audio)))\n",
    "        return audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio, sr=16000, n_mels=80, n_fft=2048, hop_length=512):\n",
    "    if audio is None:\n",
    "        return None\n",
    "\n",
    "    # Extract mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio,\n",
    "        sr=sr,\n",
    "        n_mels=n_mels,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    # Normalize features\n",
    "    log_mel_spec = (log_mel_spec - np.mean(log_mel_spec)) / np.std(log_mel_spec)\n",
    "    return log_mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution for debugging class imbalance\n",
    "def analyze_class_distribution(data_path):\n",
    "    real_count = len([f for f in os.listdir(os.path.join(data_path, 'real')) if f.endswith('.wav')])\n",
    "    fake_count = len([f for f in os.listdir(os.path.join(data_path, 'fake')) if f.endswith('.wav')])\n",
    "    total = real_count + fake_count\n",
    "    print(f\"\\nClass Distribution for {data_path}:\")\n",
    "    print(f\"Real: {real_count} ({real_count/total*100:.2f}%)\")\n",
    "    print(f\"Fake: {fake_count} ({fake_count/total*100:.2f}%)\")\n",
    "    return {'real': real_count, 'fake': fake_count}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator with sample weights and debugging utilities\n",
    "def data_generator(data_path, batch_size=32, shuffle=True):\n",
    "    real_files = [os.path.join(data_path, 'real', f) for f in os.listdir(os.path.join(data_path, 'real')) if f.endswith('.wav')]\n",
    "    fake_files = [os.path.join(data_path, 'fake', f) for f in os.listdir(os.path.join(data_path, 'fake')) if f.endswith('.wav')]\n",
    "    \n",
    "    all_files = real_files + fake_files\n",
    "    labels = [1] * len(real_files) + [0] * len(fake_files)\n",
    "    \n",
    "    total_samples = len(all_files)\n",
    "    class_weights = {\n",
    "        1: total_samples / (2 * len(real_files)),\n",
    "        0: total_samples / (2 * len(fake_files))\n",
    "    }\n",
    "    \n",
    "    while True:\n",
    "        if shuffle:\n",
    "            temp = list(zip(all_files, labels))\n",
    "            np.random.shuffle(temp)\n",
    "            all_files, labels = zip(*temp)\n",
    "        \n",
    "        for i in range(0, len(all_files), batch_size):\n",
    "            batch_files = all_files[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            \n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "            batch_weights = []\n",
    "            max_length = 0\n",
    "            \n",
    "            for file_path, label in zip(batch_files, batch_labels):\n",
    "                audio = load_and_preprocess_audio(file_path)\n",
    "                features = extract_features(audio)\n",
    "                \n",
    "                if features is not None:\n",
    "                    batch_x.append(features.T)\n",
    "                    batch_y.append(label)\n",
    "                    weight = class_weights[label]\n",
    "                    batch_weights.append(weight)\n",
    "                    max_length = max(max_length, features.T.shape[0])\n",
    "            \n",
    "            # Pad sequences to max_length\n",
    "            padded_batch_x = []\n",
    "            for x in batch_x:\n",
    "                padded_x = np.pad(x, ((0, max_length - x.shape[0]), (0, 0)), mode='constant')\n",
    "                padded_batch_x.append(padded_x)\n",
    "            \n",
    "            if padded_batch_x:\n",
    "                yield np.array(padded_batch_x), np.array(batch_y), np.array(batch_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFM(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MFM, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return tf.reshape(tf.math.maximum(inputs[:,:,:shape[-1]//2], inputs[:,:,shape[-1]//2:]), (shape[0], shape[1], shape[-1]//2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_lc_grnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = BatchNormalization()(inputs)\n",
    "\n",
    "    # Light Convolutional layers with increased regularization\n",
    "    x = Conv1D(32, 5, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
    "    x = MFM()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    x = Conv1D(64, 3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
    "    x = MFM()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    # Bidirectional GRU layers with residual connections and increased regularization\n",
    "    for units in [64, 32]:\n",
    "        gru = Bidirectional(GRU(units // 2, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.02)))\n",
    "        gru_output = gru(x)\n",
    "        gru_output = Dense(K.int_shape(x)[-1])(gru_output)\n",
    "        x = Add()([x, gru_output])\n",
    "        x = LayerNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    # Attention mechanism\n",
    "    attention = Attention()([x, x])\n",
    "    x = Add()([x, attention])\n",
    "\n",
    "    # Final GRU layer\n",
    "    x = Bidirectional(GRU(16, kernel_regularizer=tf.keras.regularizers.l2(0.02)))(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Dense layers with increased regularization\n",
    "    x = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "train_data_path = 'SceneFake/train'\n",
    "dev_data_path = 'SceneFake/dev'\n",
    "eval_data_path = 'SceneFake/eval'\n",
    "\n",
    "# Analyze class distribution\n",
    "print(\"Training set class distribution:\")\n",
    "analyze_class_distribution(train_data_path)\n",
    "analyze_class_distribution(dev_data_path)\n",
    "analyze_class_distribution(eval_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files(path):\n",
    "    real_files = [f for f in os.listdir(os.path.join(path, 'real')) if f.endswith('.wav')]\n",
    "    fake_files = [f for f in os.listdir(os.path.join(path, 'fake')) if f.endswith('.wav')]\n",
    "    return len(real_files) + len(fake_files)\n",
    "\n",
    "# Create generators\n",
    "batch_size = 32  # Increased batch size\n",
    "train_gen = data_generator(train_data_path, batch_size=batch_size)\n",
    "dev_gen = data_generator(dev_data_path, batch_size=batch_size)\n",
    "eval_gen = data_generator(eval_data_path, batch_size=batch_size)\n",
    "\n",
    "# Calculate steps per epoch\n",
    "train_samples_count = count_files(train_data_path)\n",
    "dev_samples_count = count_files(dev_data_path)\n",
    "eval_samples_count = count_files(eval_data_path)\n",
    "\n",
    "steps_per_epoch = train_samples_count // batch_size\n",
    "validation_steps = dev_samples_count // batch_size\n",
    "eval_steps = eval_samples_count // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile the model\n",
    "input_shape = (None, 80)\n",
    "model = create_enhanced_lc_grnn_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=50,  # Reduced number of epochs\n",
    "    validation_data=dev_gen,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"\\nEvaluating on Evaluation Dataset:\")\n",
    "eval_results = model.evaluate(eval_gen, steps=eval_steps)\n",
    "print(f\"Evaluation Results - Loss: {eval_results[0]}, Accuracy: {eval_results[1]}, AUC: {eval_results[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reset the generator to its initial state\n",
    "eval_gen = data_generator(eval_data_path, batch_size=batch_size)\n",
    "\n",
    "# Generate predictions and collect true labels\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for _ in range(eval_steps):\n",
    "    batch_x, batch_y, _ = next(eval_gen)\n",
    "    batch_pred = model.predict(batch_x, verbose=0)\n",
    "    y_pred.extend(batch_pred.flatten())\n",
    "    y_true.extend(batch_y)\n",
    "\n",
    "# Convert to numpy arrays and ensure same length\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "min_len = min(len(y_pred), len(y_true))\n",
    "y_pred = y_pred[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "\n",
    "# Convert predictions to binary (0 or 1)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_binary)\n",
    "\n",
    "# Convert confusion matrix to percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Visualize confusion matrix as percentages\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap='Blues', cbar_kws={'format': '%.0f%%'})\n",
    "plt.title('Confusion Matrix (Percentage)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import noisereduce as nr\n",
    "from scipy.signal import butter, sosfilt\n",
    "import soundfile as sf\n",
    "\n",
    "def denoise_and_amplify(audio, sr):\n",
    "    try:\n",
    "        # Noise Reduction\n",
    "        reduced_noise = nr.reduce_noise(y=audio, sr=sr, stationary=False)\n",
    "        return reduced_noise\n",
    "    except Exception as e:\n",
    "        print(f\"Error in denoise_and_amplify: {e}\")\n",
    "        return audio  # Return original audio if an error occurs\n",
    "\n",
    "def preprocess_and_visualize(file_path, sr=16000, duration=4):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
    "    \n",
    "    # Save original audio\n",
    "    sf.write('original_audio.wav', audio, sr)\n",
    "    \n",
    "    # Original Mel Spectrogram\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=80, n_fft=2048, hop_length=512)\n",
    "    librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.title('Original Mel Spectrogram')\n",
    "    \n",
    "    # Noise Reduction using denoise_and_amplify function\n",
    "    reduced_noise = denoise_and_amplify(audio, sr)\n",
    "    sf.write('noise_reduced_audio.wav', reduced_noise, sr)\n",
    "    \n",
    "    # Final Mel Spectrogram\n",
    "    plt.subplot(1, 3, 3)\n",
    "    final_mel_spec = librosa.feature.melspectrogram(y=reduced_noise, sr=sr, n_mels=80, n_fft=2048, hop_length=512)\n",
    "    librosa.display.specshow(librosa.power_to_db(final_mel_spec, ref=np.max), sr=sr, x_axis='time', y_axis='mel')\n",
    "    plt.title('Final Mel Spectrogram')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mel_spectrograms.png')\n",
    "    plt.close()\n",
    "\n",
    "# Use the function\n",
    "file_path = 'A_2582_0_A.wav'\n",
    "preprocess_and_visualize(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(y_true, y_pred_binary)\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
